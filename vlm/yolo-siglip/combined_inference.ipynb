{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T08:53:04.202915Z",
     "start_time": "2024-05-22T08:53:04.185918Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import orjson\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:49:18.246514Z",
     "start_time": "2024-05-22T08:49:18.146235Z"
    }
   },
   "cell_type": "code",
   "source": "yolo_model = YOLO(\"yolov8/runs/detect/yolov9c 0.99 0.769/weights/best.pt\")  # load a pretrained model (recommended for training)",
   "id": "eeb454dee613ce7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:49:20.881898Z",
     "start_time": "2024-05-22T08:49:19.086173Z"
    }
   },
   "cell_type": "code",
   "source": "image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"siglip/siglip-so400m-patch14-384\", batch_size=4, device='cuda')",
   "id": "fed0d17b62497788",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:29:13.953527Z",
     "start_time": "2024-05-22T09:29:13.909529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../../data/vlm.jsonl', 'r') as f:\n",
    "    instances = [orjson.loads(line.strip()) for line in f if line.strip() != \"\"]\n",
    "results = []\n",
    "val_percent = 0.2\n",
    "val_split = int(len(instances) * val_percent)\n",
    "train, val = instances[:-val_split], instances[-val_split:]\n",
    "bs = 4\n",
    "batched_instances = [val[i:i + bs] for i in range(0, len(val), bs)]"
   ],
   "id": "7d7734c1329ac540",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:33:58.833023Z",
     "start_time": "2024-05-22T09:29:17.293261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch_instance in tqdm(batched_instances):\n",
    "    images = [Image.open(os.path.join('../../data/images/', i['image'])) for i in batch_instance]\n",
    "    \n",
    "    # YOLO object det\n",
    "    yolo_result = yolo_model.predict(images, imgsz=1600, conf=0.365, iou=0.1, max_det=10, verbose=False)  # max F1, try augment=True and adjusting iou\n",
    "    yolo_result = [(r.boxes.xyxy.tolist(), r.boxes.conf.tolist()) for r in yolo_result]\n",
    "    yolo_result = [tuple(zip(*r)) for r in yolo_result]  # list of tuple[box, conf] in each image in xyxy format\n",
    "    \n",
    "    # crop the boxes out\n",
    "    cropped_boxes = []\n",
    "    for im, boxes in zip(images, yolo_result):\n",
    "        im_boxes = []\n",
    "        for (x1, y1, x2, y2), _ in boxes:\n",
    "            im_boxes.append(im.crop((x1, y1, x2, y2)))\n",
    "        cropped_boxes.append(im_boxes)\n",
    "    \n",
    "    captions_list = [[anno['caption'] for anno in img['annotations']] for img in batch_instance]  # list of list of str, len is n_img == 4\n",
    "    assert len(cropped_boxes) == len(captions_list)\n",
    "    \n",
    "    # siglip inference\n",
    "    siglip_results = []\n",
    "    with torch.cuda.amp.autocast():\n",
    "        for boxes, captions in zip(cropped_boxes, captions_list):\n",
    "            r = image_classifier(boxes, candidate_labels=captions)\n",
    "            image_to_text_scores = {caption: [] for caption in captions}  # {caption: [score1, score2, ...]}, scores in sequence of bbox\n",
    "            for box in r:\n",
    "                for label_score in box:\n",
    "                    image_to_text_scores[label_score['label']].append(label_score['score'])\n",
    "            siglip_results.append(image_to_text_scores)\n",
    "    \n",
    "    # combine the results\n",
    "    visualize = False\n",
    "    for im, cropped_box_PIL, yolo_box, similarity_scores, instance in zip(images, cropped_boxes, yolo_result, siglip_results, batch_instance):\n",
    "        if visualize: im_cp = im.copy()\n",
    "        result_for_im = {}\n",
    "        for caption, caption_scores in similarity_scores.items():\n",
    "            box_idx = np.argmax(caption_scores)\n",
    "            highest_caption_score = max(caption_scores)\n",
    "            box = cropped_box_PIL[box_idx]\n",
    "            result_for_im[caption] = yolo_box[box_idx][0]  # dict[caption] = xyxy in list\n",
    "            if visualize:\n",
    "                draw = ImageDraw.Draw(im_cp)  # noqa\n",
    "                (x1, y1, x2, y2), box_conf = yolo_box[box_idx]\n",
    "                draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "                draw.text((x1, y1), text=f'{caption} {box_conf:.2f} {highest_caption_score:.2f}', fill='red')\n",
    "        if visualize: im_cp.show()\n",
    "        results.append({'image': instance['image'], 'annotations': [{'bbox': v, 'caption': k} for k, v in result_for_im.items()]})\n",
    "        # save every image in case of crash\n",
    "        with open('yolo-siglip-zeroshot.json', 'wb+') as f:\n",
    "            f.write(orjson.dumps(results))"
   ],
   "id": "bd1de3e86eac3aaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 255/256 [04:41<00:01,  1.13s/it]C:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 256/256 [04:41<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:02:19.576994Z",
     "start_time": "2024-05-22T05:02:18.959162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot bbox\n",
    "for im, boxes in zip(ims, yolo_result):\n",
    "    im = im.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    for (x1, y1, x2, y2), conf in boxes:\n",
    "        draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "        draw.text((x1, y1), text=f'{conf:.2f}', fill='red')\n",
    "    im.show()"
   ],
   "id": "d43145a5bb3bfb6a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T06:47:34.854298Z",
     "start_time": "2024-05-22T06:47:34.846299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "visualize = False\n",
    "for im, cropped_box_PIL, yolo_box, similarity_scores in zip(ims, cropped_boxes, yolo_result, siglip_results):\n",
    "    if visualize: im_cp = im.copy()\n",
    "    result_for_im = {}\n",
    "    for caption, caption_scores in similarity_scores.items():\n",
    "        box_idx = np.argmax(caption_scores)\n",
    "        highest_caption_score = max(caption_scores)\n",
    "        box = cropped_box_PIL[box_idx]\n",
    "        result_for_im[caption] = yolo_box[box_idx][0]  # dict[caption] = (xyxy in list, conf)\n",
    "        if visualize:\n",
    "            draw = ImageDraw.Draw(im_cp)\n",
    "            (x1, y1, x2, y2), box_conf = yolo_box[box_idx]\n",
    "            draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "            draw.text((x1, y1), text=f'{caption} {box_conf:.2f} {highest_caption_score:.2f}', fill='red')\n",
    "    if visualize: im_cp.show()\n",
    "    results.append(result_for_im)"
   ],
   "id": "868a3fa28f6b717c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T06:47:37.331350Z",
     "start_time": "2024-05-22T06:47:37.312831Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "43cb38d309a9426e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'grey missile': [705.0738525390625,\n",
       "   506.7243347167969,\n",
       "   782.65283203125,\n",
       "   563.574951171875],\n",
       "  'red, white, and blue light aircraft': [1030.6815185546875,\n",
       "   77.49951934814453,\n",
       "   1056.74853515625,\n",
       "   110.44055938720703],\n",
       "  'green and black missile': [705.0738525390625,\n",
       "   506.7243347167969,\n",
       "   782.65283203125,\n",
       "   563.574951171875],\n",
       "  'white and red helicopter': [527.7639770507812,\n",
       "   118.3411865234375,\n",
       "   624.7859497070312,\n",
       "   161.6909637451172]},\n",
       " {'grey camouflage fighter jet': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'grey and white fighter plane': [1117.64501953125,\n",
       "   514.673828125,\n",
       "   1254.2855224609375,\n",
       "   553.1058959960938],\n",
       "  'white and black drone': [356.56414794921875,\n",
       "   455.2095031738281,\n",
       "   402.8783264160156,\n",
       "   486.3287353515625],\n",
       "  'white and black fighter jet': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'white missile': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'black and white commercial aircraft': [807.0028686523438,\n",
       "   521.8709716796875,\n",
       "   875.6414794921875,\n",
       "   572.2413940429688]}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dd9a89",
      "metadata": {
        "id": "91dd9a89"
      },
      "outputs": [],
      "source": [
        "using_colab = True\n",
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
        "\n",
        "    # !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be845da",
      "metadata": {
        "id": "0be845da"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29bc90d5",
      "metadata": {
        "id": "29bc90d5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title visualization utils\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23842fb2",
      "metadata": {
        "id": "23842fb2"
      },
      "source": [
        "## Example image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30125fd",
      "metadata": {
        "scrolled": false,
        "id": "e30125fd"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('images/truck.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "print('image.shape', image.shape)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b228b8",
      "metadata": {
        "id": "98b228b8"
      },
      "source": [
        "## Selecting objects with SAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e28150b",
      "metadata": {
        "id": "7e28150b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from typing import Optional, Tuple\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "class CustomSamPredictor(SamPredictor):\n",
        "    @torch.no_grad()\n",
        "    def set_torch_image(\n",
        "        self,\n",
        "        transformed_image,\n",
        "        original_image_size,\n",
        "    ) -> None:\n",
        "        assert (\n",
        "            len(transformed_image.shape) == 4\n",
        "            and transformed_image.shape[1] == 3\n",
        "            and max(*transformed_image.shape[2:]) == self.model.image_encoder.img_size\n",
        "        ), f\"set_torch_image input must be BCHW with long side {self.model.image_encoder.img_size}.\"\n",
        "        self.reset_image()\n",
        "\n",
        "        self.original_size = original_image_size\n",
        "        self.input_size = tuple(transformed_image.shape[-2:])\n",
        "        input_image = self.model.preprocess(transformed_image)\n",
        "        self.features = self.model.image_encoder(input_image)\n",
        "        self.input_image = input_image\n",
        "        self.transformed_image = transformed_image\n",
        "        self.is_image_set = True\n",
        "predictor = CustomSamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95d48dd",
      "metadata": {
        "id": "d95d48dd"
      },
      "outputs": [],
      "source": [
        "predictor.set_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91ba973",
      "metadata": {
        "id": "a91ba973"
      },
      "outputs": [],
      "source": [
        "input_point = np.array([[500, 375]])\n",
        "input_label = np.array([1])\n",
        "plt.figure(figsize=(10,10))\n",
        "print('image.shape', image.shape)\n",
        "print('image.max', image.max())\n",
        "print('image.min', image.min())\n",
        "plt.imshow(image)\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5373fd68",
      "metadata": {
        "id": "5373fd68"
      },
      "outputs": [],
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)"
      ],
      "metadata": {
        "id": "uGylQ3e7M4x9"
      },
      "id": "uGylQ3e7M4x9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow((logits[0] > 0)*255)"
      ],
      "metadata": {
        "id": "raEIajJlqHrw"
      },
      "id": "raEIajJlqHrw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c227a6",
      "metadata": {
        "scrolled": false,
        "id": "e9c227a6"
      },
      "outputs": [],
      "source": [
        "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(image)\n",
        "    show_mask(mask, plt.gca())\n",
        "    show_points(input_point, input_label, plt.gca())\n",
        "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits[0]"
      ],
      "metadata": {
        "id": "6IF5sz3hyU3x"
      },
      "id": "6IF5sz3hyU3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks_2, scores_2, logits_2 = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        "    mask_input=logits[:1]\n",
        ")"
      ],
      "metadata": {
        "id": "EdWv_O7iyJDJ"
      },
      "id": "EdWv_O7iyJDJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (mask, score) in enumerate(zip(masks_2, scores_2)):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(image)\n",
        "    show_mask(mask, plt.gca())\n",
        "    show_points(input_point, input_label, plt.gca())\n",
        "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "U6D_aDQWyirk"
      },
      "id": "U6D_aDQWyirk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLeuzuRhyJFb"
      },
      "id": "cLeuzuRhyJFb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXSCO7HOyJIE"
      },
      "id": "VXSCO7HOyJIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZ1s7iWexyuI"
      },
      "id": "xZ1s7iWexyuI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLEH1NWkxyw-"
      },
      "id": "PLEH1NWkxyw-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = sam.state_dict()\n",
        "print(sam)"
      ],
      "metadata": {
        "id": "tDsO9UfitZvJ"
      },
      "id": "tDsO9UfitZvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import copy\n",
        "torch_weights = copy.deepcopy(sam.state_dict())\n",
        "table = []\n",
        "num_params = 0\n",
        "for k in sorted(torch_weights):\n",
        "  if 'mask_downscaling' in k:\n",
        "    continue\n",
        "  v = torch_weights[k]\n",
        "  table.append((k, f'{v.shape}', f'{v.mean():.3f}', f'{v.std():.3f}'))\n",
        "  num_params += np.prod(np.asarray(v.shape))\n",
        "table_str = tabulate(\n",
        "    table, tablefmt=\"pipe\", headers=[\"Names\", \"shape\", \"mean\", \"std\"])\n",
        "print(table_str)\n",
        "print('num_params', num_params)"
      ],
      "metadata": {
        "id": "jc-2b4T9tdAJ"
      },
      "id": "jc-2b4T9tdAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections"
      ],
      "metadata": {
        "id": "FTRmOVneulcw"
      },
      "id": "FTRmOVneulcw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax image-encoder\n",
        "\"\"\"ViT with windows attention.\"\"\"\n",
        "\n",
        "import functools\n",
        "from typing import Any, Optional\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "KERNEL_INIT = {\n",
        "    'normal': nn.initializers.normal(stddev=0.02),\n",
        "}\n",
        "\n",
        "\n",
        "class HMAttention(nn.Module):\n",
        "  \"\"\"Multi-head Attention block with relative position embeddings.\n",
        "\n",
        "  Attributes:\n",
        "  dim (int): Number of input channels.\n",
        "  num_heads (int): Number of attention heads.\n",
        "  qkv_bias (bool:  If True, add a learnable bias to query, key, value.\n",
        "  beit_like_qkv_bias (bool): no bias for k.\n",
        "  use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "    attention map.\n",
        "  rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "    parameters.\n",
        "  input_size (int or None): Input resolution for calculating the relative\n",
        "    positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int = 8\n",
        "  qkv_bias: bool = True\n",
        "  beit_like_qkv_bias: bool = False\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  input_size: Optional[Any] = None\n",
        "  kernel_init: str = 'normal'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def get_rel_pos(self, q_size, k_size, rel_pos):\n",
        "    \"\"\"Get relative positional embeddings.\n",
        "\n",
        "    Args:\n",
        "      q_size (int): size of query q.\n",
        "      k_size (int): size of key k.\n",
        "      rel_pos (Tensor): relative position embeddings (L, C).\n",
        "    Returns:\n",
        "      Extracted positional embeddings according to relative positions.\n",
        "    \"\"\"\n",
        "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
        "    # Interpolate rel pos if needed.\n",
        "    if rel_pos.shape[0] != max_rel_dist:\n",
        "      # Interpolate rel pos.\n",
        "      rel_pos_resized = jax.image.resize(\n",
        "          rel_pos,\n",
        "          shape=(max_rel_dist, rel_pos.shape[1]),\n",
        "          method='linear',\n",
        "      )\n",
        "    else:\n",
        "      rel_pos_resized = rel_pos\n",
        "\n",
        "    # Scale the coords with short length if shapes for q and k are different.\n",
        "    q_coords = jnp.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
        "    k_coords = jnp.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
        "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(\n",
        "        q_size / k_size, 1.0)\n",
        "    relative_coords = relative_coords.astype(jnp.int32).reshape(-1)\n",
        "    return rel_pos_resized[relative_coords].reshape(q_size, k_size, -1)\n",
        "\n",
        "  def add_decomposed_rel_pos(\n",
        "      self, attn, q, rel_pos_h, rel_pos_w, q_size, k_size):\n",
        "    \"\"\"Calculate decomposed Relative Positional Embeddings from paper:`mvitv2`.\n",
        "\n",
        "    Args:\n",
        "      attn (Tensor): attention map.\n",
        "      q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
        "      rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
        "      rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
        "      q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
        "      k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
        "    Returns:\n",
        "      attn (Tensor): attention map with added relative positional embeddings.\n",
        "    \"\"\"\n",
        "    q_h, q_w = q_size\n",
        "    k_h, k_w = k_size\n",
        "    rh = self.get_rel_pos(q_h, k_h, rel_pos_h)\n",
        "    rw = self.get_rel_pos(q_w, k_w, rel_pos_w)\n",
        "\n",
        "    batch, _, dim = q.shape\n",
        "    r_q = q.reshape(batch, q_h, q_w, dim)\n",
        "    rel_h = jnp.einsum('bhwc,hkc->bhwk', r_q, rh)\n",
        "    rel_w = jnp.einsum('bhwc,wkc->bhwk', r_q, rw)\n",
        "\n",
        "    attn = (\n",
        "        attn.reshape(batch, q_h, q_w, k_h, k_w) + rel_h[\n",
        "            :, :, :, :, None] + rel_w[:, :, :, None, :]\n",
        "    ).reshape(batch, q_h * q_w, k_h * k_w)\n",
        "\n",
        "    return attn\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    batch, height, width, _ = x.shape\n",
        "    head_dim = self.dim // self.num_heads\n",
        "    if self.beit_like_qkv_bias:\n",
        "      q_bias = self.param(\n",
        "          'q_bias', nn.initializers.zeros, (self.dim,))\n",
        "      v_bias = self.param(\n",
        "          'v_bias', nn.initializers.zeros, (self.dim,))\n",
        "      k_bias = jnp.zeros((self.dim,), dtype=jnp.float32)\n",
        "      qkv_bias = jnp.concatenate([q_bias, k_bias, v_bias], axis=0)\n",
        "      qkv = nn.Dense(\n",
        "          self.dim * 3, use_bias=False, dtype=self.dtype,\n",
        "          kernel_init=KERNEL_INIT[self.kernel_init], name='qkv')(\n",
        "              x)  # batch x height x width x 3dim\n",
        "      qkv = qkv + qkv_bias[None, None, None, :]\n",
        "    else:\n",
        "      qkv = nn.Dense(\n",
        "          self.dim * 3, use_bias=self.qkv_bias, dtype=self.dtype,\n",
        "          kernel_init=KERNEL_INIT[self.kernel_init], name='qkv')(\n",
        "              x)  # batch x height x width x 3dim\n",
        "    qkv = qkv.reshape(batch, height * width, 3, self.num_heads, -1).transpose(\n",
        "        2, 0, 3, 1, 4)  # 3 x batch x num_heads x num_tokens x D\n",
        "    qkv = qkv.reshape(3, batch * self.num_heads, height * width, -1)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]  # [batch * num_heads, num_tokens, D]\n",
        "    attn = (q * (head_dim ** -0.5)) @ k.transpose(\n",
        "        0, 2, 1)  # [batch * num_heads, num_tokens, num_tokens]\n",
        "    if self.use_rel_pos:\n",
        "      rel_pos_h = self.param(\n",
        "          'rel_pos_h', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      rel_pos_w = self.param(\n",
        "          'rel_pos_w', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      attn = self.add_decomposed_rel_pos(\n",
        "          attn, q, rel_pos_h, rel_pos_w,\n",
        "          (height, width), (height, width))\n",
        "    attn = jax.nn.softmax(attn)\n",
        "    x = (attn @ v).reshape(batch, self.num_heads, height, width, -1).transpose(\n",
        "        0, 2, 3, 1, 4).reshape(batch, height, width, -1)\n",
        "    x = nn.Dense(\n",
        "        self.dim, dtype=self.dtype, kernel_init=KERNEL_INIT[self.kernel_init],\n",
        "        name='proj')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "\n",
        "  hidden_features: int\n",
        "  out_features: int\n",
        "  kernel_init: str = 'normal'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(\n",
        "        self.hidden_features, dtype=self.dtype,\n",
        "        kernel_init=KERNEL_INIT[self.kernel_init], name='lin1')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    x = nn.Dense(\n",
        "        self.out_features, dtype=self.dtype,\n",
        "        kernel_init=KERNEL_INIT[self.kernel_init], name='lin2')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer blocks with support of window attention and residual blocks.\n",
        "\n",
        "  Attributes:\n",
        "    dim (int): Number of input channels.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    beit_like_qkv_bias (bool): no bias for k.\n",
        "    drop_path (float): Stochastic depth rate.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks. If it equals 0,\n",
        "      then not use window attention.\n",
        "    input_size (int or None): Input resolution for calculating the relative\n",
        "      positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int\n",
        "  mlp_ratio: float = 4.0\n",
        "  qkv_bias: bool = True\n",
        "  beit_like_qkv_bias: bool = False\n",
        "  drop_path: float = 0.0\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 0\n",
        "  input_size: Optional[Any] = None\n",
        "  kernel_init: str = 'normal'\n",
        "  layer_scale_init_value: float = -1.0\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def window_partition(self, x):\n",
        "    \"\"\"Partition into non-overlapping windows with padding if needed.\n",
        "\n",
        "    Args:\n",
        "      x (array): input tokens with [B, H, W, C].\n",
        "    Returns:\n",
        "      windows: windows after partition with [B * num_windows, window_size,\n",
        "        window_size, C].\n",
        "      (Hp, Wp): padded height and width before partition\n",
        "    \"\"\"\n",
        "    batch, h, w, c = x.shape\n",
        "\n",
        "    pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
        "    pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "      x = jnp.pad(\n",
        "          x, ((0, 0), (0, pad_w), (0, pad_h), (0, 0)),\n",
        "          'constant', constant_values=0)\n",
        "    hp, wp = h + pad_h, w + pad_w\n",
        "\n",
        "    x = x.reshape(\n",
        "        batch, hp // self.window_size, self.window_size,\n",
        "        wp // self.window_size, self.window_size, c)\n",
        "    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(\n",
        "        -1, self.window_size, self.window_size, c)\n",
        "    return windows, (hp, wp)\n",
        "\n",
        "  def window_unpartition(self, windows, pad_hw, hw):\n",
        "    \"\"\"Window unpartition into original sequences and removing padding.\n",
        "\n",
        "    Args:\n",
        "      windows (array): inputs: [B * num_windows, window_size, window_size, C].\n",
        "      pad_hw (Tuple): padded height and width (Hp, Wp).\n",
        "      hw (Tuple): original height and width (H, W) before padding.\n",
        "\n",
        "    Returns:\n",
        "      x: unpartitioned sequences with [B, H, W, C].\n",
        "    \"\"\"\n",
        "    hp, wp = pad_hw\n",
        "    h, w = hw\n",
        "    batch = windows.shape[0] // (\n",
        "        hp * wp // self.window_size // self.window_size)\n",
        "    x = windows.reshape(\n",
        "        batch,\n",
        "        hp // self.window_size, wp // self.window_size,\n",
        "        self.window_size, self.window_size, -1)\n",
        "    x = x.transpose(0, 1, 3, 2, 4, 5).reshape(batch, hp, wp, -1)\n",
        "    if hp > h or wp > w:\n",
        "      x = x[:, :h, :w, :]\n",
        "    return x\n",
        "\n",
        "  def get_keep_pattern(self,\n",
        "                       x: jnp.ndarray,\n",
        "                       deterministic: bool):\n",
        "    \"\"\"DropPath Layer.\"\"\"\n",
        "    if not deterministic and self.drop_path:\n",
        "      shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "      drop_pattern = jax.random.bernoulli(\n",
        "          self.make_rng('dropout'), self.drop_path, shape).astype(self.dtype)\n",
        "      keep_pattern = (1. - drop_pattern)\n",
        "      if self.drop_path < 1.:\n",
        "        keep_pattern = keep_pattern / (1. - self.drop_path)\n",
        "      return keep_pattern\n",
        "    else:\n",
        "      return 1.0\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train=False):\n",
        "    shortcut = x\n",
        "    ln = functools.partial(nn.LayerNorm, epsilon=1e-6, dtype=self.dtype)\n",
        "    x = ln(name='norm1')(x)\n",
        "    # Window partition\n",
        "    if self.window_size > 0:\n",
        "      h, w = x.shape[1], x.shape[2]\n",
        "      x, pad_hw = self.window_partition(x)\n",
        "\n",
        "    x = HMAttention(\n",
        "        self.dim,\n",
        "        num_heads=self.num_heads,\n",
        "        qkv_bias=self.qkv_bias,\n",
        "        beit_like_qkv_bias=self.beit_like_qkv_bias,\n",
        "        use_rel_pos=self.use_rel_pos,\n",
        "        rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "        input_size=self.input_size if self.window_size == 0 else (\n",
        "            self.window_size, self.window_size),\n",
        "        kernel_init=self.kernel_init,\n",
        "        dtype=self.dtype,\n",
        "        name='attn')(x)\n",
        "    # Reverse window partition\n",
        "    if self.window_size > 0:\n",
        "      x = self.window_unpartition(x, pad_hw, (h, w))\n",
        "\n",
        "    if self.layer_scale_init_value > 0:\n",
        "      gamma_1 = self.param(\n",
        "          'gamma_1',\n",
        "          nn.initializers.constant(self.layer_scale_init_value),\n",
        "          (self.dim))\n",
        "      x = x * gamma_1[..., :]\n",
        "    x = shortcut + self.get_keep_pattern(x, not train) * x\n",
        "    y = ln(name='norm2')(x)\n",
        "    y = Mlp(\n",
        "        int(self.dim * self.mlp_ratio),\n",
        "        self.dim,\n",
        "        kernel_init=self.kernel_init,\n",
        "        dtype=self.dtype,\n",
        "        name='mlp')(y)\n",
        "    if self.layer_scale_init_value > 0:\n",
        "      gamma_2 = self.param(\n",
        "          'gamma_2',\n",
        "          nn.initializers.constant(self.layer_scale_init_value),\n",
        "          (self.dim))\n",
        "      y = y * gamma_2[..., :]\n",
        "    x = x + self.get_keep_pattern(y, not train) * y\n",
        "    return x\n",
        "\n",
        "\n",
        "class Neck(nn.Module):\n",
        "  \"\"\"Sam convolutional neck blocks.\"\"\"\n",
        "  out_chans: int = 768\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    \"\"\"Forward pass.\n",
        "\n",
        "    Args:\n",
        "      x: (batch_size, height, width, dim)\n",
        "    Returns:\n",
        "      x: (batch_size, height, width, dim)\n",
        "    \"\"\"\n",
        "    x = nn.Conv(\n",
        "        self.out_chans,\n",
        "        (1, 1),\n",
        "        strides=(1, 1),\n",
        "        padding='VALID',\n",
        "        use_bias=False,\n",
        "        dtype=self.dtype,\n",
        "        name='0')(x)\n",
        "    x = nn.LayerNorm(name='1')(x)\n",
        "    x = nn.Conv(\n",
        "        self.out_chans,\n",
        "        (3, 3),\n",
        "        strides=(1, 1),\n",
        "        padding=[(1, 1), (1, 1)],\n",
        "        use_bias=False,\n",
        "        dtype=self.dtype,\n",
        "        name='2')(x)\n",
        "    x = nn.LayerNorm(name='3')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ImageEncoderViT(nn.Module):\n",
        "  \"\"\"This ViT model in Sam.\n",
        "\n",
        "  TODO(zhouxy): check difference from ViTDet:\n",
        "    - neck block after transformers\n",
        "    - no droppath (inference only?).\n",
        "\n",
        "  Attributes:\n",
        "    img_size (int): Input image size.\n",
        "    patch_size (int): Patch size.\n",
        "    in_chans (int): Number of input image channels.\n",
        "    embed_dim (int): Patch embedding dimension.\n",
        "    depth (int): Depth of ViT.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    out_chans (int): output channals\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    beit_like_qkv_bias (bool): no bias for k.\n",
        "    drop_path_rate (float): Stochastic depth rate.\n",
        "    use_abs_pos (bool): If True, use absolute positional embeddings.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks.\n",
        "    window_block_indexes (list): Indexes for blocks using window attention.\n",
        "    pretrain_img_size (int): input image size for pretraining models.\n",
        "  \"\"\"\n",
        "  img_size: int = 1024\n",
        "  patch_size: int = 16\n",
        "  in_chans: int = 3\n",
        "  embed_dim: int = 768\n",
        "  depth: int = 12\n",
        "  num_heads: int = 12\n",
        "  mlp_ratio: float = 4.0\n",
        "  out_chans: int = 256\n",
        "  qkv_bias: bool = True\n",
        "  beit_like_qkv_bias: bool = False\n",
        "  drop_path_rate: float = 0.1\n",
        "  use_abs_pos: bool = True\n",
        "  use_rel_pos: bool = True\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 14\n",
        "  window_block_indexes: Any = (0, 1, 3, 4, 6, 7, 9, 10)\n",
        "  pretrain_img_size: int = 224\n",
        "  kernel_init: str = 'normal'\n",
        "  layer_scale_init_value: float = -1.0\n",
        "  freeze_vit_layer: int = -1\n",
        "  use_ln_pre: bool = False\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               x: jnp.ndarray,\n",
        "               train: bool = False,):\n",
        "    \"\"\"Forward vit.\n",
        "\n",
        "    Args:\n",
        "      x: (batch_size, H, W, 3)\n",
        "      train: bool\n",
        "    Returns:\n",
        "      x: (batch_size, H // patch_size, W // patch_size, embed_dim)\n",
        "    \"\"\"\n",
        "    x = nn.Conv(\n",
        "        self.embed_dim, (self.patch_size, self.patch_size),\n",
        "        strides=(self.patch_size, self.patch_size),\n",
        "        padding='VALID',\n",
        "        dtype=self.dtype,\n",
        "        name='patch_embed.proj')(x)\n",
        "    if self.use_abs_pos:\n",
        "      pos_embed = self.param(\n",
        "          'pos_embed', nn.initializers.zeros,\n",
        "          (1, self.img_size // self.patch_size,\n",
        "           self.img_size // self.patch_size, self.embed_dim))\n",
        "      x = x + pos_embed\n",
        "    dp_rates = [\n",
        "        self.drop_path_rate * i / (self.depth - 1) for i in range(self.depth)]\n",
        "    if self.use_ln_pre:\n",
        "      x = nn.LayerNorm(name='ln_pre')(x)\n",
        "\n",
        "    for i in range(self.depth):\n",
        "      x = Block(\n",
        "          dim=self.embed_dim,\n",
        "          num_heads=self.num_heads,\n",
        "          mlp_ratio=self.mlp_ratio,\n",
        "          qkv_bias=self.qkv_bias,\n",
        "          beit_like_qkv_bias=self.beit_like_qkv_bias,\n",
        "          drop_path=dp_rates[i],\n",
        "          use_rel_pos=self.use_rel_pos,\n",
        "          rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "          window_size=self.window_size if i in self.window_block_indexes else 0,\n",
        "          input_size=(\n",
        "              self.img_size // self.patch_size,\n",
        "              self.img_size // self.patch_size),\n",
        "          kernel_init=self.kernel_init,\n",
        "          dtype=self.dtype,\n",
        "          layer_scale_init_value=self.layer_scale_init_value,\n",
        "          name=f'blocks.{i}',\n",
        "          )(x, train=train)\n",
        "      if i + 1 < self.freeze_vit_layer:\n",
        "        x = jax.lax.stop_gradient(x)\n",
        "\n",
        "    x = Neck(out_chans=self.out_chans, name='neck')(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Gqwh6j3YuMll",
        "cellView": "form"
      },
      "id": "Gqwh6j3YuMll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax transformer\n",
        "\"\"\"Sam transformer for running cross-attention.\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import Any\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "class TwoWayTransformer(nn.Module):\n",
        "  \"\"\"Transformer with query and key/ value inputs.\"\"\"\n",
        "\n",
        "  depth: int = 2\n",
        "  embedding_dim: int = 256\n",
        "  num_heads: int = 8\n",
        "  mlp_dim: int = 2048\n",
        "  activation: Any = nn.relu\n",
        "  attention_downsample_rate: int = 2\n",
        "\n",
        "  def setup(self):\n",
        "    layers = []\n",
        "    for i in range(self.depth):\n",
        "      layer = TwoWayAttentionBlock(\n",
        "          embedding_dim=self.embedding_dim,\n",
        "          num_heads=self.num_heads,\n",
        "          mlp_dim=self.mlp_dim,\n",
        "          activation=self.activation,\n",
        "          attention_downsample_rate=self.attention_downsample_rate,\n",
        "          skip_first_layer_pe=(i == 0),\n",
        "          name=f'layers.{i}')\n",
        "      layers.append(layer)\n",
        "    self.layers = layers\n",
        "\n",
        "    self.final_attn_token_to_image = Attention(\n",
        "        self.embedding_dim, self.num_heads, self.attention_downsample_rate,\n",
        "        name='final_attn_token_to_image')\n",
        "    self.norm_final_attn = nn.LayerNorm(epsilon=1e-5, name='norm_final_attn')\n",
        "\n",
        "  def __call__(self, image_embedding, image_pe, point_embedding):\n",
        "    \"\"\"Forward pass.\n",
        "\n",
        "    Args:\n",
        "      image_embedding: (batch_size, h, w, embedding_dim)\n",
        "      image_pe: (batch_size, h, w, embedding_dim)\n",
        "      point_embedding: (batch_size, num_points, embedding_dim)\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    batch_size, c = image_embedding.shape[0], image_embedding.shape[3]\n",
        "    image_embedding = image_embedding.reshape((batch_size, -1, c))\n",
        "    image_pe = image_pe.reshape((batch_size, -1, c))\n",
        "\n",
        "    # Prepare queries\n",
        "    queries = point_embedding\n",
        "    keys = image_embedding\n",
        "\n",
        "    # Apply transformer blocks and final layernorm\n",
        "    for layer in self.layers:\n",
        "      queries, keys = layer(\n",
        "          queries=queries,\n",
        "          keys=keys,\n",
        "          query_pe=point_embedding,\n",
        "          key_pe=image_pe,\n",
        "      )\n",
        "\n",
        "    # Apply the final attention layer from the points to the image\n",
        "    q = queries + point_embedding\n",
        "    k = keys + image_pe\n",
        "    attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
        "    queries = queries + attn_out\n",
        "    queries = self.norm_final_attn(queries)\n",
        "\n",
        "    return queries, keys\n",
        "\n",
        "\n",
        "class TwoWayAttentionBlock(nn.Module):\n",
        "  \"\"\"Transformer block.\"\"\"\n",
        "\n",
        "  embedding_dim: int\n",
        "  num_heads: int\n",
        "  mlp_dim: int = 2048\n",
        "  activation: Any = nn.relu\n",
        "  attention_downsample_rate: int = 2\n",
        "  skip_first_layer_pe: bool = False\n",
        "\n",
        "  def setup(self):\n",
        "    self.self_attn = Attention(\n",
        "        self.embedding_dim, self.num_heads, name='self_attn')\n",
        "    self.norm1 = nn.LayerNorm(epsilon=1e-5, name='norm1')\n",
        "\n",
        "    self.cross_attn_token_to_image = Attention(\n",
        "        self.embedding_dim, self.num_heads, self.attention_downsample_rate,\n",
        "        name='cross_attn_token_to_image')\n",
        "    self.norm2 = nn.LayerNorm(epsilon=1e-5, name='norm2')\n",
        "\n",
        "    self.mlp = MLPBlock(\n",
        "        self.embedding_dim, self.mlp_dim, self.activation,\n",
        "        name='mlp')\n",
        "    self.norm3 = nn.LayerNorm(epsilon=1e-5, name='norm3')\n",
        "\n",
        "    self.norm4 = nn.LayerNorm(epsilon=1e-5, name='norm4')\n",
        "    self.cross_attn_image_to_token = Attention(\n",
        "        self.embedding_dim, self.num_heads, self.attention_downsample_rate,\n",
        "        name='cross_attn_image_to_token')\n",
        "\n",
        "  def __call__(self, queries, keys, query_pe, key_pe):\n",
        "    \"\"\"Forward two-way attention block.\n",
        "\n",
        "    Args:\n",
        "      queries: (batch_size, query_tokens, embedding_dim)\n",
        "      keys: (batch_size, key_tokens, embedding_dim)\n",
        "      query_pe: (batch_size, query_tokens, embedding_dim)\n",
        "      key_pe: (batch_size, key_tokens, embedding_dim)\n",
        "    Returns:\n",
        "      queries: (batch_size, query_tokens, embedding_dim)\n",
        "      keys: (batch_size, key_tokens, embedding_dim)\n",
        "    \"\"\"\n",
        "    # Self attention block\n",
        "    if self.skip_first_layer_pe:\n",
        "      queries = self.self_attn(q=queries, k=queries, v=queries)\n",
        "    else:\n",
        "      q = queries + query_pe\n",
        "      attn_out = self.self_attn(q=q, k=q, v=queries)\n",
        "      queries = queries + attn_out\n",
        "    queries = self.norm1(queries)\n",
        "\n",
        "    # Cross attention block, tokens attending to image embedding\n",
        "    q = queries + query_pe\n",
        "    k = keys + key_pe\n",
        "    attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
        "    queries = queries + attn_out\n",
        "    queries = self.norm2(queries)\n",
        "\n",
        "    # MLP block\n",
        "    mlp_out = self.mlp(queries)\n",
        "    queries = queries + mlp_out\n",
        "    queries = self.norm3(queries)\n",
        "\n",
        "    # Cross attention block, image embedding attending to tokens\n",
        "    q = queries + query_pe\n",
        "    k = keys + key_pe\n",
        "    attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
        "    keys = keys + attn_out\n",
        "    keys = self.norm4(keys)\n",
        "\n",
        "    return queries, keys\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Attention module.\"\"\"\n",
        "  embedding_dim: int\n",
        "  num_heads: int\n",
        "  downsample_rate: int = 1\n",
        "\n",
        "  def setup(self):\n",
        "    self.internal_dim = self.embedding_dim // self.downsample_rate\n",
        "    assert self.internal_dim % self.num_heads == 0, (\n",
        "        'num_heads must divide embedding_dim.')\n",
        "\n",
        "    self.q_proj = nn.Dense(self.internal_dim, name='q_proj')\n",
        "    self.k_proj = nn.Dense(self.internal_dim, name='k_proj')\n",
        "    self.v_proj = nn.Dense(self.internal_dim, name='v_proj')\n",
        "    self.out_proj = nn.Dense(self.embedding_dim, name='out_proj')\n",
        "\n",
        "  def _separate_heads(self, x):\n",
        "    b, n, c = x.shape\n",
        "    x = x.reshape(b, n, self.num_heads, c // self.num_heads)\n",
        "    return x.transpose((0, 2, 1, 3))  # B x N_heads x N_tokens x C_per_head\n",
        "\n",
        "  def _recombine_heads(self, x):\n",
        "    b, n_heads, n_tokens, c_per_head = x.shape\n",
        "    x = x.transpose((0, 2, 1, 3))\n",
        "    return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
        "\n",
        "  def __call__(self, q, k, v):\n",
        "    \"\"\"Forward attention module.\n",
        "\n",
        "    Args:\n",
        "      q: (batch_size, query_tokens, embedding_dim)\n",
        "      k: (batch_size, key_tokens, embedding_dim)\n",
        "      v: (batch_size, key_tokens, embedding_dim)\n",
        "    Returns:\n",
        "      out: (batch_size, query_tokens, embedding_dim)\n",
        "    \"\"\"\n",
        "    # Input projections\n",
        "    q = self.q_proj(q)\n",
        "    k = self.k_proj(k)\n",
        "    v = self.v_proj(v)\n",
        "\n",
        "    # Separate into heads\n",
        "    q = self._separate_heads(q)  # (batch_size, num_heads, n, c_per_head)\n",
        "    k = self._separate_heads(k)  # (batch_size, num_heads, m, c_per_head)\n",
        "    v = self._separate_heads(v)  # (batch_size, num_heads, m, c_per_head)\n",
        "\n",
        "    # Attention\n",
        "    _, _, _, c_per_head = q.shape\n",
        "    attn = jnp.matmul(\n",
        "        q, k.transpose((0, 1, 3, 2)))  # B x N_heads x N_tokens x N_tokens\n",
        "    attn = attn / math.sqrt(c_per_head)\n",
        "    attn = nn.softmax(attn, axis=-1)\n",
        "\n",
        "    # Get output\n",
        "    out = jnp.matmul(attn, v)\n",
        "    out = self._recombine_heads(out)\n",
        "    out = self.out_proj(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "  embedding_dim: int\n",
        "  mlp_dim: int\n",
        "  activation: Any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(self.mlp_dim, name='lin1')(x)\n",
        "    x = self.activation(x)\n",
        "    x = nn.Dense(self.embedding_dim, name='lin2')(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "mTlFjcegtvXo",
        "cellView": "form"
      },
      "id": "mTlFjcegtvXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax mask decoder.\n",
        "r\"\"\"Sam mask decoder.\n",
        "\n",
        "Pytorch reference:\n",
        "\n",
        "https://github.com/facebookresearch/segment-anything/blob/HEAD/\\\n",
        "segment_anything/modeling/mask_decoder.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "# from scenic.projects.segment_anything.modeling import transformer\n",
        "\n",
        "\n",
        "class MaskDecoder(nn.Module):\n",
        "  \"\"\"Sam mask decoder.\"\"\"\n",
        "\n",
        "  transformer_dim: int = 256\n",
        "  num_multimask_outputs: int = 3\n",
        "  iou_head_depth: int = 3\n",
        "  iou_head_hidden_dim: int = 256\n",
        "\n",
        "  def setup(self):\n",
        "    self.iou_token = self.param(\n",
        "        'iou_token.weight',\n",
        "        nn.initializers.normal(stddev=1.),\n",
        "        (1, self.transformer_dim))\n",
        "    self.mask_tokens = self.param(\n",
        "        'mask_tokens.weight',\n",
        "        nn.initializers.normal(stddev=1.),\n",
        "        (self.num_multimask_outputs + 1, self.transformer_dim))\n",
        "    self.output_upscaling = OutputScaling(\n",
        "        transformer_dim=self.transformer_dim, name='output_upscaling')\n",
        "\n",
        "    self.output_hypernework_mlps = [\n",
        "        MLP(hidden_dim=self.iou_head_hidden_dim,\n",
        "            output_dim=self.transformer_dim // 8, num_layers=3,\n",
        "            name=f'output_hypernetworks_mlps.{i}',\n",
        "           ) for i in range(self.num_multimask_outputs + 1)]\n",
        "\n",
        "    self.iou_prediction_head = MLP(\n",
        "        hidden_dim=self.iou_head_hidden_dim,\n",
        "        output_dim=self.num_multimask_outputs + 1,\n",
        "        num_layers=self.iou_head_depth,\n",
        "        name='iou_prediction_head')\n",
        "\n",
        "    self.transformer = TwoWayTransformer(name='transformer')\n",
        "\n",
        "  def predict_masks(\n",
        "      self, image_embeddings, image_pe,\n",
        "      sparse_prompt_embeddings, dense_prompt_embeddings):\n",
        "    \"\"\"Predict masks for a single image.\n",
        "\n",
        "    Args:\n",
        "      image_embeddings: (H, W, embed_dim)\n",
        "      image_pe: (H, W, embed_dim)\n",
        "      sparse_prompt_embeddings: (num_prompts, num_points, embed_dim)\n",
        "      dense_prompt_embeddings: (num_prompts, H, W, embed_dim)\n",
        "    Returns:\n",
        "      masks: (num_prompts, num_multimask_outputs + 1, h', w')\n",
        "      iou_pred: (num_prompts, num_multimask_outputs + 1)\n",
        "    \"\"\"\n",
        "    output_tokens = jnp.concatenate(\n",
        "        [self.iou_token, self.mask_tokens],\n",
        "        axis=0)  # (num_multimask_outputs + 2, transformer_dim)\n",
        "    num_prompts = sparse_prompt_embeddings.shape[0]\n",
        "    output_tokens = jnp.broadcast_to(\n",
        "        output_tokens[None],\n",
        "        (num_prompts, self.num_multimask_outputs + 2, self.transformer_dim))\n",
        "    tokens = jnp.concatenate(\n",
        "        [output_tokens, sparse_prompt_embeddings], axis=1,\n",
        "    )  # (num_prompts, num_multimask_outputs + 2 + num_points, embed_dim)\n",
        "\n",
        "    src = jnp.repeat(\n",
        "        image_embeddings[None], tokens.shape[0],\n",
        "        axis=0)  # (num_prompts, H, W, D)\n",
        "    src = src + dense_prompt_embeddings\n",
        "    pos_src = jnp.repeat(\n",
        "        image_pe[None], tokens.shape[0], axis=0)  # (num_prompts, H, W, D)\n",
        "    num_prompts, h, w, d = src.shape\n",
        "\n",
        "    hs, src = self.transformer(src, pos_src, tokens)\n",
        "    iou_token_out = hs[:, 0, :]\n",
        "    mask_tokens_out = hs[:, 1: (1 + self.num_multimask_outputs + 1), :]\n",
        "\n",
        "    src = src.reshape(num_prompts, h, w, d)\n",
        "    upscaled_embedding = self.output_upscaling(src)  # (num_prompts, h', w', d)\n",
        "    hyper_in_list = []\n",
        "    for i in range(self.num_multimask_outputs + 1):\n",
        "      hyper_in_list.append(\n",
        "          self.output_hypernework_mlps[i](\n",
        "              mask_tokens_out[:, i, :])  # (num_prompts, d)\n",
        "      )\n",
        "    hyper_in = jnp.stack(hyper_in_list, axis=1)  # (num_prompts, num_masks, d)\n",
        "    num_prompts, h, w, d = upscaled_embedding.shape\n",
        "    masks = hyper_in @ upscaled_embedding.reshape(\n",
        "        num_prompts, h * w, d).transpose(\n",
        "            0, 2, 1)  # (num_prompts, num_masks, h'w')\n",
        "    masks = masks.reshape(num_prompts, self.num_multimask_outputs + 1, h, w)\n",
        "\n",
        "    iou_pred = self.iou_prediction_head(iou_token_out)\n",
        "    return masks, iou_pred\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, image_embeddings, image_pe,\n",
        "      sparse_prompt_embeddings, dense_prompt_embeddings,\n",
        "      multimask_output: bool = True):\n",
        "    \"\"\"Forward model for a single image.\n",
        "\n",
        "    Args:\n",
        "      image_embeddings: (H, W, 3)\n",
        "      image_pe: (H, W, D)\n",
        "      sparse_prompt_embeddings: (num_prompts, num_points, embed_dim)\n",
        "      dense_prompt_embeddings: (num_prompts, H, W, embed_dim)\n",
        "      multimask_output: bool\n",
        "    Returns:\n",
        "      masks: (num_prompts, num_multimask_outputs, h', w'),\n",
        "        num_multimask_outputs = 3 if multimask_output is True, otherwise 1.\n",
        "      iou_pred: (num_prompts, num_multimask_outputs)\n",
        "    \"\"\"\n",
        "    masks, iou_pred = self.predict_masks(\n",
        "        image_embeddings=image_embeddings,\n",
        "        image_pe=image_pe,\n",
        "        sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
        "        dense_prompt_embeddings=dense_prompt_embeddings,\n",
        "    )\n",
        "    if multimask_output:\n",
        "      return masks[:, 1:], iou_pred[:, 1:]\n",
        "    else:\n",
        "      return masks[:, :1], iou_pred[:, :1]\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  hidden_dim: int\n",
        "  output_dim: int\n",
        "  num_layers: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    for i in range(self.num_layers - 1):\n",
        "      x = nn.Dense(self.hidden_dim, name=f'layers.{i}')(x)\n",
        "      x = nn.relu(x)\n",
        "    x = nn.Dense(self.output_dim, name=f'layers.{self.num_layers - 1}')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class OutputScaling(nn.Module):\n",
        "  \"\"\"Output scaling.\"\"\"\n",
        "  transformer_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.ConvTranspose(\n",
        "        self.transformer_dim // 4, kernel_size=(2, 2), strides=(2, 2),\n",
        "        transpose_kernel=True,\n",
        "        name='0')(x)\n",
        "    x = nn.LayerNorm(name='1')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    x = nn.ConvTranspose(\n",
        "        self.transformer_dim // 8, kernel_size=(2, 2), strides=(2, 2),\n",
        "        transpose_kernel=True,\n",
        "        name='3')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "MkaZQaGyuItk"
      },
      "id": "MkaZQaGyuItk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax prompt-encoder\n",
        "r\"\"\"Sam prompt encoder.\n",
        "\n",
        "Pytorch reference:\n",
        "\n",
        "https://github.com/facebookresearch/segment-anything/blob/HEAD/\\\n",
        "segment_anything/modeling/prompt_encoder.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "class PromptEncoder(nn.Module):\n",
        "  \"\"\"Sam prompt encoder for points and boxes.\"\"\"\n",
        "\n",
        "  embed_dim: int = 256\n",
        "  image_embedding_size: Tuple[int, int] = (1024 // 16, 1024 // 16)\n",
        "  input_image_size: Tuple[int, int] = (1024, 1024)\n",
        "  num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
        "  mask_in_chans: int = 16\n",
        "\n",
        "  def setup(self):\n",
        "    self.pe_layer = PositionEmbeddingRandom(\n",
        "        self.embed_dim // 2, name='pe_layer')\n",
        "    point_embeddings = []\n",
        "    # TODO(zhouxy): check if `nn.initializers.normal(stddev=1.)` is the same as\n",
        "    # pytorch nn.Embedding default initialization.\n",
        "    for i in range(self.num_point_embeddings):\n",
        "      point_embeddings.append(self.param(\n",
        "          f'point_embeddings.{i}.weight',\n",
        "          nn.initializers.normal(stddev=1.),\n",
        "          (1, self.embed_dim)))\n",
        "    self.point_embeddings = point_embeddings\n",
        "    del point_embeddings\n",
        "    self.not_a_point_embed = self.param(\n",
        "        'not_a_point_embed.weight',\n",
        "        nn.initializers.normal(stddev=1.),\n",
        "        (1, self.embed_dim))\n",
        "    self.no_mask_embed = self.param(\n",
        "        'no_mask_embed.weight',\n",
        "        nn.initializers.normal(stddev=1.),\n",
        "        (1, self.embed_dim))\n",
        "    self.mask_downscaling = MaskDownScaling(\n",
        "        mask_in_chans=self.mask_in_chans, embed_dim=self.embed_dim,\n",
        "        name='mask_downscaling')\n",
        "\n",
        "  def get_dense_pe(self):\n",
        "    return self.pe_layer(self.image_embedding_size)\n",
        "\n",
        "  def _embed_points(self, points, labels, pad):\n",
        "    \"\"\"Embed points.\n",
        "\n",
        "    Args:\n",
        "      points: (num_prompts, num_points, 2). In absolute coordinates.\n",
        "      labels: (num_prompts, num_points)\n",
        "      pad: bool\n",
        "    Returns:\n",
        "      point_embeddings: (num_prompts, num_points, embed_dim)\n",
        "    \"\"\"\n",
        "    # Shift to center of pixel following:\n",
        "    # https://github.com/facebookresearch/segment-anything/blob/main/\\\n",
        "    # segment_anything/modeling/prompt_encoder.py#L80\n",
        "    points = points + 0.5\n",
        "    if pad:\n",
        "      padding_point = jnp.zeros((points.shape[0], 1, 2), dtype=jnp.float32)\n",
        "      padding_label = - jnp.ones((labels.shape[0], 1), dtype=jnp.float32)\n",
        "      points = jnp.concatenate([points, padding_point], axis=1)\n",
        "      labels = jnp.concatenate([labels, padding_label], axis=1)\n",
        "    point_embedding = self.pe_layer.forward_with_coords(\n",
        "        points, self.input_image_size)  # (num_prompts, num_points, embed_dim)\n",
        "    ignored_points = labels[..., None] == -1  # (num_prompts, num_points, 1)\n",
        "    point_embedding = point_embedding * (1 - ignored_points) + (\n",
        "        self.not_a_point_embed[None] * ignored_points)\n",
        "    neg_points = labels[..., None] == 0  # (num_prompts, num_points, 1)\n",
        "    point_embedding += neg_points * self.point_embeddings[0][None]\n",
        "    pos_points = labels[..., None] == 1  # (num_prompts, num_points, 1)\n",
        "    point_embedding += pos_points * self.point_embeddings[1][None]\n",
        "    return point_embedding\n",
        "\n",
        "  def _embed_boxes(self, boxes):\n",
        "    boxes = boxes + 0.5\n",
        "    coords = boxes.reshape(-1, 2, 2)\n",
        "    corner_embedding = self.pe_layer.forward_with_coords(\n",
        "        coords, self.input_image_size)  # (num_prompts, 2, embed_dim)\n",
        "    lt_emb = corner_embedding[:, 0, :] + self.point_embeddings[2]\n",
        "    rb_emb = corner_embedding[:, 1, :] + self.point_embeddings[3]\n",
        "    corner_embedding = jnp.stack(\n",
        "        [lt_emb, rb_emb], axis=1)  # (num_prompts, 2, embed_dim)\n",
        "    return corner_embedding\n",
        "\n",
        "  def _embed_masks(self, masks):\n",
        "    mask_embedding = self.mask_downscaling(masks)\n",
        "    return mask_embedding\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, points, point_labels, boxes=None, masks=None):\n",
        "    \"\"\"Forward pass. Currently only supports points prompt.\n",
        "\n",
        "    Args:\n",
        "      points: (num_prompts, num_points, 2)\n",
        "      point_labels: (num_prompts, num_points): labels of each point. 1 means\n",
        "        positive points, 0 means negative points (shouldn't be included in the\n",
        "        mask), and -1 means padded/ ignored points.\n",
        "      boxes: (num_prompts, 4) or None\n",
        "      masks: (num_prompts, height, width) or None\n",
        "    Returns:\n",
        "      point_embeddings: (num_prompts, num_points, embed_dim)\n",
        "      dense_embeddings: (num_prompts, H, W, embed_dim)\n",
        "    \"\"\"\n",
        "    num_prompts = points.shape[0] if points is not None else (\n",
        "        boxes.shape[0] if boxes is not None else masks.shape[0])\n",
        "    sparse_embeddings = jnp.zeros(\n",
        "        (num_prompts, 0, self.embed_dim), dtype=jnp.float32)\n",
        "    if points is not None:\n",
        "      assert boxes is None\n",
        "      point_embeddings = self._embed_points(\n",
        "          points, point_labels, pad=(boxes is None))\n",
        "      sparse_embeddings = point_embeddings\n",
        "    if boxes is not None:\n",
        "      assert points is None\n",
        "      box_embeddings = self._embed_boxes(boxes)\n",
        "      sparse_embeddings = box_embeddings\n",
        "    if masks is not None:\n",
        "      dense_embeddings = self._embed_masks(masks)\n",
        "    else:\n",
        "      dense_embeddings = jnp.broadcast_to(\n",
        "          self.no_mask_embed[:, None, None],\n",
        "          (num_prompts, self.image_embedding_size[0],\n",
        "           self.image_embedding_size[1], self.embed_dim,)\n",
        "      )\n",
        "    return sparse_embeddings, dense_embeddings\n",
        "\n",
        "\n",
        "class PositionEmbeddingRandom(nn.Module):\n",
        "  \"\"\"Positional encoding using random spatial frequencies.\"\"\"\n",
        "\n",
        "  num_pos_feats: int\n",
        "  scale: Optional[float] = None\n",
        "\n",
        "  def setup(self):\n",
        "    scale = 1.0 if self.scale is None or self.scale <= 0.0 else self.scale\n",
        "    self.positional_encoding_gaussian_matrix = self.param(\n",
        "        'positional_encoding_gaussian_matrix',\n",
        "        nn.initializers.normal(stddev=scale),\n",
        "        (2, self.num_pos_feats)\n",
        "    )\n",
        "\n",
        "  def _pe_encoding(self, coords):\n",
        "    \"\"\"PE encoding.\"\"\"\n",
        "    # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
        "    coords = 2 * coords - 1\n",
        "    coords = coords @ jax.lax.stop_gradient(\n",
        "        self.positional_encoding_gaussian_matrix)\n",
        "    coords = 2 * jnp.pi * coords\n",
        "    # outputs d_1 x ... x d_n x C shape\n",
        "    return jnp.concatenate([jnp.sin(coords), jnp.cos(coords)], axis=-1)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, size):\n",
        "    \"\"\"Forward pass.\n",
        "\n",
        "    Args:\n",
        "      size: 2\n",
        "    Returns:\n",
        "      pe: H x W x D\n",
        "    \"\"\"\n",
        "    h, w = size\n",
        "    grid = jnp.ones((h, w), dtype=jnp.float32)\n",
        "    y_embed = jnp.cumsum(grid, axis=0) - 0.5\n",
        "    x_embed = jnp.cumsum(grid, axis=1) - 0.5\n",
        "    y_embed = y_embed / h\n",
        "    x_embed = x_embed / w\n",
        "    pe = self._pe_encoding(jnp.stack([x_embed, y_embed], axis=-1))\n",
        "    return pe\n",
        "\n",
        "  def forward_with_coords(self, coords_input, image_size):\n",
        "    \"\"\"Forward with points.\n",
        "\n",
        "    Args:\n",
        "      coords_input: (num_prompts, num_points, 2)\n",
        "      image_size: (2,)\n",
        "    Returns:\n",
        "      embedding: (num_prompts, num_points, self.num_pos_feats * 2)\n",
        "    \"\"\"\n",
        "    x = coords_input[:, :, 0] / image_size[1]\n",
        "    y = coords_input[:, :, 1] / image_size[0]\n",
        "    return self._pe_encoding(jnp.stack([x, y], axis=-1))\n",
        "\n",
        "\n",
        "class MaskDownScaling(nn.Module):\n",
        "  \"\"\"Mask downscaling.\"\"\"\n",
        "  mask_in_chans: int = 16\n",
        "  embed_dim: int = 256\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Conv(\n",
        "        self.mask_in_chans // 4, kernel_size=(2, 2), strides=(2, 2),\n",
        "        name='0')(x)\n",
        "    x = nn.LayerNorm(name='1')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    x = nn.Conv(\n",
        "        self.mask_in_chans, kernel_size=(2, 2), strides=(2, 2),\n",
        "        name='3')(x)\n",
        "    x = nn.LayerNorm(name='4')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    x = nn.Conv(\n",
        "        self.embed_dim, kernel_size=(1, 1), strides=(1, 1),\n",
        "        name='6')(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "twkefuPIuaLv",
        "cellView": "form"
      },
      "id": "twkefuPIuaLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Util functions\n",
        "\"\"\"Util functions for Segment Anything models.\"\"\"\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "# from scenic.projects.segment_anything.modeling import nms as nms_lib\n",
        "\n",
        "\n",
        "def build_point_grid(points_per_side):\n",
        "  \"\"\"Generates a 2D grid of points evenly spaced in [0, 1] x [0, 1].\"\"\"\n",
        "  offset = 1. / (2 * points_per_side)\n",
        "  points_one_side = jnp.linspace(offset, 1 - offset, points_per_side)\n",
        "  points_x = jnp.tile(points_one_side[None, :], (points_per_side, 1))\n",
        "  points_y = jnp.tile(points_one_side[:, None], (1, points_per_side))\n",
        "  points = jnp.stack([points_x, points_y], axis=-1).reshape(-1, 2)\n",
        "  return points  # (points_per_side ** 2, 1)\n",
        "\n",
        "\n",
        "def batched_mask_to_box(masks):\n",
        "  \"\"\"Convert binary masks in (n, h, w) to boxes (n, 4).\"\"\"\n",
        "  if masks.shape[0] == 0:\n",
        "    return jnp.zeros((0, 4), dtype=jnp.float32)\n",
        "\n",
        "  h, w = masks.shape[-2:]\n",
        "  in_height = jnp.max(masks, axis=-1)  # (n, h)\n",
        "  in_height_coords = in_height * jnp.arange(h)[None]  # (n, h)\n",
        "  bottom_edges = jnp.max(in_height_coords, axis=-1)  # (n, )\n",
        "  # Mark \"0\" as \"h\" so that we can take min.\n",
        "  in_height_coords = in_height_coords + h * (1 - in_height)  # (n, h)\n",
        "  top_edges = jnp.min(in_height_coords, axis=-1)  # (n,)\n",
        "\n",
        "  in_width = jnp.max(masks, axis=-2)  # (n, w)\n",
        "  in_width_coords = in_width * jnp.arange(w)[None]  # (n, w)\n",
        "  right_edges = jnp.max(in_width_coords, axis=-1)  # (n,)\n",
        "  in_width_coords = in_width_coords + w * (1 - in_width)  # (n, w)\n",
        "  left_edges = jnp.min(in_width_coords, axis=-1)\n",
        "\n",
        "  # mark empty mask as [0, 0, 0, 0]\n",
        "  empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)\n",
        "  out = jnp.stack(\n",
        "      [left_edges, top_edges, right_edges, bottom_edges], axis=-1)  # (n, 4)\n",
        "  out = out * (1 - empty_filter)[:, None]\n",
        "  return out\n",
        "\n",
        "\n",
        "def batched_mask_to_box_np(masks):\n",
        "  \"\"\"Convert binary masks in (n, h, w) to boxes (n, 4).\"\"\"\n",
        "  if masks.shape[0] == 0:\n",
        "    return np.zeros((0, 4), dtype=np.float32)\n",
        "\n",
        "  h, w = masks.shape[-2:]\n",
        "  in_height = np.max(masks, axis=-1)  # (n, h)\n",
        "  in_height_coords = in_height * np.arange(h)[None]  # (n, h)\n",
        "  bottom_edges = np.max(in_height_coords, axis=-1)  # (n, )\n",
        "  # Mark \"0\" as \"h\" so that we can take min.\n",
        "  in_height_coords = in_height_coords + h * (1 - in_height)  # (n, h)\n",
        "  top_edges = np.min(in_height_coords, axis=-1)  # (n,)\n",
        "\n",
        "  in_width = np.max(masks, axis=-2)  # (n, w)\n",
        "  in_width_coords = in_width * np.arange(w)[None]  # (n, w)\n",
        "  right_edges = np.max(in_width_coords, axis=-1)  # (n,)\n",
        "  in_width_coords = in_width_coords + w * (1 - in_width)  # (n, w)\n",
        "  left_edges = np.min(in_width_coords, axis=-1)\n",
        "\n",
        "  # mark empty mask as [0, 0, 0, 0]\n",
        "  empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)\n",
        "  out = np.stack(\n",
        "      [left_edges, top_edges, right_edges, bottom_edges], axis=-1)  # (n, 4)\n",
        "  out = out * (1 - empty_filter)[:, None]\n",
        "  return out\n",
        "\n",
        "\n",
        "def calculate_stability_score(\n",
        "    mask_logits, mask_threshold, stability_score_offset):\n",
        "  \"\"\"The stability score measures if the mask changes with different thresh.\"\"\"\n",
        "  low = (mask_logits > (mask_threshold + stability_score_offset)).sum(\n",
        "      axis=-1).sum(axis=-1)\n",
        "  high = (mask_logits > (mask_threshold - stability_score_offset)).sum(\n",
        "      axis=-1).sum(axis=-1)\n",
        "  return low / high\n",
        "\n",
        "\n",
        "def nms(boxes, scores, iou_threshold, num_outputs=100):\n",
        "  _, _, keep = nms_lib.non_max_suppression_padded(\n",
        "      scores[None], boxes[None], num_outputs, iou_threshold,\n",
        "      return_idx=True)  # pytype: disable=wrong-arg-types\n",
        "  return keep[0]  # undo batch\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2NAKAK-DqQnQ"
      },
      "id": "2NAKAK-DqQnQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax SAM model\n",
        "r\"\"\"Segment Anything Model.\n",
        "\n",
        "Pytorch reference:\n",
        "\n",
        "https://github.com/facebookresearch/segment-anything/blob/HEAD/\\\n",
        "segment_anything/modeling/sam.py\n",
        "\n",
        "\"\"\"\n",
        "from typing import Any\n",
        "\n",
        "from flax import linen as nn\n",
        "# import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "import dataclasses\n",
        "# from scenic.projects.segment_anything.modeling.image_encoder import ImageEncoderViT\n",
        "# from scenic.projects.segment_anything.modeling.mask_decoder import MaskDecoder\n",
        "# from scenic.projects.segment_anything.modeling.prompt_encoder import PromptEncoder\n",
        "\n",
        "PIXEL_MEAN = (123.675, 116.28, 103.53)\n",
        "PIXEL_STD = (58.395, 57.12, 57.375)\n",
        "\n",
        "class Sam(nn.Module):\n",
        "  \"\"\"Segment anything model.\n",
        "\n",
        "  Default parameters following\n",
        "  https://github.com/facebookresearch/segment-anything/blob/main/\n",
        "  segment_anything/automatic_mask_generator.py#L35\n",
        "\n",
        "  Attributes:\n",
        "    mask_threshold: threshold to convert output logits to binary masks.\n",
        "    pixel_mean: used in preprocessing inputs.\n",
        "    pixel_std: used in preprocessing inputs.\n",
        "    max_objects: number of output objects in \"segment anything\" mode.\n",
        "    points_per_side: number of point anchors perside in \"segment anything\" mode.\n",
        "    points_per_batch: batch size for processing point anchors.\n",
        "    pred_iou_thresh: score threshold in \"segment anything\" mode.\n",
        "    box_nms_thresh: NMS threshold\n",
        "    stability_score_thresh: threshold for filtering with a stability metric.\n",
        "    stability_score_offset: used in computing the stability metric.\n",
        "    pre_nms_topk: new hyper-parameter in this implementation. Used for keeping a\n",
        "      fixed shape after filtering mask predictions.\n",
        "    image_encoder_args: args for image backbone.\n",
        "    prompt_encoder_args: args for prompt encoder.\n",
        "    mask_decoder_args: args for mask decoder.\n",
        "  \"\"\"\n",
        "  mask_threshold: float = 0.0\n",
        "  pixel_mean: Any = PIXEL_MEAN\n",
        "  pixel_std: Any = PIXEL_STD\n",
        "  max_objects: int = 100\n",
        "  points_per_side: Optional[int] = 32\n",
        "  points_per_batch: int = 64\n",
        "  pred_iou_thresh: float = 0.88\n",
        "  box_nms_thresh: float = 0.7\n",
        "  stability_score_thresh: float = 0.95\n",
        "  stability_score_offset: float = 1.0\n",
        "  pre_nms_topk: int = 1536\n",
        "  image_encoder_args: ml_collections.ConfigDict = dataclasses.field(\n",
        "      default_factory=ml_collections.ConfigDict)\n",
        "  prompt_encoder_args: ml_collections.ConfigDict = dataclasses.field(\n",
        "      default_factory=ml_collections.ConfigDict)\n",
        "  mask_decoder_args: ml_collections.ConfigDict = dataclasses.field(\n",
        "      default_factory=ml_collections.ConfigDict)\n",
        "\n",
        "  def setup(self):\n",
        "    # pylint: disable=not-a-mapping\n",
        "    self.image_encoder = ImageEncoderViT(\n",
        "        **self.image_encoder_args, name='image_encoder')\n",
        "    self.prompt_encoder = PromptEncoder(\n",
        "        **self.prompt_encoder_args, name='prompt_encoder')\n",
        "    self.mask_decoder = MaskDecoder(\n",
        "        **self.mask_decoder_args, name='mask_decoder')\n",
        "    # pylint: enable=not-a-mapping\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, image, point_coords, point_labels, padding_mask=None,\n",
        "      image_embeddings=None, boxes=None, mask_inputs=None,\n",
        "      multimask_output: bool = True, return_image_embedding: bool = False,\n",
        "      upsample_mask: bool = True, return_batch_as_list: bool = True,\n",
        "      train: bool = False, debug: bool = False):\n",
        "    \"\"\"Forward Sam model.\n",
        "\n",
        "    Args:\n",
        "      image: (batch_size, H, W, 3). Input pixels in RGB values [0, 255].\n",
        "      point_coords: (batch_size, num_prompts, num_points, 2). Input point\n",
        "        prompts. In absolute range [0, image.shape[1 or 2]].\n",
        "      point_labels: (batch_size, num_prompts, num_points). 1: positive points;\n",
        "        0: negative points. -1: padded/ ignored points.\n",
        "      padding_mask: (batch_size, H, W). Indicate which pixels in the input are\n",
        "        padded. 1: not padded; 0: padded. This is used to match the pytorch\n",
        "        preprocessing process: normalize then pad, while in Jax we need to pad\n",
        "        first.\n",
        "      image_embeddings: cached image embeddings if they are provided.\n",
        "        (batch_size, H', W', D). If not provided, image must be not None.\n",
        "      boxes: (batch_size, num_prompts, 4); box prompts;\n",
        "      mask_inputs: (batch_size, num_prompts, 1, H, W); mask prompts.\n",
        "      multimask_output: bool. If false, C = 1, otherwise,\n",
        "        C = self.mask_decoder_args.num_multimask_outputs\n",
        "      return_image_embedding: bool\n",
        "      upsample_mask: bool; If False, only return the 4x downsampled masks. This\n",
        "        saves memory.\n",
        "      return_batch_as_list: If True, return a list where each item is the\n",
        "        results of a single image; If False, return a dict with batched results.\n",
        "      train: bool\n",
        "      debug: bool\n",
        "    Returns:\n",
        "      ret: a list (batch) of dicts, each with the following keys:\n",
        "        'masks': (num_prompts, C, H, W). C is the num of masks (see above).\n",
        "        'iou_predictions': (num_prompts, C). Predicted mask quality scores.\n",
        "        'low_res_logits': (num_prompts, C, H', W'). The output mask of the\n",
        "          mask decoder. The final masks are resized from this.\n",
        "    \"\"\"\n",
        "    del debug\n",
        "    msg = 'One of \"image\" or \"image_embedding\" should be provided!'\n",
        "    assert image is not None or image_embeddings is not None, msg\n",
        "    assert image is None or image_embeddings is None, msg\n",
        "    if image_embeddings is None:\n",
        "      assert image is not None\n",
        "      image_embeddings = self.get_image_embeddings(\n",
        "          image, padding_mask=padding_mask,\n",
        "          train=train)  # (batch_size, H', W', D)\n",
        "\n",
        "    ret = []\n",
        "    for b, curr_embedding in enumerate(image_embeddings):\n",
        "      curr_point_coords = point_coords[b] if point_coords is not None else None\n",
        "      curr_point_labels = point_labels[b] if point_labels is not None else None\n",
        "      box_prompt = boxes[b] if boxes is not None else None\n",
        "      mask_prompt = mask_inputs[b] if mask_inputs is not None else None\n",
        "      sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "          curr_point_coords, curr_point_labels,\n",
        "          boxes=box_prompt, masks=mask_prompt)\n",
        "      low_res_masks, iou_predictions = self.mask_decoder(\n",
        "          image_embeddings=curr_embedding,\n",
        "          image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "          sparse_prompt_embeddings=sparse_embeddings,\n",
        "          dense_prompt_embeddings=dense_embeddings,\n",
        "          multimask_output=multimask_output,\n",
        "      )\n",
        "      size = self.image_encoder.img_size\n",
        "      out = {\n",
        "          'iou_predictions': iou_predictions,\n",
        "          'low_res_logits': low_res_masks,\n",
        "      }\n",
        "      if upsample_mask:\n",
        "        masks = self.postprocess_masks(\n",
        "            low_res_masks, size, size) > self.mask_threshold\n",
        "        out['masks'] = masks\n",
        "      ret.append(out)\n",
        "    if return_image_embedding:\n",
        "      for batch_i, image_embedding in enumerate(image_embeddings):\n",
        "        ret[batch_i]['image_embedding'] = image_embedding\n",
        "    if not return_batch_as_list:\n",
        "      ret = {k: jnp.stack([ret[i][k] for i in range(len(ret))], axis=0)\n",
        "             for k in ret[0].keys()}\n",
        "    return ret\n",
        "\n",
        "  def get_image_embeddings(self, image, padding_mask=None, train=False):\n",
        "    image = self.preprocess(image, padding_mask)  # (batch_size, H, W, 3)\n",
        "    image_embeddings = self.image_encoder(\n",
        "        image, train=train)  # (batch_size, H', W', D)\n",
        "    return image_embeddings\n",
        "\n",
        "  @staticmethod\n",
        "  def postprocess_masks(masks, h, w):\n",
        "    \"\"\"Resize masks to input resolution.\"\"\"\n",
        "    masks = jax.image.resize(\n",
        "        masks, (masks.shape[0], masks.shape[1], h, w),\n",
        "        method='bilinear', antialias=False)\n",
        "    return masks\n",
        "\n",
        "  @staticmethod\n",
        "  def postprocess_to_orig(\n",
        "      lowres_masks, unpad_size, orig_size, mask_threshold=0.0):\n",
        "    \"\"\"Resize masks to input resolution.\"\"\"\n",
        "    lowres_h, lowres_w = lowres_masks.shape[1:]\n",
        "    unpad_h, unpad_w = unpad_size\n",
        "    down_ratio = max(lowres_h, lowres_w) / max(unpad_h, unpad_w)\n",
        "    h, w = int(unpad_h * down_ratio), int(unpad_w * down_ratio)\n",
        "    orig_h, orig_w = orig_size\n",
        "\n",
        "    masks = (\n",
        "        jax.image.resize(\n",
        "            jax.device_put(\n",
        "                lowres_masks[:, :h, :w],\n",
        "                device=jax.local_devices(backend='cpu')[0],\n",
        "            ),\n",
        "            (lowres_masks.shape[0], orig_h, orig_w),\n",
        "            method='bilinear',\n",
        "            antialias=False,\n",
        "        )\n",
        "        > mask_threshold\n",
        "    )\n",
        "    boxes = batched_mask_to_box_np(np.asarray(masks))\n",
        "    return masks, boxes\n",
        "\n",
        "  def preprocess(self, inputs, padding_mask=None):\n",
        "    \"\"\"Proprocess images. Normalize pixels for non-padded pixels.\"\"\"\n",
        "    mean = jnp.asarray(self.pixel_mean, dtype=jnp.float32).reshape(1, 1, 1, 3)\n",
        "    std = jnp.asarray(self.pixel_std, dtype=jnp.float32).reshape(1, 1, 1, 3)\n",
        "    inputs = (inputs - mean) / std\n",
        "    if padding_mask is not None:\n",
        "      inputs = inputs * padding_mask[..., None]  # Padded pixels remain 0\n",
        "    return inputs\n",
        "\n",
        "  def generate(\n",
        "      self, image=None, padding_mask=None, upsample_mask=True,\n",
        "      image_embedding=None, return_image_embedding=False):\n",
        "    \"\"\"Automatically generate masks for all objects.\n",
        "\n",
        "    This function is from the original SamAutomaticMaskGenerator at\n",
        "    https://github.com/facebookresearch/segment-anything/blob/HEAD/\n",
        "    segment_anything/automatic_mask_generator.py.\n",
        "\n",
        "    Here we merge it inside the Sam flax model, as we don't use a separate\n",
        "    predictor class.\n",
        "\n",
        "    Here are a few key differences compared to the original implementation:\n",
        "\n",
        "      - The original implementation did filtering inside each prompt-batch. We\n",
        "        can't do this in jax as the filtering changes the data shape. Instead,\n",
        "        we do a filtering after concatenating the raw outputs from all batches,\n",
        "        and use an additional parameter \"pre_nms_topk\" to control the output\n",
        "        shape. By default \"pre_nms_topk\" is half of all prompts.\n",
        "\n",
        "      - We move mask upsampling (i.e., \"postprocess_masks\") to the very end of\n",
        "        the process (after NMS), to save peak memory. This means the box-NMS and\n",
        "        the stability_score are computed on the 4x-downsampled masks. This\n",
        "        introduces small errors compared to the original implementation.\n",
        "\n",
        "      - We don't support the multi-crop testing in the original code as this is\n",
        "        not enabled in the default config.\n",
        "\n",
        "    Args:\n",
        "      image: a single image, (H x W x 3)\n",
        "      padding_mask: (H x W)\n",
        "      upsample_mask: bool; If False, only return the 4x downsampled masks. This\n",
        "        saves memory.\n",
        "      image_embedding: image embeddings if they are provided. (H', W', D). If\n",
        "        not provided, image must be not None.\n",
        "      return_image_embedding: bool\n",
        "    Returns:\n",
        "      Result dict of that image, with keys:\n",
        "        'masks': (self.max_objects H, W).\n",
        "        'iou_predictions': (self.max_objects,). Predicted mask quality scores.\n",
        "        'low_res_logits': (self.max_objects, H', W'). The output mask of the\n",
        "          mask decoder. The final masks are resized from this.\n",
        "        'boxes': (self.max_objects, 4). Box from the masks.\n",
        "        'stability_score': (stability_score,). A measurement of how stable the\n",
        "          mask is when self.mask_threshold changes.\n",
        "    \"\"\"\n",
        "    msg = 'One of \"image\" or \"image_embedding\" should be provided!'\n",
        "    assert image is not None or image_embedding is not None, msg\n",
        "    assert image is None or image_embedding is None, msg\n",
        "    if image_embedding is None:\n",
        "      padding_mask = padding_mask if padding_mask is not None else (\n",
        "          jnp.ones((image.shape[0], image.shape[1]), dtype=jnp.float32))\n",
        "      image_embedding = self.get_image_embeddings(\n",
        "          image[None], padding_mask=padding_mask[None])[0]  # (H', W', D)\n",
        "    else:\n",
        "      nopadding_msg = 'Padding_mask should be provided if using image_embedding'\n",
        "      assert padding_mask is not None, nopadding_msg\n",
        "\n",
        "    point_grid = build_point_grid(\n",
        "        self.points_per_side)[:, None]  # (points_per_side ** 2, 1, 2)\n",
        "    # Ignore padded region in creating grid.\n",
        "    valid_h = padding_mask.max(axis=1).sum()\n",
        "    valid_w = padding_mask.max(axis=0).sum()\n",
        "    point_grid = point_grid * jnp.asarray(\n",
        "        [valid_w, valid_h], dtype=jnp.float32).reshape(1, 1, 2)\n",
        "    point_labels = jnp.ones(\n",
        "        (point_grid.shape[0], point_grid.shape[1]),\n",
        "        dtype=jnp.int32)  # (points_per_side ** 2, 1)\n",
        "\n",
        "    num_prompts = point_grid.shape[0]\n",
        "    bs = self.points_per_batch\n",
        "    assert num_prompts % bs == 0, num_prompts\n",
        "    num_batches = num_prompts // bs\n",
        "    low_res_masks, iou_predictions = [], []\n",
        "    for b in range(num_batches):\n",
        "      in_points = point_grid[b * bs: (b + 1) * bs]\n",
        "      in_labels = point_labels[b * bs: (b + 1) * bs]\n",
        "      sparse_embeddings_cur, dense_embeddings_cur = self.prompt_encoder(\n",
        "          in_points, in_labels)\n",
        "      low_res_masks_cur, iou_predictions_cur = self.mask_decoder(\n",
        "          image_embeddings=image_embedding,\n",
        "          image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "          sparse_prompt_embeddings=sparse_embeddings_cur,\n",
        "          dense_prompt_embeddings=dense_embeddings_cur,\n",
        "          multimask_output=True,\n",
        "      )  # low_res_masks: (bs, 3, h', w')\n",
        "      low_res_masks.append(low_res_masks_cur)\n",
        "      iou_predictions.append(iou_predictions_cur)\n",
        "    ret = {}\n",
        "    if return_image_embedding:\n",
        "      ret['image_embedding'] = image_embedding\n",
        "    del image_embedding\n",
        "\n",
        "    low_res_masks = jnp.concatenate(\n",
        "        low_res_masks, axis=0)\n",
        "    iou_predictions = jnp.concatenate(iou_predictions, axis=0)\n",
        "    low_res_masks = low_res_masks.reshape(\n",
        "        (-1,) + low_res_masks.shape[-2:])  # (points_per_side ** 2 * 3, h', w')\n",
        "    iou_predictions = iou_predictions.reshape(-1)  # (points_per_side ** 2 * 3,)\n",
        "    keep_mask = iou_predictions > self.pred_iou_thresh\n",
        "\n",
        "    # Note: the original code computes stability_score on upsampled masks.\n",
        "    stability_score = calculate_stability_score(\n",
        "        low_res_masks,\n",
        "        self.mask_threshold, self.stability_score_offset)\n",
        "    if self.stability_score_thresh > 0.0:\n",
        "      keep_mask = keep_mask & (stability_score > self.stability_score_thresh)\n",
        "\n",
        "    iou_predictions = iou_predictions * keep_mask\n",
        "\n",
        "    _, inds = jax.lax.top_k(iou_predictions, k=self.pre_nms_topk)\n",
        "    iou_predictions = jnp.take_along_axis(iou_predictions, inds, axis=0)\n",
        "    low_res_masks = jnp.take_along_axis(\n",
        "        low_res_masks, inds[:, None, None], axis=0)\n",
        "\n",
        "    # Note: the original code run NMS on upsampled masks.\n",
        "    low_res_boxes = batched_mask_to_box(\n",
        "        low_res_masks > self.mask_threshold)\n",
        "    keep_inds = nms(\n",
        "        low_res_boxes, iou_predictions,\n",
        "        iou_threshold=self.box_nms_thresh,\n",
        "        num_outputs=self.max_objects)  # (max_objects,)\n",
        "    low_res_masks = jnp.take_along_axis(\n",
        "        low_res_masks, keep_inds[:, None, None], axis=0)\n",
        "    ret.update({\n",
        "        'iou_predictions': jnp.take_along_axis(\n",
        "            iou_predictions, keep_inds, axis=0),\n",
        "        'low_res_logits': low_res_masks,\n",
        "        'low_res_boxes': jnp.take_along_axis(\n",
        "            low_res_boxes, keep_inds[:, None], axis=0),\n",
        "        'stability_score': jnp.take_along_axis(\n",
        "            stability_score, keep_inds, axis=0),\n",
        "    })\n",
        "    if upsample_mask:\n",
        "      size = self.image_encoder.img_size\n",
        "      masks = self.postprocess_masks(\n",
        "          low_res_masks[None], size, size)[0] > self.mask_threshold\n",
        "      boxes = batched_mask_to_box(masks)\n",
        "      ret['masks'] = masks\n",
        "      ret['boxes'] = boxes\n",
        "    return ret\n",
        "\n",
        "  def batch_generate(self, image, padding_mask, upsample_mask=True):\n",
        "    return jax.vmap(lambda x, y: self.generate(x, y, upsample_mask))(\n",
        "        image, padding_mask)"
      ],
      "metadata": {
        "id": "G14IctsEucTC"
      },
      "id": "G14IctsEucTC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_model = Sam()"
      ],
      "metadata": {
        "id": "Uj0XtiLfrVP8"
      },
      "id": "Uj0XtiLfrVP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = {'dropout': jax.random.PRNGKey(0), 'params': jax.random.PRNGKey(0)}\n",
        "S = 1024\n",
        "num_prompts, num_points = 1, 1\n",
        "inp = jax.random.normal(jax.random.PRNGKey(0), (1, S, S, 3))\n",
        "mask_inputs = jax.random.normal(jax.random.PRNGKey(0), (1, S // 4, S // 4, 1))\n",
        "point_coords = jnp.zeros((1, num_prompts, num_points, 2), jnp.float32)\n",
        "point_labels = jnp.zeros((1, num_prompts, num_points), jnp.int32)\n",
        "sam_vars = sam_model.init(\n",
        "    rng, inp, point_coords, point_labels, padding_mask=None,\n",
        "      image_embeddings=None, boxes=None, mask_inputs=mask_inputs)"
      ],
      "metadata": {
        "id": "t4drZJNvunYc"
      },
      "id": "t4drZJNvunYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flax"
      ],
      "metadata": {
        "id": "ZGNK6hiaZRkX"
      },
      "id": "ZGNK6hiaZRkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "flattened_tree = flax.traverse_util.flatten_dict(sam_vars['params'], sep='.')\n",
        "table = []\n",
        "num_params = 0\n",
        "for k in sorted(flattened_tree):\n",
        "  v = flattened_tree[k]\n",
        "  table.append((k, f'{v.shape}', f'{v.mean():.3f}', f'{v.std():.3f}'))\n",
        "  num_params += jnp.prod(jnp.asarray(v.shape))\n",
        "table_str = tabulate(\n",
        "    table, tablefmt=\"pipe\", headers=[\"Names\", \"shape\", \"mean\", \"std\"])\n",
        "print(table_str)\n",
        "print('num_params', num_params)"
      ],
      "metadata": {
        "id": "ixCqhbu9wnao"
      },
      "id": "ixCqhbu9wnao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs(k, v, converted_torch_weight):\n",
        "  \"\"\"Recursively match weights.\"\"\"\n",
        "  if isinstance(v, jnp.ndarray):\n",
        "    if k in converted_torch_weight:\n",
        "      torch_data = converted_torch_weight[k]\n",
        "      if len(v.shape) == 2 and 'not_a_point_embed' not in k and \\\n",
        "      'positional_encoding_gaussian_matrix' not in k and \\\n",
        "      'rel_pos' not in k and \\\n",
        "      'point_embeddings' not in k and \\\n",
        "      'iou_token' not in k and \\\n",
        "      'mask_tokens' not in k and\\\n",
        "      'no_mask_embed' not in k:\n",
        "        torch_data = np.transpose(torch_data, (1, 0))\n",
        "      if len(v.shape) == 4:\n",
        "        if 'output_upscaling' in k:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 1, 0))\n",
        "        elif 'image_encoder.pos_embed' in k:\n",
        "          torch_data = torch_data\n",
        "        else:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 1, 0))\n",
        "      if torch_data.shape != v.shape:\n",
        "        print('Wrong shape! {} {} {}'.format(\n",
        "            k, torch_data.shape, v.shape))\n",
        "    else:\n",
        "      print(f'{k} not in checkpoint')\n",
        "      torch_data = v\n",
        "    return [(k, torch_data.shape)], torch_data\n",
        "  lst, tree = [], {}\n",
        "  for kk, vv in v.items():\n",
        "    if isinstance(vv, jnp.ndarray) and (\n",
        "        kk == 'kernel' or kk == 'scale' or kk == 'embedding'):\n",
        "      if 'proposal_generator.scales' not in k:\n",
        "        new_kk = 'weight'\n",
        "      else:\n",
        "        new_kk = kk\n",
        "    else:\n",
        "      new_kk = kk\n",
        "    sub_lst, sub_tree = dfs(\n",
        "        '{}.{}'.format(k, new_kk) if k else new_kk,\n",
        "        vv,\n",
        "        converted_torch_weight)\n",
        "    lst.extend(sub_lst)\n",
        "    tree[kk] = sub_tree\n",
        "  return lst, tree\n",
        "\n",
        "COMMEN_NAME_MAP = []\n",
        "\n",
        "def map_names(state_dict, name_map):\n",
        "  \"\"\"Change names according to a pre-defined map.\"\"\"\n",
        "  ret = {}\n",
        "  for k, v in state_dict.items():\n",
        "    new_k = k\n",
        "    for ori_name, new_name in name_map:\n",
        "      new_k = new_k.replace(ori_name, new_name)\n",
        "    ret[new_k] = v\n",
        "  return ret\n",
        "\n",
        "converted_torch_weight = {\n",
        "    k: v for k, v in torch_weights.items()}\n",
        "converted_torch_weight = {k: v.cpu().numpy() for k, v in converted_torch_weight.items()}"
      ],
      "metadata": {
        "id": "BrPPrgOoZSqd"
      },
      "id": "BrPPrgOoZSqd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret, tree = dfs('', sam_vars['params'], converted_torch_weight)\n",
        "num_params = 0\n",
        "for k, v in converted_torch_weight.items():\n",
        "  num_params += np.prod(v.shape)\n",
        "print('#params in loaded model:', num_params)\n",
        "num_params = 0\n",
        "for k, v in ret:\n",
        "  num_params += np.prod(v)\n",
        "print('#params in converted model:', num_params)"
      ],
      "metadata": {
        "id": "-6gZAg93XcAH"
      },
      "id": "-6gZAg93XcAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "flattened_tree = flax.traverse_util.flatten_dict(tree, sep='.')\n",
        "table = []\n",
        "num_params = 0\n",
        "for k in sorted(flattened_tree):\n",
        "  v = flattened_tree[k]\n",
        "  table.append((k, f'{v.shape}', f'{v.mean():.3f}', f'{v.std():.3f}'))\n",
        "  num_params += jnp.prod(jnp.asarray(v.shape))\n",
        "table_str = tabulate(\n",
        "    table, tablefmt=\"pipe\", headers=[\"Names\", \"shape\", \"mean\", \"std\"])\n",
        "print(table_str)\n",
        "print('num_params', num_params)"
      ],
      "metadata": {
        "id": "ZkPVY-9AXhvo"
      },
      "id": "ZkPVY-9AXhvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_image = predictor.transformed_image.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "inp = np.zeros((1, S, S, 3), np.float32)\n",
        "padding_mask = np.zeros((1, S, S), np.float32)\n",
        "inp[0, :transformed_image.shape[1], :transformed_image.shape[2]] = transformed_image#[..., ::-1]\n",
        "padding_mask[0, :transformed_image.shape[1], :transformed_image.shape[2]] = 1\n",
        "point_coords = np.asarray(input_point.copy(), dtype=np.float32).reshape(1, 1, 1, 2) # jnp.zeros((1, num_prompts, num_points, 2), jnp.float32)\n",
        "point_coords[..., 0] = point_coords[..., 0] / max(image.shape[:2]) * S\n",
        "point_coords[..., 1] = point_coords[..., 1] / max(image.shape[:2]) * S\n",
        "point_labels = jnp.asarray(input_label).reshape(1, 1, 1) # jnp.zeros((1, num_prompts, num_points), jnp.int32)"
      ],
      "metadata": {
        "id": "MrYH4H-xaemA"
      },
      "id": "MrYH4H-xaemA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret = sam_model.apply(\n",
        "    {'params': tree},\n",
        "    inp,\n",
        "    point_coords,\n",
        "    point_labels,\n",
        "    padding_mask,\n",
        "    train=False)\n"
      ],
      "metadata": {
        "id": "ucpaMaSLfrrD"
      },
      "id": "ucpaMaSLfrrD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_image = predictor.transformed_image.cpu().numpy().transpose(0, 2, 3, 1)[0].astype(np.uint8)\n",
        "for i, (mask, score) in enumerate(zip(ret[0]['masks'][0], ret[0]['iou_predictions'][0])):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(transformed_image)\n",
        "    show_mask(mask, plt.gca())\n",
        "    show_points(point_coords[0, 0], input_label, plt.gca())\n",
        "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qkL9zZUolS9I"
      },
      "id": "qkL9zZUolS9I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree_util.tree_map(lambda x: x.shape, ret[0])"
      ],
      "metadata": {
        "id": "-XUC0c7dxnfQ"
      },
      "id": "-XUC0c7dxnfQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret_with_mask_prompt = sam_model.apply(\n",
        "    {'params': tree},\n",
        "    inp,\n",
        "    point_coords,\n",
        "    point_labels,\n",
        "    padding_mask,\n",
        "    mask_inputs=ret[0]['low_res_logits'][:, 0, :, :, None],\n",
        "    train=False)"
      ],
      "metadata": {
        "id": "4nTNruIExcR_"
      },
      "id": "4nTNruIExcR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.tree_util.tree_map(lambda x: x.shape, ret_with_mask_prompt[0]))\n",
        "transformed_image = predictor.transformed_image.cpu().numpy().transpose(0, 2, 3, 1)[0].astype(np.uint8)\n",
        "for i, (mask, score) in enumerate(zip(ret_with_mask_prompt[0]['masks'][0], ret_with_mask_prompt[0]['iou_predictions'][0])):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(transformed_image)\n",
        "    show_mask(mask, plt.gca())\n",
        "    show_points(point_coords[0, 0], input_label, plt.gca())\n",
        "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AH_3_WwfxzlW"
      },
      "id": "AH_3_WwfxzlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import checkpoints\n",
        "flax.config.update('flax_use_orbax_checkpointing', False)\n",
        "out_path = 'sam_vit_b'\n",
        "checkpoints.save_checkpoint(out_path, {'params': tree}, 0)"
      ],
      "metadata": {
        "id": "M0FPKhRiw3IY"
      },
      "id": "M0FPKhRiw3IY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download(f'{out_path}/checkpoint_0')"
      ],
      "metadata": {
        "id": "t5WYYCuQk_sD"
      },
      "id": "t5WYYCuQk_sD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x04S2LsKKqtS"
      },
      "id": "x04S2LsKKqtS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

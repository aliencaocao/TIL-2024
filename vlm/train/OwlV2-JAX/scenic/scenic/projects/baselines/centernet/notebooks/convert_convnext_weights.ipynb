{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtrkMwI0o4es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a12a78e-c079-4262-dedf-ac313340989a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "from typing import Tuple, Callable, Any, Optional, Union, Dict\n",
        "\n",
        "from absl import logging\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax.nn import initializers\n",
        "import jax.numpy as jnp\n",
        "!pip install ml_collections\n",
        "import ml_collections\n",
        "from jax import random\n",
        "import numpy as np\n",
        "from flax.training import checkpoints\n",
        "import torch\n",
        "import torch.utils.model_zoo\n",
        "flax.config.update('flax_use_orbax_checkpointing', False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convnext.py\n",
        "MODEL_URLS = dict(\n",
        "    convnext_tiny_1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',\n",
        "    convnext_small_1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',\n",
        "    convnext_base_1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',\n",
        "    convnext_large_1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',\n",
        "    convnext_tiny_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_224.pth',\n",
        "    convnext_small_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_224.pth',\n",
        "    convnext_base_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth',\n",
        "    convnext_large_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_224.pth',\n",
        "    convnext_xlarge_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth',\n",
        "\n",
        "    convnext_tiny_384_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_384.pth',\n",
        "    convnext_small_384_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_384.pth',\n",
        "    convnext_base_384_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth',\n",
        "    convnext_large_384_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_384.pth',\n",
        "    convnext_xlarge_384_in22ft1k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_384_ema.pth',\n",
        "\n",
        "    convnext_tiny_in22k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',\n",
        "    convnext_small_in22k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',\n",
        "    convnext_base_in22k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',\n",
        "    convnext_large_in22k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',\n",
        "    convnext_xlarge_in22k=\n",
        "    'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',\n",
        ")\n",
        "\n",
        "SIZE_MAP = {\n",
        "    'tiny': 'T',\n",
        "    'small': 'S',\n",
        "    'base': 'B',\n",
        "    'large': 'L',\n",
        "    'xlarge': 'XL'\n",
        "}"
      ],
      "metadata": {
        "id": "qgf104GJ348q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Jax model implementation\n",
        "\n",
        "class ConvNeXtBlock(nn.Module):\n",
        "  \"\"\"Bottleneck ResNet block.\n",
        "  \"\"\"\n",
        "\n",
        "  dim: int\n",
        "  droplayer_p: float = 0\n",
        "  layer_scale_init_value: float = 1e-6\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def get_drop_pattern(self,\n",
        "                       x: jnp.ndarray,\n",
        "                       deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"Returns dropout mask for stochastic depth regularisation.\"\"\"\n",
        "    if not deterministic and self.droplayer_p:\n",
        "      shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "      return jax.random.bernoulli(\n",
        "          self.make_rng('dropout'), self.droplayer_p, shape).astype(self.dtype)\n",
        "    else:\n",
        "      return 0.0\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, train: bool = False) -> jnp.ndarray:\n",
        "    residual = x\n",
        "    x = nn.Conv(\n",
        "        self.dim, (7, 7),\n",
        "        1,\n",
        "        padding=3,\n",
        "        feature_group_count=self.dim,\n",
        "        use_bias=True,\n",
        "        dtype=self.dtype,\n",
        "        name='dwconv')(\n",
        "            x)\n",
        "    x = nn.LayerNorm(epsilon=1e-6, name='norm')(x)\n",
        "    x = nn.Dense(4 * self.dim, name='pwconv1')(x)  # B x H x W x 4C\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.Dense(self.dim, name='pwconv2')(x)  # B x H x W x C\n",
        "    if self.layer_scale_init_value > 0:\n",
        "      gamma = self.param(\n",
        "          'gamma',\n",
        "          initializers.constant(self.layer_scale_init_value),\n",
        "          (self.dim))\n",
        "      x = x * gamma[..., :]\n",
        "    drop_pattern = self.get_drop_pattern(x, deterministic=not train)\n",
        "    x = residual + (1.0 - drop_pattern) * x\n",
        "    return x\n",
        "\n",
        "SIZE_OPTIONS = {\n",
        "    'T': ([3, 3, 9, 3], [96, 192, 384, 768], 0.1),\n",
        "    'S': ([3, 3, 27, 3], [96, 192, 384, 768], 0.4),\n",
        "    'B': ([3, 3, 27, 3], [128, 256, 512, 1024], 0.5),\n",
        "    'L': ([3, 3, 27, 3], [192, 384, 768, 1536], 0.5),\n",
        "    'XL': ([3, 3, 27, 3], [256, 512, 1024, 2048], 0.5),\n",
        "}\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "  \"\"\"ConvNeXt architecture.\n",
        "\n",
        "  Attributes:\n",
        "    num_outputs: Num output classes. If None, a dict of intermediate feature\n",
        "      maps is returned.\n",
        "    size: size as pre-defined in the paper. Options: T, S, B, L\n",
        "    kernel_init: Kernel initialization.\n",
        "    bias_init: Bias initialization.\n",
        "    dtype: Data type, e.g. jnp.float32.\n",
        "  \"\"\"\n",
        "  num_outputs: Optional[int]\n",
        "  size: str = 'T'\n",
        "  layer_scale_init_value: float = 1e-6\n",
        "  kernel_init: Callable[..., Any] = initializers.lecun_normal()\n",
        "  bias_init: Callable[..., Any] = initializers.zeros\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      x: jnp.ndarray,\n",
        "      train: bool = False,\n",
        "      debug: bool = False) -> Union[jnp.ndarray, Dict[str, jnp.ndarray]]:\n",
        "    \"\"\"Applies ResNet model to the inputs.\n",
        "\n",
        "    Args:\n",
        "      x: Inputs to the model.\n",
        "      train: Whether it is training or not.\n",
        "      debug: Whether the debug mode is enabled. debug=True enables model\n",
        "        specific logging/storing some values using jax.host_callback.\n",
        "\n",
        "    Returns:\n",
        "       Un-normalized logits.\n",
        "    \"\"\"\n",
        "    if self.size not in SIZE_OPTIONS:\n",
        "      raise ValueError('Please provide a valid size')\n",
        "    depths, dims, drop_path_rate = SIZE_OPTIONS[self.size]\n",
        "    sum_depth = sum(depths)\n",
        "    dp_rates = [drop_path_rate * i / (sum_depth - 1) for i in range(sum_depth)]\n",
        "    layernorm = functools.partial(nn.LayerNorm, epsilon=1e-6)\n",
        "    block = functools.partial(\n",
        "        ConvNeXtBlock,\n",
        "        layer_scale_init_value=self.layer_scale_init_value,\n",
        "        dtype=self.dtype)\n",
        "    x = nn.Conv(\n",
        "        dims[0],\n",
        "        kernel_size=(4, 4),\n",
        "        strides=(4, 4),\n",
        "        dtype=self.dtype,\n",
        "        name='downsample_layers.0.0')(\n",
        "            x)\n",
        "    x = layernorm(name='downsample_layers.0.1')(x)\n",
        "    representations = {'stem': x}\n",
        "    cur = 0\n",
        "    for i, (depth, dim) in enumerate(zip(depths, dims)):\n",
        "      if i > 0:\n",
        "        x = layernorm(name='downsample_layers.{}.0'.format(i))(x)\n",
        "        x = nn.Conv(\n",
        "            dims[i],\n",
        "            kernel_size=(2, 2),\n",
        "            strides=(2, 2),\n",
        "            dtype=self.dtype,\n",
        "            name='downsample_layers.{}.1'.format(i))(\n",
        "                x)\n",
        "      for j in range(depth):\n",
        "        x = block(\n",
        "            dim=dim, droplayer_p=dp_rates[cur + j],\n",
        "            name='stages.{}.{}'.format(i, j))(x, train)\n",
        "      cur += depth\n",
        "      representations[f'stage_{i + 1}'] = x\n",
        "\n",
        "    # Head.\n",
        "    if self.num_outputs:\n",
        "      x = jnp.mean(x, axis=(1, 2))\n",
        "      x = layernorm(name='norm')(x)\n",
        "      # x = nn_layers.IdentityLayer(name='pre_logits')(x)\n",
        "      x = nn.Dense(\n",
        "          self.num_outputs,\n",
        "          kernel_init=self.kernel_init,\n",
        "          bias_init=self.bias_init,\n",
        "          dtype=self.dtype,\n",
        "          name='output_projection')(\n",
        "              x)\n",
        "      return x\n",
        "    else:\n",
        "      return representations"
      ],
      "metadata": {
        "id": "OAqvrREPPTt4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursively matching layer names\n",
        "def dfs(k, ori_k, v):\n",
        "  if isinstance(v, jnp.ndarray):\n",
        "    torch_data = torch_weights[k]\n",
        "    if len(v.shape) == 2: # FC layers\n",
        "      torch_data = np.transpose(torch_data, (1, 0))\n",
        "    if len(v.shape) == 4: # Conv layers\n",
        "      torch_data = np.transpose(torch_data, (2, 3, 1, 0))\n",
        "    return [(k, torch_data.shape)], torch_data\n",
        "  lst, tree = [], {}\n",
        "  for kk, vv in v.items():\n",
        "    if isinstance(vv, jnp.ndarray) and (kk == 'kernel' or kk == 'scale'):\n",
        "      new_kk = 'weight'\n",
        "    elif kk == 'output_projection':\n",
        "      new_kk = 'head'\n",
        "    else:\n",
        "      new_kk = kk\n",
        "    sub_lst, sub_tree = dfs(\n",
        "        '{}.{}'.format(k, new_kk) if k != '' else new_kk, kk, vv)\n",
        "    lst.extend(sub_lst)\n",
        "    tree[kk] = sub_tree\n",
        "  return lst, tree"
      ],
      "metadata": {
        "id": "LcYt2ByvwMOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RefK_x578Fqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pytorch weights (converted state_dict to npy files) in list\n",
        "#   and convert to Jax weights in tree\n",
        "MODEL_NAMES = ['convnext_tiny_in22k']\n",
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "  print('Processing', model_name)\n",
        "  model_path = MODEL_URLS[model_name]\n",
        "  torch_weights = torch.utils.model_zoo.load_url(model_path)['model']\n",
        "  torch_weights = {k: v.cpu().numpy() for k, v in torch_weights.items()}\n",
        "  num_params_torch = 0\n",
        "  for k, v in torch_weights.items():\n",
        "    num_params_torch += np.prod(v.shape)\n",
        "  print('num_params_torch', num_params_torch)\n",
        "  num_class = 21841 if model_name.endswith('_in22k') else 1000\n",
        "  size = SIZE_MAP[model_name[9: model_name[10:].find('_') + 10]]\n",
        "  res = 384 if '384' in model_name else 224\n",
        "  model = ConvNeXt(num_outputs=num_class, size=size)\n",
        "  x = jnp.zeros((1, res, res, 3))\n",
        "  rngs = {'params': random.PRNGKey(0), 'dropout': random.PRNGKey(0)}\n",
        "  variables = model.init(rngs, x)\n",
        "  ret, tree = dfs('', '', variables['params'])\n",
        "  ret = [(k, v) for k, v in sorted(ret)]\n",
        "  tot_params = 0\n",
        "  for k, v in ret:\n",
        "    tot_params += np.prod(v)\n",
        "  print('tot_params      ', tot_params)\n",
        "  tree.keys()\n",
        "  new_variables = {'params': tree}\n",
        "  save_path = '{}'.format(model_name)\n",
        "  checkpoints.save_checkpoint(save_path, new_variables, 0)"
      ],
      "metadata": {
        "id": "MFUz5nFs38ZP",
        "outputId": "8c4e614c-66d5-4f6b-e05e-2ea6b0994fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing convnext_tiny_in22k\n",
            "num_params_torch 44615857\n",
            "tot_params       44615857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTBTa9EnDMSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Uaoeb0TF8Gz",
        "outputId": "a96d4381-ec4d-4b4f-9432-eb5c5d819223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  2.1 ; cuda:  cu121\n",
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install ml_collections\n",
        "import ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MAE/mae_pretrain_vit_base.pth\"\n",
        "mae_model = torch.load('mae_pretrain_vit_base.pth')\n",
        "torch_weights = {k: v.numpy() for k, v in mae_model['model'].items()}"
      ],
      "metadata": {
        "id": "OUlRll4TGKS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a2a45f-29f3-4d64-8f1a-f1afe273e1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-18 18:41:05--  https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MAE/mae_pretrain_vit_base.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.51, 3.162.163.11, 3.162.163.19, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343249461 (327M) [binary/octet-stream]\n",
            "Saving to: ‘mae_pretrain_vit_base.pth’\n",
            "\n",
            "mae_pretrain_vit_ba 100%[===================>] 327.35M   136MB/s    in 2.4s    \n",
            "\n",
            "2024-01-18 18:41:08 (136 MB/s) - ‘mae_pretrain_vit_base.pth’ saved [343249461/343249461]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs(k, v, converted_torch_weight):\n",
        "  \"\"\"Recursively match weights.\"\"\"\n",
        "  if isinstance(v, jnp.ndarray):\n",
        "    if k in converted_torch_weight:\n",
        "      torch_data = converted_torch_weight[k]\n",
        "      if len(v.shape) == 2 and not 'rel_pos' in k:\n",
        "        torch_data = np.transpose(torch_data, (\n",
        "          1, 0))\n",
        "      if len(v.shape) == 4:\n",
        "        if 'simfp_2.0' in k or 'simfp_2.3' in k or 'simfp_3.0' in k:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 0, 1))\n",
        "        else:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 1, 0))\n",
        "    else:\n",
        "      print(f'{k} not in checkpoint')\n",
        "      torch_data = v\n",
        "    return [(k, torch_data.shape)], torch_data\n",
        "  lst, tree = [], {}\n",
        "  for kk, vv in v.items():\n",
        "    if isinstance(vv, jnp.ndarray) and (kk == 'kernel' or kk == 'scale'):\n",
        "      new_kk = 'weight'\n",
        "    elif kk == 'output_projection':\n",
        "      new_kk = 'head'\n",
        "    else:\n",
        "      new_kk = kk\n",
        "    sub_lst, sub_tree = dfs(\n",
        "        '{}.{}'.format(k, new_kk) if k else new_kk,\n",
        "        vv,\n",
        "        converted_torch_weight)\n",
        "    lst.extend(sub_lst)\n",
        "    tree[kk] = sub_tree\n",
        "  return lst, tree\n"
      ],
      "metadata": {
        "id": "Lovw3WQKIOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title jax ViTDet implementation\n",
        "\"\"\"ViTDet with simple FPN.\"\"\"\n",
        "\n",
        "import functools\n",
        "from typing import Any, Optional\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "\n",
        "\n",
        "__all__ = ['ViT', 'SimpleFeaturePyramid']\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Multi-head Attention block with relative position embeddings.\n",
        "\n",
        "  Attributes:\n",
        "  dim (int): Number of input channels.\n",
        "  num_heads (int): Number of attention heads.\n",
        "  qkv_bias (bool:  If True, add a learnable bias to query, key, value.\n",
        "  use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "    attention map.\n",
        "  rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "    parameters.\n",
        "  input_size (int or None): Input resolution for calculating the relative\n",
        "    positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int = 8\n",
        "  qkv_bias: bool = True\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  input_size: Optional[Any] = None\n",
        "\n",
        "  def get_rel_pos(self, q_size, k_size, rel_pos):\n",
        "    \"\"\"Get relative positional embeddings.\n",
        "\n",
        "    Args:\n",
        "      q_size (int): size of query q.\n",
        "      k_size (int): size of key k.\n",
        "      rel_pos (Tensor): relative position embeddings (L, C).\n",
        "    Returns:\n",
        "      Extracted positional embeddings according to relative positions.\n",
        "    \"\"\"\n",
        "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
        "    # Interpolate rel pos if needed.\n",
        "    if rel_pos.shape[0] != max_rel_dist:\n",
        "      # Interpolate rel pos.\n",
        "      rel_pos_resized = jax.image.resize(\n",
        "          rel_pos,\n",
        "          shape=(max_rel_dist, rel_pos.shape[1]),\n",
        "          method='linear',\n",
        "      )\n",
        "    else:\n",
        "      rel_pos_resized = rel_pos\n",
        "\n",
        "    # Scale the coords with short length if shapes for q and k are different.\n",
        "    q_coords = jnp.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
        "    k_coords = jnp.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
        "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(\n",
        "        q_size / k_size, 1.0)\n",
        "    relative_coords = relative_coords.astype(jnp.int32).reshape(-1)\n",
        "    return rel_pos_resized[relative_coords].reshape(q_size, k_size, -1)\n",
        "\n",
        "  def add_decomposed_rel_pos(\n",
        "      self, attn, q, rel_pos_h, rel_pos_w, q_size, k_size):\n",
        "    \"\"\"Calculate decomposed Relative Positional Embeddings from paper:`mvitv2`.\n",
        "\n",
        "    Args:\n",
        "      attn (Tensor): attention map.\n",
        "      q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
        "      rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
        "      rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
        "      q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
        "      k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
        "    Returns:\n",
        "      attn (Tensor): attention map with added relative positional embeddings.\n",
        "    \"\"\"\n",
        "    q_h, q_w = q_size\n",
        "    k_h, k_w = k_size\n",
        "    rh = self.get_rel_pos(q_h, k_h, rel_pos_h)\n",
        "    rw = self.get_rel_pos(q_w, k_w, rel_pos_w)\n",
        "\n",
        "    batch, _, dim = q.shape\n",
        "    r_q = q.reshape(batch, q_h, q_w, dim)\n",
        "    rel_h = jnp.einsum('bhwc,hkc->bhwk', r_q, rh)\n",
        "    rel_w = jnp.einsum('bhwc,wkc->bhwk', r_q, rw)\n",
        "\n",
        "    attn = (\n",
        "        attn.reshape(batch, q_h, q_w, k_h, k_w) + rel_h[\n",
        "            :, :, :, :, None] + rel_w[:, :, :, None, :]\n",
        "    ).reshape(batch, q_h * q_w, k_h * k_w)\n",
        "\n",
        "    return attn\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    batch, height, width, _ = x.shape\n",
        "    head_dim = self.dim // self.num_heads\n",
        "    qkv = nn.Dense(self.dim * 3, use_bias=self.qkv_bias, name='qkv')(\n",
        "        x)  # batch x height x width x 3dim\n",
        "    qkv = qkv.reshape(batch, height * width, 3, self.num_heads, -1).transpose(\n",
        "        2, 0, 3, 1, 4)  # 3 x batch x num_heads x num_tokens x D\n",
        "    qkv = qkv.reshape(3, batch * self.num_heads, height * width, -1)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]  # [batch * num_heads, num_tokens, D]\n",
        "    attn = (q * (head_dim ** -0.5)) @ k.transpose(\n",
        "        0, 2, 1)  # [batch * num_heads, num_tokens, num_tokens]\n",
        "    if self.use_rel_pos:\n",
        "      rel_pos_h = self.param(\n",
        "          'rel_pos_h', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      rel_pos_w = self.param(\n",
        "          'rel_pos_w', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      attn = self.add_decomposed_rel_pos(\n",
        "          attn, q, rel_pos_h, rel_pos_w,\n",
        "          (height, width), (height, width))\n",
        "    attn = jax.nn.softmax(attn)\n",
        "    x = (attn @ v).reshape(batch, self.num_heads, height, width, -1).transpose(\n",
        "        0, 2, 3, 1, 4).reshape(batch, height, width, -1)\n",
        "    x = nn.Dense(self.dim, name='proj')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "\n",
        "  hidden_features: int\n",
        "  out_features: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(self.hidden_features, name='fc1')(x)\n",
        "    x = nn.gelu(x, approximate=False)\n",
        "    x = nn.Dense(self.out_features, name='fc2')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer blocks with support of window attention and residual blocks.\n",
        "\n",
        "  Attributes:\n",
        "    dim (int): Number of input channels.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    drop_path (float): Stochastic depth rate.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks. If it equals 0,\n",
        "      then not use window attention.\n",
        "    input_size (int or None): Input resolution for calculating the relative\n",
        "      positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int\n",
        "  mlp_ratio: float = 4.0\n",
        "  qkv_bias: bool = True\n",
        "  drop_path: float = 0.0\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 0\n",
        "  input_size: Optional[int] = None\n",
        "\n",
        "  def window_partition(self, x):\n",
        "    \"\"\"Partition into non-overlapping windows with padding if needed.\n",
        "\n",
        "    Args:\n",
        "      x (array): input tokens with [B, H, W, C].\n",
        "    Returns:\n",
        "      windows: windows after partition with [B * num_windows, window_size,\n",
        "        window_size, C].\n",
        "      (Hp, Wp): padded height and width before partition\n",
        "    \"\"\"\n",
        "    batch, h, w, c = x.shape\n",
        "\n",
        "    pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
        "    pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "      x = jnp.pad(\n",
        "          x, ((0, 0), (0, pad_w), (0, pad_h), (0, 0)),\n",
        "          'constant', constant_values=0)\n",
        "    hp, wp = h + pad_h, w + pad_w\n",
        "\n",
        "    x = x.reshape(\n",
        "        batch, hp // self.window_size, self.window_size,\n",
        "        wp // self.window_size, self.window_size, c)\n",
        "    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(\n",
        "        -1, self.window_size, self.window_size, c)\n",
        "    return windows, (hp, wp)\n",
        "\n",
        "  def window_unpartition(self, windows, pad_hw, hw):\n",
        "    \"\"\"Window unpartition into original sequences and removing padding.\n",
        "\n",
        "    Args:\n",
        "      windows (array): inputs: [B * num_windows, window_size, window_size, C].\n",
        "      pad_hw (Tuple): padded height and width (Hp, Wp).\n",
        "      hw (Tuple): original height and width (H, W) before padding.\n",
        "\n",
        "    Returns:\n",
        "      x: unpartitioned sequences with [B, H, W, C].\n",
        "    \"\"\"\n",
        "    hp, wp = pad_hw\n",
        "    h, w = hw\n",
        "    batch = windows.shape[0] // (\n",
        "        hp * wp // self.window_size // self.window_size)\n",
        "    x = windows.reshape(\n",
        "        batch,\n",
        "        hp // self.window_size, wp // self.window_size,\n",
        "        self.window_size, self.window_size, -1)\n",
        "    x = x.transpose(0, 1, 3, 2, 4, 5).reshape(batch, hp, wp, -1)\n",
        "    if hp > h or wp > w:\n",
        "      x = x[:, :h, :w, :]\n",
        "    return x\n",
        "\n",
        "  def get_drop_pattern(self, x, deterministic):\n",
        "    if not deterministic and self.drop_path:\n",
        "      shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "      return jax.random.bernoulli(\n",
        "          self.make_rng('dropout'), self.drop_path, shape).astype('float32')\n",
        "    else:\n",
        "      return 0.0\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train = False):\n",
        "    shortcut = x\n",
        "    ln = functools.partial(nn.LayerNorm, epsilon=1e-6)\n",
        "    x = ln(name='norm1')(x)\n",
        "    # Window partition\n",
        "    if self.window_size > 0:\n",
        "      h, w = x.shape[1], x.shape[2]\n",
        "      x, pad_hw = self.window_partition(x)\n",
        "\n",
        "    x = Attention(\n",
        "        self.dim,\n",
        "        num_heads=self.num_heads,\n",
        "        qkv_bias=self.qkv_bias,\n",
        "        use_rel_pos=self.use_rel_pos,\n",
        "        rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "        input_size=self.input_size if self.window_size == 0 else (\n",
        "            self.window_size, self.window_size),\n",
        "        name='attn')(x)\n",
        "    # Reverse window partition\n",
        "    if self.window_size > 0:\n",
        "      x = self.window_unpartition(x, pad_hw, (h, w))\n",
        "\n",
        "    x = shortcut + (1.0 - self.get_drop_pattern(x, not train)) * x\n",
        "    y = ln(name='norm2')(x)\n",
        "    y = Mlp(int(self.dim * self.mlp_ratio), self.dim, name='mlp')(y)\n",
        "    x = x + (1.0 - self.get_drop_pattern(y, not train)) * y\n",
        "    return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "  \"\"\"This module implements Vision Transformer (ViT) backbone in paper:`vitdet`.\n",
        "\n",
        "  \"Exploring Plain Vision Transformer Backbones for Object Detection\",\n",
        "  https://arxiv.org/abs/2203.16527\n",
        "\n",
        "  Attributes:\n",
        "    img_size (int): Input image size.\n",
        "    patch_size (int): Patch size.\n",
        "    in_chans (int): Number of input image channels.\n",
        "    embed_dim (int): Patch embedding dimension.\n",
        "    depth (int): Depth of ViT.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    drop_path_rate (float): Stochastic depth rate.\n",
        "    use_abs_pos (bool): If True, use absolute positional embeddings.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks.\n",
        "    window_block_indexes (list): Indexes for blocks using window attention.\n",
        "    pretrain_img_size (int): input image size for pretraining models.\n",
        "    pretrain_use_cls_token (bool): If True, pretrainig models use class token.\n",
        "  \"\"\"\n",
        "  img_size: int = 1024\n",
        "  patch_size: int = 16\n",
        "  in_chans: int = 3\n",
        "  embed_dim: int = 768\n",
        "  depth: int = 12\n",
        "  num_heads: int = 12\n",
        "  mlp_ratio: float = 4.0\n",
        "  qkv_bias: bool = True\n",
        "  drop_path_rate: float = 0.1\n",
        "  use_abs_pos: bool = True\n",
        "  use_rel_pos: bool = True\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 14\n",
        "  window_block_indexes: Any = (0, 1, 3, 4, 6, 7, 9, 10)\n",
        "  pretrain_img_size: int = 224\n",
        "  pretrain_use_cls_token: bool = True\n",
        "\n",
        "  def _get_abs_pos(self, abs_pos, hw):\n",
        "    \"\"\"Calculate absolute positional embeddings.\n",
        "\n",
        "    If needed, resize embeddings and remove cls_token dimension for the original\n",
        "      embeddings.\n",
        "    Args:\n",
        "      abs_pos (array): absolute positional embeddings with (1, num_position, C).\n",
        "      hw (Tuple): size of input image tokens.\n",
        "    Returns:\n",
        "      Absolute positional embeddings after processing with shape (1, H, W, C)\n",
        "    \"\"\"\n",
        "    h, w = hw\n",
        "    if self.pretrain_use_cls_token:\n",
        "      abs_pos = abs_pos[:, 1:]\n",
        "    xy_num = abs_pos.shape[1]\n",
        "    size = int(xy_num ** 0.5)\n",
        "    assert size * size == xy_num\n",
        "    abs_pos = abs_pos.reshape(abs_pos.shape[0], size, size, -1)\n",
        "    if size != h or size != w:\n",
        "      new_abs_pos = jax.image.resize(\n",
        "          abs_pos,\n",
        "          (abs_pos.shape[0], h, w, abs_pos.shape[3]),\n",
        "          method='bicubic',\n",
        "      )\n",
        "    return new_abs_pos\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, train: bool = False):\n",
        "    # print('input', x.shape)\n",
        "    x = nn.Conv(\n",
        "        self.embed_dim, (self.patch_size, self.patch_size),\n",
        "        strides=(self.patch_size, self.patch_size),\n",
        "        padding='VALID',\n",
        "        name='patch_embed.proj')(x)\n",
        "    # print('after conv', x.shape, x[0, 0, 0, :10])\n",
        "    if self.use_abs_pos:\n",
        "      num_patches = (self.pretrain_img_size // self.patch_size) ** 2\n",
        "      num_positions = (\n",
        "          num_patches + 1) if self.pretrain_use_cls_token else num_patches\n",
        "      pos_embed = self.param(\n",
        "          'pos_embed', nn.initializers.zeros,\n",
        "          (1, num_positions, self.embed_dim))\n",
        "      x = x + self._get_abs_pos(pos_embed, (x.shape[1], x.shape[2]))\n",
        "    # print('after pos emb', x.shape, x[0, 0, 0, :10])\n",
        "    dp_rates = [\n",
        "        self.drop_path_rate * i / (self.depth - 1) for i in range(self.depth)]\n",
        "    for i in range(self.depth):\n",
        "      x = Block(\n",
        "          dim=self.embed_dim,\n",
        "          num_heads=self.num_heads,\n",
        "          mlp_ratio=self.mlp_ratio,\n",
        "          qkv_bias=self.qkv_bias,\n",
        "          drop_path=dp_rates[i],\n",
        "          use_rel_pos=self.use_rel_pos,\n",
        "          rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "          window_size=self.window_size if i in self.window_block_indexes else 0,\n",
        "          input_size=(\n",
        "              self.img_size // self.patch_size,\n",
        "              self.img_size // self.patch_size),\n",
        "          name=f'blocks.{i}',\n",
        "          )(x, train=train)\n",
        "    #   print(f'after block {i}', x.shape, x[0, 0, 0, :10])\n",
        "    return x\n",
        "\n",
        "\n",
        "SIZE_CONFIGS = {\n",
        "    'B': (768, 12, 12, 0.1, (0, 1, 3, 4, 6, 7, 9, 10)),\n",
        "    'L': (1024, 24, 16, 0.4, (\n",
        "        0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22)),\n",
        "    'H': (1280, 32, 16, 0.5, (\n",
        "        0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21,\n",
        "        22, 24, 25, 26, 27, 28, 29, 30)),\n",
        "}\n",
        "\n",
        "\n",
        "class SimpleFeaturePyramid(nn.Module):\n",
        "  \"\"\"This module implements SimpleFeaturePyramid in paper:`vitdet`.\n",
        "\n",
        "  It creates pyramid features built on top of the input feature map.\n",
        "\n",
        "  Attributes:\n",
        "    in_dim (int): input dim\n",
        "    out_channels (int): number of channels in the output feature maps.\n",
        "    scale_factors (list[float]): list of scaling factors to upsample or\n",
        "      downsample the input features for creating pyramid features.\n",
        "    num_top_blocks (int): top level downsample block\n",
        "    norm (str): the normalization to use.\n",
        "    square_pad (int): If > 0, require input images to be padded to specific\n",
        "      square size.\n",
        "  \"\"\"\n",
        "  in_dim: int = 768\n",
        "  out_channels: int = 256\n",
        "  scale_factors: Any = (4.0, 2.0, 1.0, 0.5)\n",
        "  num_top_blocks: int = 1\n",
        "  square_pad: int = 1024\n",
        "  backbone_args: ml_collections.ConfigDict = ml_collections.ConfigDict()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, train: bool = False):\n",
        "    sz = self.backbone_args.pop('size', 'B')\n",
        "    dim, depth, num_heads, dp, window_block_indexes = SIZE_CONFIGS[sz]\n",
        "    self.backbone_args['embed_dim'] = self.backbone_args.get(\n",
        "        'embed_dim', dim)\n",
        "    self.backbone_args['depth'] = self.backbone_args.get('depth', depth)\n",
        "    self.backbone_args['num_heads'] = self.backbone_args.get(\n",
        "        'num_heads', num_heads)\n",
        "    self.backbone_args['drop_path_rate'] = self.backbone_args.get(\n",
        "        'drop_path_rate', dp)\n",
        "    self.backbone_args['window_block_indexes'] = self.backbone_args.get(\n",
        "        'window_block_indexes', window_block_indexes)\n",
        "    features = ViT(**self.backbone_args, name='net')(x, train=train)\n",
        "    # features = ViT(name='net')(x, train=train)\n",
        "    results = []\n",
        "    dim = self.in_dim\n",
        "    conv_transpose = functools.partial(\n",
        "        nn.ConvTranspose, kernel_size=(2, 2), strides=(2, 2))\n",
        "    ln = functools.partial(nn.LayerNorm, epsilon=1e-6)\n",
        "    conv = functools.partial(nn.Conv, use_bias=False)\n",
        "    for scale in self.scale_factors:\n",
        "      x = features\n",
        "      if scale == 4.0:\n",
        "        stage, idx_base = 2, 4\n",
        "        x = conv_transpose(dim // 2, name='simfp_2.0')(x)\n",
        "        x = ln(name='simfp_2.1')(x)\n",
        "        x = nn.gelu(x, approximate=False)\n",
        "        x = conv_transpose(dim // 4, name='simfp_2.3')(x)\n",
        "      elif scale == 2.0:\n",
        "        stage, idx_base = 3, 1\n",
        "        x = conv_transpose(dim // 2, name='simfp_3.0')(x)\n",
        "      elif scale == 1.0:\n",
        "        stage, idx_base = 4, 0\n",
        "      elif scale == 0.5:\n",
        "        stage, idx_base = 5, 1\n",
        "        x = nn.max_pool(x, (2, 2), strides=(2, 2))\n",
        "      else:\n",
        "        raise NotImplementedError(f'scale_factor={scale} is not supported yet.')\n",
        "      x = conv(\n",
        "          self.out_channels, kernel_size=(1, 1),\n",
        "          name=f'simfp_{stage}.{idx_base}')(x)\n",
        "      x = ln(name=f'simfp_{stage}.{idx_base}.norm')(x)\n",
        "      x = conv(\n",
        "          self.out_channels, kernel_size=(3, 3), padding=[(1, 1), (1, 1)],\n",
        "          name=f'simfp_{stage}.{idx_base + 1}')(x)\n",
        "      x = ln(name=f'simfp_{stage}.{idx_base + 1}.norm')(x)\n",
        "      results.append(x)\n",
        "    if self.num_top_blocks == 1:\n",
        "      x = nn.max_pool(\n",
        "          results[-1], (1, 1), strides=(2, 2), padding=[(0, 0), (0, 0)])\n",
        "      results.append(x)\n",
        "    elif self.num_top_blocks == 2:\n",
        "      top_block = TwiceDownsampleBlock(\n",
        "          out_channels=self.out_channels, name='top_block')\n",
        "      p6, p7 = top_block(results[-1])\n",
        "      results.extend([p6, p7])\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          f'num_top_blocks={self.num_top_blocks} is not supported yet.')\n",
        "    return results"
      ],
      "metadata": {
        "id": "vQqESU6MILoT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import ml_collections\n",
        "backbone_args = ml_collections.ConfigDict()\n",
        "sz = 'B'\n",
        "dim, depth, num_heads, dp, window_block_indexes = SIZE_CONFIGS[sz]\n",
        "backbone_args['embed_dim'] = backbone_args.get(\n",
        "    'embed_dim', dim)\n",
        "backbone_args['depth'] = backbone_args.get('depth', depth)\n",
        "backbone_args['num_heads'] = backbone_args.get(\n",
        "    'num_heads', num_heads)\n",
        "backbone_args['drop_path_rate'] = backbone_args.get(\n",
        "    'drop_path_rate', dp)\n",
        "backbone_args['window_block_indexes'] = backbone_args.get(\n",
        "    'window_block_indexes', window_block_indexes)\n",
        "vit_model = ViT(**backbone_args)\n",
        "\n",
        "rng = {'dropout': jax.random.PRNGKey(0), 'params': jax.random.PRNGKey(0)}\n",
        "input = jax.random.normal(jax.random.PRNGKey(0), (1, 1024, 1024, 3))\n",
        "vit_vars = vit_model.init(rng, input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR7nggeHSBKx",
        "outputId": "dbae5c9e-aea9-4f43-ef87-681fd6ff0ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret, tree = dfs('', vit_vars['params'], torch_weights)\n",
        "num_params = 0\n",
        "for k, v in torch_weights.items():\n",
        "  if 'cls_token' not in k and 'norm.' not in k:\n",
        "    num_params += np.prod(v.shape)\n",
        "  else:\n",
        "    print(f'{k} not loaded')\n",
        "print('#params in loaded model:', num_params)\n",
        "num_params = 0\n",
        "for k, v in ret:\n",
        "  if 'rel_pos' not in k:\n",
        "    num_params += np.prod(v)\n",
        "print('#params in converted model:', num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrgNQVbYSYzG",
        "outputId": "f0314032-30e8-4064-9e33-d777072434bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocks.0.attn.rel_pos_h not in checkpoint\n",
            "blocks.0.attn.rel_pos_w not in checkpoint\n",
            "blocks.1.attn.rel_pos_h not in checkpoint\n",
            "blocks.1.attn.rel_pos_w not in checkpoint\n",
            "blocks.2.attn.rel_pos_h not in checkpoint\n",
            "blocks.2.attn.rel_pos_w not in checkpoint\n",
            "blocks.3.attn.rel_pos_h not in checkpoint\n",
            "blocks.3.attn.rel_pos_w not in checkpoint\n",
            "blocks.4.attn.rel_pos_h not in checkpoint\n",
            "blocks.4.attn.rel_pos_w not in checkpoint\n",
            "blocks.5.attn.rel_pos_h not in checkpoint\n",
            "blocks.5.attn.rel_pos_w not in checkpoint\n",
            "blocks.6.attn.rel_pos_h not in checkpoint\n",
            "blocks.6.attn.rel_pos_w not in checkpoint\n",
            "blocks.7.attn.rel_pos_h not in checkpoint\n",
            "blocks.7.attn.rel_pos_w not in checkpoint\n",
            "blocks.8.attn.rel_pos_h not in checkpoint\n",
            "blocks.8.attn.rel_pos_w not in checkpoint\n",
            "blocks.9.attn.rel_pos_h not in checkpoint\n",
            "blocks.9.attn.rel_pos_w not in checkpoint\n",
            "blocks.10.attn.rel_pos_h not in checkpoint\n",
            "blocks.10.attn.rel_pos_w not in checkpoint\n",
            "blocks.11.attn.rel_pos_h not in checkpoint\n",
            "blocks.11.attn.rel_pos_w not in checkpoint\n",
            "cls_token not loaded\n",
            "norm.weight not loaded\n",
            "norm.bias not loaded\n",
            "#params in loaded model: 85796352\n",
            "#params in converted model: 85796352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flax\n",
        "from flax.training import checkpoints\n",
        "flax.config.update('flax_use_orbax_checkpointing', False)\n",
        "out_path = 'mae_pretrain_vit_base'\n",
        "checkpoints.save_checkpoint(out_path, {'params': tree}, 0)"
      ],
      "metadata": {
        "id": "OQ74viedZCRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ab19d0a-c9ca-4987-95fe-435dae1902bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mae_pretrain_vit_base/checkpoint_0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download(f'{out_path}/checkpoint_0')"
      ],
      "metadata": {
        "id": "d6oCOAvjpWyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijt7f4h1Yzdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7AqdgbEG-6e",
        "outputId": "27cadeab-5201-4f78-8b96-01641dc6690e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7vth7faz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7vth7faz\n",
            "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=fa189eb58de02a82e89d3ce61276e5c0c3874de88b0d0e3ee7efdae5ae2d743c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvfc_cq7/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Uaoeb0TF8Gz",
        "outputId": "67e3daf8-6116-4468-c9ff-6e4964e2ea3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  1.13 ; cuda:  cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.3-py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from flax) (1.21.6)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-13.1.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from flax) (4.4.0)\n",
            "Collecting tensorstore\n",
            "  Downloading tensorstore-0.1.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from flax) (3.2.2)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.8/dist-packages (from flax) (0.3.25)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax) (1.7.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax) (3.0.9)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (0.3.25+cuda11.cudnn805)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->flax) (1.3.0)\n",
            "Collecting cached_property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from orbax->flax) (3.6.4)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax) (1.0.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax) (5.10.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->flax) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib_resources->orbax->flax) (3.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (9.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (22.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax) (1.4.1)\n",
            "Installing collected packages: commonmark, cached_property, tensorstore, rich, chex, optax, orbax, flax\n",
            "Successfully installed cached_property-1.5.2 chex-0.1.5 commonmark-0.9.1 flax-0.6.3 optax-0.1.4 orbax-0.1.0 rich-13.1.0 tensorstore-0.1.30\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from ml_collections) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from ml_collections) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from ml_collections) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from ml_collections) (0.5.5)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=dc8d66ebc2f71bd1d0f9d62e94515e232c54616ab1f0f52d8db8aac04a9bf6fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/9f/a9/9e8309035a5bf09ed9086bbca8c9b74cb6413d3eb203e2bc8c\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: ml_collections\n",
            "Successfully installed ml_collections-0.1.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "!pip install flax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install ml_collections\n",
        "import ml_collections\n",
        "import flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "clip_model, preprocess = clip.load(\"ViT-B/16\", device='cpu')"
      ],
      "metadata": {
        "id": "7dsgOh72IKIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_weights = {k: v for k, v in clip_model.visual.state_dict().items()}"
      ],
      "metadata": {
        "id": "8voRTMbyEJU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "table = []\n",
        "for k in sorted(torch_weights):\n",
        "  v = torch_weights[k]\n",
        "  table.append((k, f'{v.shape}', f'{v.mean():.3f}', f'{v.std():.3f}'))\n",
        "table_str = tabulate(\n",
        "    table, tablefmt=\"pipe\", headers=[\"Names\", \"shape\", \"mean\", \"std\"])\n",
        "print(table_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbCu9fmlD745",
        "outputId": "8abddecc-ac0d-4428-a6a1-ab9bfc062899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Names                                         | shape                        |   mean |   std |\n",
            "|:----------------------------------------------|:-----------------------------|-------:|------:|\n",
            "| class_embedding                               | torch.Size([768])            |  0.003 | 0.234 |\n",
            "| conv1.weight                                  | torch.Size([768, 3, 16, 16]) |  0     | 0.019 |\n",
            "| ln_post.bias                                  | torch.Size([768])            |  0.192 | 0.31  |\n",
            "| ln_post.weight                                | torch.Size([768])            |  0.951 | 0.102 |\n",
            "| ln_pre.bias                                   | torch.Size([768])            | -0.001 | 0.073 |\n",
            "| ln_pre.weight                                 | torch.Size([768])            |  0.393 | 0.455 |\n",
            "| positional_embedding                          | torch.Size([197, 768])       | -0.007 | 0.028 |\n",
            "| proj                                          | torch.Size([768, 512])       |  0     | 0.013 |\n",
            "| transformer.resblocks.0.attn.in_proj_bias     | torch.Size([2304])           |  0.008 | 0.538 |\n",
            "| transformer.resblocks.0.attn.in_proj_weight   | torch.Size([2304, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.0.attn.out_proj.bias    | torch.Size([768])            |  0.001 | 0.043 |\n",
            "| transformer.resblocks.0.attn.out_proj.weight  | torch.Size([768, 768])       | -0     | 0.011 |\n",
            "| transformer.resblocks.0.ln_1.bias             | torch.Size([768])            | -0.014 | 0.181 |\n",
            "| transformer.resblocks.0.ln_1.weight           | torch.Size([768])            |  0.493 | 0.335 |\n",
            "| transformer.resblocks.0.ln_2.bias             | torch.Size([768])            |  0.023 | 0.223 |\n",
            "| transformer.resblocks.0.ln_2.weight           | torch.Size([768])            |  0.919 | 0.401 |\n",
            "| transformer.resblocks.0.mlp.c_fc.bias         | torch.Size([3072])           | -0.455 | 0.336 |\n",
            "| transformer.resblocks.0.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.015 |\n",
            "| transformer.resblocks.0.mlp.c_proj.bias       | torch.Size([768])            | -0.004 | 0.074 |\n",
            "| transformer.resblocks.0.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.009 |\n",
            "| transformer.resblocks.1.attn.in_proj_bias     | torch.Size([2304])           |  0.005 | 0.474 |\n",
            "| transformer.resblocks.1.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.018 |\n",
            "| transformer.resblocks.1.attn.out_proj.bias    | torch.Size([768])            | -0.001 | 0.081 |\n",
            "| transformer.resblocks.1.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.011 |\n",
            "| transformer.resblocks.1.ln_1.bias             | torch.Size([768])            |  0.001 | 0.159 |\n",
            "| transformer.resblocks.1.ln_1.weight           | torch.Size([768])            |  0.824 | 0.243 |\n",
            "| transformer.resblocks.1.ln_2.bias             | torch.Size([768])            |  0.009 | 0.171 |\n",
            "| transformer.resblocks.1.ln_2.weight           | torch.Size([768])            |  1.172 | 0.474 |\n",
            "| transformer.resblocks.1.mlp.c_fc.bias         | torch.Size([3072])           | -0.392 | 0.221 |\n",
            "| transformer.resblocks.1.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.1.mlp.c_proj.bias       | torch.Size([768])            | -0.005 | 0.102 |\n",
            "| transformer.resblocks.1.mlp.c_proj.weight     | torch.Size([768, 3072])      | -0     | 0.01  |\n",
            "| transformer.resblocks.10.attn.in_proj_bias    | torch.Size([2304])           |  0.008 | 0.395 |\n",
            "| transformer.resblocks.10.attn.in_proj_weight  | torch.Size([2304, 768])      | -0     | 0.016 |\n",
            "| transformer.resblocks.10.attn.out_proj.bias   | torch.Size([768])            |  0     | 0.088 |\n",
            "| transformer.resblocks.10.attn.out_proj.weight | torch.Size([768, 768])       | -0     | 0.015 |\n",
            "| transformer.resblocks.10.ln_1.bias            | torch.Size([768])            |  0.049 | 0.21  |\n",
            "| transformer.resblocks.10.ln_1.weight          | torch.Size([768])            |  2.018 | 0.146 |\n",
            "| transformer.resblocks.10.ln_2.bias            | torch.Size([768])            |  0.011 | 0.587 |\n",
            "| transformer.resblocks.10.ln_2.weight          | torch.Size([768])            |  4.746 | 0.409 |\n",
            "| transformer.resblocks.10.mlp.c_fc.bias        | torch.Size([3072])           | -0.4   | 0.178 |\n",
            "| transformer.resblocks.10.mlp.c_fc.weight      | torch.Size([3072, 768])      |  0     | 0.015 |\n",
            "| transformer.resblocks.10.mlp.c_proj.bias      | torch.Size([768])            | -0.002 | 0.1   |\n",
            "| transformer.resblocks.10.mlp.c_proj.weight    | torch.Size([768, 3072])      | -0     | 0.017 |\n",
            "| transformer.resblocks.11.attn.in_proj_bias    | torch.Size([2304])           |  0.002 | 0.179 |\n",
            "| transformer.resblocks.11.attn.in_proj_weight  | torch.Size([2304, 768])      | -0     | 0.015 |\n",
            "| transformer.resblocks.11.attn.out_proj.bias   | torch.Size([768])            | -0.003 | 0.302 |\n",
            "| transformer.resblocks.11.attn.out_proj.weight | torch.Size([768, 768])       |  0     | 0.018 |\n",
            "| transformer.resblocks.11.ln_1.bias            | torch.Size([768])            |  0.024 | 0.202 |\n",
            "| transformer.resblocks.11.ln_1.weight          | torch.Size([768])            |  2.028 | 0.159 |\n",
            "| transformer.resblocks.11.ln_2.bias            | torch.Size([768])            |  0.044 | 0.523 |\n",
            "| transformer.resblocks.11.ln_2.weight          | torch.Size([768])            |  1.79  | 0.137 |\n",
            "| transformer.resblocks.11.mlp.c_fc.bias        | torch.Size([3072])           | -0.386 | 0.215 |\n",
            "| transformer.resblocks.11.mlp.c_fc.weight      | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.11.mlp.c_proj.bias      | torch.Size([768])            | -0.006 | 0.164 |\n",
            "| transformer.resblocks.11.mlp.c_proj.weight    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| transformer.resblocks.2.attn.in_proj_bias     | torch.Size([2304])           | -0.005 | 0.458 |\n",
            "| transformer.resblocks.2.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.017 |\n",
            "| transformer.resblocks.2.attn.out_proj.bias    | torch.Size([768])            | -0.006 | 0.091 |\n",
            "| transformer.resblocks.2.attn.out_proj.weight  | torch.Size([768, 768])       | -0     | 0.012 |\n",
            "| transformer.resblocks.2.ln_1.bias             | torch.Size([768])            | -0.005 | 0.194 |\n",
            "| transformer.resblocks.2.ln_1.weight           | torch.Size([768])            |  0.922 | 0.189 |\n",
            "| transformer.resblocks.2.ln_2.bias             | torch.Size([768])            |  0.008 | 0.146 |\n",
            "| transformer.resblocks.2.ln_2.weight           | torch.Size([768])            |  1.21  | 0.157 |\n",
            "| transformer.resblocks.2.mlp.c_fc.bias         | torch.Size([3072])           | -0.331 | 0.162 |\n",
            "| transformer.resblocks.2.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.2.mlp.c_proj.bias       | torch.Size([768])            | -0     | 0.071 |\n",
            "| transformer.resblocks.2.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.011 |\n",
            "| transformer.resblocks.3.attn.in_proj_bias     | torch.Size([2304])           | -0     | 0.405 |\n",
            "| transformer.resblocks.3.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.017 |\n",
            "| transformer.resblocks.3.attn.out_proj.bias    | torch.Size([768])            | -0.001 | 0.06  |\n",
            "| transformer.resblocks.3.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| transformer.resblocks.3.ln_1.bias             | torch.Size([768])            | -0.009 | 0.192 |\n",
            "| transformer.resblocks.3.ln_1.weight           | torch.Size([768])            |  1.186 | 0.132 |\n",
            "| transformer.resblocks.3.ln_2.bias             | torch.Size([768])            |  0.01  | 0.145 |\n",
            "| transformer.resblocks.3.ln_2.weight           | torch.Size([768])            |  1.333 | 0.134 |\n",
            "| transformer.resblocks.3.mlp.c_fc.bias         | torch.Size([3072])           | -0.309 | 0.178 |\n",
            "| transformer.resblocks.3.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.3.mlp.c_proj.bias       | torch.Size([768])            |  0.001 | 0.055 |\n",
            "| transformer.resblocks.3.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.01  |\n",
            "| transformer.resblocks.4.attn.in_proj_bias     | torch.Size([2304])           | -0.008 | 0.322 |\n",
            "| transformer.resblocks.4.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.4.attn.out_proj.bias    | torch.Size([768])            | -0     | 0.057 |\n",
            "| transformer.resblocks.4.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| transformer.resblocks.4.ln_1.bias             | torch.Size([768])            |  0.002 | 0.255 |\n",
            "| transformer.resblocks.4.ln_1.weight           | torch.Size([768])            |  1.29  | 0.122 |\n",
            "| transformer.resblocks.4.ln_2.bias             | torch.Size([768])            | -0.008 | 0.212 |\n",
            "| transformer.resblocks.4.ln_2.weight           | torch.Size([768])            |  1.444 | 0.105 |\n",
            "| transformer.resblocks.4.mlp.c_fc.bias         | torch.Size([3072])           | -0.316 | 0.177 |\n",
            "| transformer.resblocks.4.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.4.mlp.c_proj.bias       | torch.Size([768])            |  0.001 | 0.057 |\n",
            "| transformer.resblocks.4.mlp.c_proj.weight     | torch.Size([768, 3072])      | -0     | 0.011 |\n",
            "| transformer.resblocks.5.attn.in_proj_bias     | torch.Size([2304])           |  0.005 | 0.316 |\n",
            "| transformer.resblocks.5.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.5.attn.out_proj.bias    | torch.Size([768])            |  0     | 0.069 |\n",
            "| transformer.resblocks.5.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| transformer.resblocks.5.ln_1.bias             | torch.Size([768])            |  0.002 | 0.22  |\n",
            "| transformer.resblocks.5.ln_1.weight           | torch.Size([768])            |  1.286 | 0.146 |\n",
            "| transformer.resblocks.5.ln_2.bias             | torch.Size([768])            | -0.052 | 0.306 |\n",
            "| transformer.resblocks.5.ln_2.weight           | torch.Size([768])            |  1.492 | 0.112 |\n",
            "| transformer.resblocks.5.mlp.c_fc.bias         | torch.Size([3072])           | -0.331 | 0.181 |\n",
            "| transformer.resblocks.5.mlp.c_fc.weight       | torch.Size([3072, 768])      |  0     | 0.014 |\n",
            "| transformer.resblocks.5.mlp.c_proj.bias       | torch.Size([768])            |  0     | 0.053 |\n",
            "| transformer.resblocks.5.mlp.c_proj.weight     | torch.Size([768, 3072])      | -0     | 0.011 |\n",
            "| transformer.resblocks.6.attn.in_proj_bias     | torch.Size([2304])           | -0.006 | 0.303 |\n",
            "| transformer.resblocks.6.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.6.attn.out_proj.bias    | torch.Size([768])            |  0.001 | 0.085 |\n",
            "| transformer.resblocks.6.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| transformer.resblocks.6.ln_1.bias             | torch.Size([768])            |  0.006 | 0.21  |\n",
            "| transformer.resblocks.6.ln_1.weight           | torch.Size([768])            |  1.369 | 0.117 |\n",
            "| transformer.resblocks.6.ln_2.bias             | torch.Size([768])            | -0.051 | 0.462 |\n",
            "| transformer.resblocks.6.ln_2.weight           | torch.Size([768])            |  1.649 | 0.141 |\n",
            "| transformer.resblocks.6.mlp.c_fc.bias         | torch.Size([3072])           | -0.359 | 0.192 |\n",
            "| transformer.resblocks.6.mlp.c_fc.weight       | torch.Size([3072, 768])      |  0     | 0.015 |\n",
            "| transformer.resblocks.6.mlp.c_proj.bias       | torch.Size([768])            | -0     | 0.062 |\n",
            "| transformer.resblocks.6.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.012 |\n",
            "| transformer.resblocks.7.attn.in_proj_bias     | torch.Size([2304])           |  0.015 | 0.308 |\n",
            "| transformer.resblocks.7.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.7.attn.out_proj.bias    | torch.Size([768])            |  0     | 0.113 |\n",
            "| transformer.resblocks.7.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| transformer.resblocks.7.ln_1.bias             | torch.Size([768])            |  0.016 | 0.16  |\n",
            "| transformer.resblocks.7.ln_1.weight           | torch.Size([768])            |  1.471 | 0.105 |\n",
            "| transformer.resblocks.7.ln_2.bias             | torch.Size([768])            | -0.095 | 0.464 |\n",
            "| transformer.resblocks.7.ln_2.weight           | torch.Size([768])            |  1.945 | 0.172 |\n",
            "| transformer.resblocks.7.mlp.c_fc.bias         | torch.Size([3072])           | -0.368 | 0.188 |\n",
            "| transformer.resblocks.7.mlp.c_fc.weight       | torch.Size([3072, 768])      |  0     | 0.015 |\n",
            "| transformer.resblocks.7.mlp.c_proj.bias       | torch.Size([768])            | -0     | 0.087 |\n",
            "| transformer.resblocks.7.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.013 |\n",
            "| transformer.resblocks.8.attn.in_proj_bias     | torch.Size([2304])           |  0.009 | 0.334 |\n",
            "| transformer.resblocks.8.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.8.attn.out_proj.bias    | torch.Size([768])            |  0     | 0.046 |\n",
            "| transformer.resblocks.8.attn.out_proj.weight  | torch.Size([768, 768])       |  0     | 0.013 |\n",
            "| transformer.resblocks.8.ln_1.bias             | torch.Size([768])            |  0.021 | 0.163 |\n",
            "| transformer.resblocks.8.ln_1.weight           | torch.Size([768])            |  1.598 | 0.115 |\n",
            "| transformer.resblocks.8.ln_2.bias             | torch.Size([768])            | -0.038 | 0.509 |\n",
            "| transformer.resblocks.8.ln_2.weight           | torch.Size([768])            |  2.693 | 0.259 |\n",
            "| transformer.resblocks.8.mlp.c_fc.bias         | torch.Size([3072])           | -0.379 | 0.216 |\n",
            "| transformer.resblocks.8.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.8.mlp.c_proj.bias       | torch.Size([768])            |  0     | 0.125 |\n",
            "| transformer.resblocks.8.mlp.c_proj.weight     | torch.Size([768, 3072])      |  0     | 0.014 |\n",
            "| transformer.resblocks.9.attn.in_proj_bias     | torch.Size([2304])           |  0.011 | 0.331 |\n",
            "| transformer.resblocks.9.attn.in_proj_weight   | torch.Size([2304, 768])      |  0     | 0.016 |\n",
            "| transformer.resblocks.9.attn.out_proj.bias    | torch.Size([768])            |  0.002 | 0.106 |\n",
            "| transformer.resblocks.9.attn.out_proj.weight  | torch.Size([768, 768])       | -0     | 0.014 |\n",
            "| transformer.resblocks.9.ln_1.bias             | torch.Size([768])            |  0.031 | 0.159 |\n",
            "| transformer.resblocks.9.ln_1.weight           | torch.Size([768])            |  1.801 | 0.114 |\n",
            "| transformer.resblocks.9.ln_2.bias             | torch.Size([768])            |  0.057 | 0.665 |\n",
            "| transformer.resblocks.9.ln_2.weight           | torch.Size([768])            |  4.431 | 0.438 |\n",
            "| transformer.resblocks.9.mlp.c_fc.bias         | torch.Size([3072])           | -0.382 | 0.241 |\n",
            "| transformer.resblocks.9.mlp.c_fc.weight       | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| transformer.resblocks.9.mlp.c_proj.bias       | torch.Size([768])            |  0.001 | 0.113 |\n",
            "| transformer.resblocks.9.mlp.c_proj.weight     | torch.Size([768, 3072])      | -0     | 0.015 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Map layer names\n",
        "\n",
        "def dfs(k, v, converted_torch_weight):\n",
        "  \"\"\"Recursively match weights.\"\"\"\n",
        "  if isinstance(v, jnp.ndarray):\n",
        "    if k in converted_torch_weight:\n",
        "      torch_data = converted_torch_weight[k]\n",
        "      if len(v.shape) == 2 and 'embedding' not in k and 'rel_pos' not in k:\n",
        "        torch_data = np.transpose(torch_data, (1, 0))\n",
        "      if len(v.shape) == 4:\n",
        "        if 'fpn_stride_16_8' in k:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 0, 1))\n",
        "        else:\n",
        "          torch_data = np.transpose(torch_data, (2, 3, 1, 0))\n",
        "      if torch_data.shape != v.shape:\n",
        "        print('Wrong shape! {} {} {}'.format(\n",
        "            k, torch_data.shape, v.shape))\n",
        "    else:\n",
        "      print(f'{k} not in checkpoint')\n",
        "      torch_data = v\n",
        "    return [(k, torch_data.shape)], torch_data\n",
        "  lst, tree = [], {}\n",
        "  for kk, vv in v.items():\n",
        "    if isinstance(vv, jnp.ndarray) and (\n",
        "        kk == 'kernel' or kk == 'scale' or kk == 'embedding'):\n",
        "      if 'proposal_generator.scales' not in k:\n",
        "        new_kk = 'weight'\n",
        "      else:\n",
        "        new_kk = kk\n",
        "    else:\n",
        "      new_kk = kk\n",
        "    sub_lst, sub_tree = dfs(\n",
        "        '{}.{}'.format(k, new_kk) if k else new_kk,\n",
        "        vv,\n",
        "        converted_torch_weight)\n",
        "    lst.extend(sub_lst)\n",
        "    tree[kk] = sub_tree\n",
        "  return lst, tree\n",
        "\n",
        "COMMEN_NAME_MAP = [\n",
        "    ('transformer.', ''),\n",
        "    ('resblocks', 'blocks'),\n",
        "    ('in_proj_bias', 'qkv.bias'),\n",
        "    ('in_proj_weight', 'qkv.weight'),\n",
        "    ('out_proj', 'proj'),\n",
        "    ('ln_1', 'norm1'),\n",
        "    ('ln_2', 'norm2'),\n",
        "    ('c_fc', 'fc1'),\n",
        "    ('c_proj', 'fc2'),\n",
        "    ('conv1', 'patch_embed.proj'),\n",
        "    ('positional_embedding', 'pos_embed'),\n",
        "]\n",
        "\n",
        "def map_names(state_dict, name_map):\n",
        "  \"\"\"Change names according to a pre-defined map.\"\"\"\n",
        "  ret = {}\n",
        "  for k, v in state_dict.items():\n",
        "    new_k = k\n",
        "    for ori_name, new_name in name_map:\n",
        "      new_k = new_k.replace(ori_name, new_name)\n",
        "    ret[new_k] = v\n",
        "  return ret\n",
        "\n",
        "converted_torch_weight = map_names(torch_weights, COMMEN_NAME_MAP)\n",
        "remove_keys = ['class_embedding', 'ln_post.bias', 'ln_post.weight', 'proj']\n",
        "for k in remove_keys:\n",
        "  del converted_torch_weight[k]\n",
        "converted_torch_weight['pos_embed'] = converted_torch_weight['pos_embed'][None]\n",
        "converted_torch_weight = {k: v.numpy() for k, v in converted_torch_weight.items()}\n",
        "# for k, v in converted_torch_weight.items():\n",
        "#   print(k, v.shape)"
      ],
      "metadata": {
        "id": "Lovw3WQKIOjC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title JAX ViT implementation\n",
        "\"\"\"ViTDet with simple FPN.\"\"\"\n",
        "\n",
        "import functools\n",
        "from typing import Any, Optional\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "\n",
        "KERNEL_INIT = {\n",
        "    'normal': nn.initializers.normal(stddev=0.02),\n",
        "}\n",
        "\n",
        "__all__ = ['ViT', 'SimpleFeaturePyramid']\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Multi-head Attention block with relative position embeddings.\n",
        "\n",
        "  Attributes:\n",
        "  dim (int): Number of input channels.\n",
        "  num_heads (int): Number of attention heads.\n",
        "  qkv_bias (bool:  If True, add a learnable bias to query, key, value.\n",
        "  use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "    attention map.\n",
        "  rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "    parameters.\n",
        "  input_size (int or None): Input resolution for calculating the relative\n",
        "    positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int = 8\n",
        "  qkv_bias: bool = True\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  input_size: Optional[Any] = None\n",
        "  kernel_init: str = 'normal'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def get_rel_pos(self, q_size, k_size, rel_pos):\n",
        "    \"\"\"Get relative positional embeddings.\n",
        "\n",
        "    Args:\n",
        "      q_size (int): size of query q.\n",
        "      k_size (int): size of key k.\n",
        "      rel_pos (Tensor): relative position embeddings (L, C).\n",
        "    Returns:\n",
        "      Extracted positional embeddings according to relative positions.\n",
        "    \"\"\"\n",
        "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
        "    # Interpolate rel pos if needed.\n",
        "    if rel_pos.shape[0] != max_rel_dist:\n",
        "      # Interpolate rel pos.\n",
        "      rel_pos_resized = jax.image.resize(\n",
        "          rel_pos,\n",
        "          shape=(max_rel_dist, rel_pos.shape[1]),\n",
        "          method='linear',\n",
        "      )\n",
        "    else:\n",
        "      rel_pos_resized = rel_pos\n",
        "\n",
        "    # Scale the coords with short length if shapes for q and k are different.\n",
        "    q_coords = jnp.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
        "    k_coords = jnp.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
        "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(\n",
        "        q_size / k_size, 1.0)\n",
        "    relative_coords = relative_coords.astype(jnp.int32).reshape(-1)\n",
        "    return rel_pos_resized[relative_coords].reshape(q_size, k_size, -1)\n",
        "\n",
        "  def add_decomposed_rel_pos(\n",
        "      self, attn, q, rel_pos_h, rel_pos_w, q_size, k_size):\n",
        "    \"\"\"Calculate decomposed Relative Positional Embeddings from paper:`mvitv2`.\n",
        "\n",
        "    Args:\n",
        "      attn (Tensor): attention map.\n",
        "      q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
        "      rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
        "      rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
        "      q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
        "      k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
        "    Returns:\n",
        "      attn (Tensor): attention map with added relative positional embeddings.\n",
        "    \"\"\"\n",
        "    q_h, q_w = q_size\n",
        "    k_h, k_w = k_size\n",
        "    rh = self.get_rel_pos(q_h, k_h, rel_pos_h)\n",
        "    rw = self.get_rel_pos(q_w, k_w, rel_pos_w)\n",
        "\n",
        "    batch, _, dim = q.shape\n",
        "    r_q = q.reshape(batch, q_h, q_w, dim)\n",
        "    rel_h = jnp.einsum('bhwc,hkc->bhwk', r_q, rh)\n",
        "    rel_w = jnp.einsum('bhwc,wkc->bhwk', r_q, rw)\n",
        "\n",
        "    attn = (\n",
        "        attn.reshape(batch, q_h, q_w, k_h, k_w) + rel_h[\n",
        "            :, :, :, :, None] + rel_w[:, :, :, None, :]\n",
        "    ).reshape(batch, q_h * q_w, k_h * k_w)\n",
        "\n",
        "    return attn\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    batch, height, width, _ = x.shape\n",
        "    head_dim = self.dim // self.num_heads\n",
        "    qkv = nn.Dense(self.dim * 3, use_bias=self.qkv_bias, name='qkv')(\n",
        "        x)  # batch x height x width x 3dim\n",
        "    qkv = qkv.reshape(batch, height * width, 3, self.num_heads, -1).transpose(\n",
        "        2, 0, 3, 1, 4)  # 3 x batch x num_heads x num_tokens x D\n",
        "    qkv = qkv.reshape(3, batch * self.num_heads, height * width, -1)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]  # [batch * num_heads, num_tokens, D]\n",
        "    attn = (q * (head_dim ** -0.5)) @ k.transpose(\n",
        "        0, 2, 1)  # [batch * num_heads, num_tokens, num_tokens]\n",
        "    if self.use_rel_pos:\n",
        "      rel_pos_h = self.param(\n",
        "          'rel_pos_h', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      rel_pos_w = self.param(\n",
        "          'rel_pos_w', nn.initializers.zeros,\n",
        "          (2 * self.input_size[0] - 1, head_dim))\n",
        "      attn = self.add_decomposed_rel_pos(\n",
        "          attn, q, rel_pos_h, rel_pos_w,\n",
        "          (height, width), (height, width))\n",
        "    attn = jax.nn.softmax(attn)\n",
        "    x = (attn @ v).reshape(batch, self.num_heads, height, width, -1).transpose(\n",
        "        0, 2, 3, 1, 4).reshape(batch, height, width, -1)\n",
        "    x = nn.Dense(self.dim, name='proj')(x)\n",
        "    return x\n",
        "\n",
        "def quick_gelu(x: jnp.ndarray) -> jnp.ndarray:\n",
        "  return x * jax.nn.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "\n",
        "  hidden_features: int\n",
        "  out_features: int\n",
        "  kernel_init: str = 'normal'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(\n",
        "        self.hidden_features, dtype=self.dtype,\n",
        "        kernel_init=KERNEL_INIT[self.kernel_init], name='fc1')(x)\n",
        "    # x = nn.gelu(x, approximate=False)\n",
        "    x = quick_gelu(x)\n",
        "    x = nn.Dense(\n",
        "        self.out_features, dtype=self.dtype,\n",
        "        kernel_init=KERNEL_INIT[self.kernel_init], name='fc2')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer blocks with support of window attention and residual blocks.\n",
        "\n",
        "  Attributes:\n",
        "    dim (int): Number of input channels.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    drop_path (float): Stochastic depth rate.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks. If it equals 0,\n",
        "      then not use window attention.\n",
        "    input_size (int or None): Input resolution for calculating the relative\n",
        "      positional parameter size.\n",
        "  \"\"\"\n",
        "  dim: int\n",
        "  num_heads: int\n",
        "  mlp_ratio: float = 4.0\n",
        "  qkv_bias: bool = True\n",
        "  drop_path: float = 0.0\n",
        "  use_rel_pos: bool = False\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 0\n",
        "  input_size: Optional[int] = None\n",
        "  layer_scale_init_value: float = -1.0\n",
        "  kernel_init: str = 'normal'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def window_partition(self, x):\n",
        "    \"\"\"Partition into non-overlapping windows with padding if needed.\n",
        "\n",
        "    Args:\n",
        "      x (array): input tokens with [B, H, W, C].\n",
        "    Returns:\n",
        "      windows: windows after partition with [B * num_windows, window_size,\n",
        "        window_size, C].\n",
        "      (Hp, Wp): padded height and width before partition\n",
        "    \"\"\"\n",
        "    batch, h, w, c = x.shape\n",
        "\n",
        "    pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
        "    pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
        "    if pad_h > 0 or pad_w > 0:\n",
        "      x = jnp.pad(\n",
        "          x, ((0, 0), (0, pad_w), (0, pad_h), (0, 0)),\n",
        "          'constant', constant_values=0)\n",
        "    hp, wp = h + pad_h, w + pad_w\n",
        "\n",
        "    x = x.reshape(\n",
        "        batch, hp // self.window_size, self.window_size,\n",
        "        wp // self.window_size, self.window_size, c)\n",
        "    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(\n",
        "        -1, self.window_size, self.window_size, c)\n",
        "    return windows, (hp, wp)\n",
        "\n",
        "  def window_unpartition(self, windows, pad_hw, hw):\n",
        "    \"\"\"Window unpartition into original sequences and removing padding.\n",
        "\n",
        "    Args:\n",
        "      windows (array): inputs: [B * num_windows, window_size, window_size, C].\n",
        "      pad_hw (Tuple): padded height and width (Hp, Wp).\n",
        "      hw (Tuple): original height and width (H, W) before padding.\n",
        "\n",
        "    Returns:\n",
        "      x: unpartitioned sequences with [B, H, W, C].\n",
        "    \"\"\"\n",
        "    hp, wp = pad_hw\n",
        "    h, w = hw\n",
        "    batch = windows.shape[0] // (\n",
        "        hp * wp // self.window_size // self.window_size)\n",
        "    x = windows.reshape(\n",
        "        batch,\n",
        "        hp // self.window_size, wp // self.window_size,\n",
        "        self.window_size, self.window_size, -1)\n",
        "    x = x.transpose(0, 1, 3, 2, 4, 5).reshape(batch, hp, wp, -1)\n",
        "    if hp > h or wp > w:\n",
        "      x = x[:, :h, :w, :]\n",
        "    return x\n",
        "\n",
        "  def get_keep_pattern(self,\n",
        "                       x: jnp.ndarray,\n",
        "                       deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"DropPath Layer.\"\"\"\n",
        "    if not deterministic and self.drop_path:\n",
        "      shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "      drop_pattern = jax.random.bernoulli(\n",
        "          self.make_rng('dropout'), self.drop_path, shape).astype(self.dtype)\n",
        "      keep_pattern = (1. - drop_pattern)\n",
        "      if self.drop_path < 1.:\n",
        "        keep_pattern = keep_pattern / (1. - self.drop_path)\n",
        "      return keep_pattern\n",
        "    else:\n",
        "      return 1.0\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train = False):\n",
        "    shortcut = x\n",
        "    ln = functools.partial(nn.LayerNorm, epsilon=1e-6)\n",
        "    x = ln(name='norm1')(x)\n",
        "    # Window partition\n",
        "    if self.window_size > 0:\n",
        "      h, w = x.shape[1], x.shape[2]\n",
        "      x, pad_hw = self.window_partition(x)\n",
        "\n",
        "    x = Attention(\n",
        "        self.dim,\n",
        "        num_heads=self.num_heads,\n",
        "        qkv_bias=self.qkv_bias,\n",
        "        use_rel_pos=self.use_rel_pos,\n",
        "        rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "        input_size=self.input_size if self.window_size == 0 else (\n",
        "            self.window_size, self.window_size),\n",
        "        name='attn')(x)\n",
        "    # Reverse window partition\n",
        "    if self.window_size > 0:\n",
        "      x = self.window_unpartition(x, pad_hw, (h, w))\n",
        "\n",
        "    if self.layer_scale_init_value > 0:\n",
        "      gamma_1 = self.param(\n",
        "          'gamma_1',\n",
        "          nn.initializers.constant(self.layer_scale_init_value),\n",
        "          (self.dim))\n",
        "      x = x * gamma_1[..., :]\n",
        "    x = shortcut + self.get_keep_pattern(x, not train) * x\n",
        "\n",
        "    y = ln(name='norm2')(x)\n",
        "    y = Mlp(\n",
        "        int(self.dim * self.mlp_ratio),\n",
        "        self.dim,\n",
        "        kernel_init=self.kernel_init,\n",
        "        dtype=self.dtype,\n",
        "        name='mlp')(y)\n",
        "    if self.layer_scale_init_value > 0:\n",
        "      gamma_2 = self.param(\n",
        "          'gamma_2',\n",
        "          nn.initializers.constant(self.layer_scale_init_value),\n",
        "          (self.dim))\n",
        "      y = y * gamma_2[..., :]\n",
        "    x = x + self.get_keep_pattern(y, not train) * y\n",
        "    return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "  \"\"\"This module implements Vision Transformer (ViT) backbone in paper:`vitdet`.\n",
        "\n",
        "  \"Exploring Plain Vision Transformer Backbones for Object Detection\",\n",
        "  https://arxiv.org/abs/2203.16527\n",
        "\n",
        "  Attributes:\n",
        "    img_size (int): Input image size.\n",
        "    patch_size (int): Patch size.\n",
        "    in_chans (int): Number of input image channels.\n",
        "    embed_dim (int): Patch embedding dimension.\n",
        "    depth (int): Depth of ViT.\n",
        "    num_heads (int): Number of attention heads in each ViT block.\n",
        "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "    qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "    drop_path_rate (float): Stochastic depth rate.\n",
        "    use_abs_pos (bool): If True, use absolute positional embeddings.\n",
        "    use_rel_pos (bool): If True, add relative positional embeddings to the\n",
        "      attention map.\n",
        "    rel_pos_zero_init (bool): If True, zero initialize relative positional\n",
        "      parameters.\n",
        "    window_size (int): Window size for window attention blocks.\n",
        "    window_block_indexes (list): Indexes for blocks using window attention.\n",
        "    pretrain_img_size (int): input image size for pretraining models.\n",
        "    pretrain_use_cls_token (bool): If True, pretrainig models use class token.\n",
        "  \"\"\"\n",
        "  img_size: int = 1024\n",
        "  patch_size: int = 16\n",
        "  in_chans: int = 3\n",
        "  embed_dim: int = 768\n",
        "  depth: int = 12\n",
        "  num_heads: int = 12\n",
        "  mlp_ratio: float = 4.0\n",
        "  qkv_bias: bool = True\n",
        "  drop_path_rate: float = 0.1\n",
        "  use_abs_pos: bool = True\n",
        "  use_rel_pos: bool = True\n",
        "  rel_pos_zero_init: bool = True\n",
        "  window_size: int = 14\n",
        "  window_block_indexes: Any = (0, 1, 3, 4, 6, 7, 9, 10)\n",
        "  pretrain_img_size: int = 224\n",
        "  pretrain_use_cls_token: bool = True\n",
        "  layer_scale_init_value: float = -1.0\n",
        "  kernel_init: str = 'normal'\n",
        "  with_cls_token: bool = False\n",
        "  use_ln_pre: bool = False\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  def _get_abs_pos(self, abs_pos, hw):\n",
        "    \"\"\"Calculate absolute positional embeddings.\n",
        "\n",
        "    If needed, resize embeddings and remove cls_token dimension for the original\n",
        "      embeddings.\n",
        "    Args:\n",
        "      abs_pos (array): absolute positional embeddings with (1, num_position, C).\n",
        "      hw (Tuple): size of input image tokens.\n",
        "    Returns:\n",
        "      Absolute positional embeddings after processing with shape (1, H, W, C)\n",
        "    \"\"\"\n",
        "    h, w = hw\n",
        "    if self.pretrain_use_cls_token:\n",
        "      abs_pos = abs_pos[:, 1:]\n",
        "    xy_num = abs_pos.shape[1]\n",
        "    size = int(xy_num ** 0.5)\n",
        "    assert size * size == xy_num\n",
        "    abs_pos = abs_pos.reshape(abs_pos.shape[0], size, size, -1)\n",
        "    if size != h or size != w:\n",
        "      new_abs_pos = jax.image.resize(\n",
        "          abs_pos,\n",
        "          (abs_pos.shape[0], h, w, abs_pos.shape[3]),\n",
        "          method='bicubic',\n",
        "      )\n",
        "    else:\n",
        "      new_abs_pos = abs_pos\n",
        "    return new_abs_pos\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, train: bool = False):\n",
        "    print('input', x.shape)\n",
        "    x = nn.Conv(\n",
        "        self.embed_dim, (self.patch_size, self.patch_size),\n",
        "        strides=(self.patch_size, self.patch_size),\n",
        "        padding='VALID',\n",
        "        name='patch_embed.proj')(x)\n",
        "    print('after conv', x.shape, x[0, 0, 0, :10])\n",
        "    if self.use_abs_pos:\n",
        "      num_patches = (self.pretrain_img_size // self.patch_size) ** 2\n",
        "      num_positions = (\n",
        "          num_patches + 1) if self.pretrain_use_cls_token else num_patches\n",
        "      pos_embed = self.param(\n",
        "          'pos_embed', nn.initializers.zeros,\n",
        "          (1, num_positions, self.embed_dim))\n",
        "      x = x + self._get_abs_pos(pos_embed, (x.shape[1], x.shape[2]))\n",
        "    print('after pos emb', x.shape, x[0, 0, 0, :10])\n",
        "    if self.with_cls_token:\n",
        "      cls_token = self.param(\n",
        "          'cls_token', nn.initializers.zeros, (1, 1, self.embed_dim))\n",
        "    dp_rates = [\n",
        "        self.drop_path_rate * i / (self.depth - 1) for i in range(self.depth)]\n",
        "    if self.use_ln_pre:\n",
        "      x = nn.LayerNorm(name='ln_pre')(x)\n",
        "    print('after ln pre', x.shape, x[0, 0, 0, :10])\n",
        "    for i in range(self.depth):\n",
        "      x = Block(\n",
        "          dim=self.embed_dim,\n",
        "          num_heads=self.num_heads,\n",
        "          mlp_ratio=self.mlp_ratio,\n",
        "          qkv_bias=self.qkv_bias,\n",
        "          drop_path=dp_rates[i],\n",
        "          use_rel_pos=self.use_rel_pos,\n",
        "          rel_pos_zero_init=self.rel_pos_zero_init,\n",
        "          window_size=self.window_size if i in self.window_block_indexes else 0,\n",
        "          input_size=(\n",
        "              self.img_size // self.patch_size,\n",
        "              self.img_size // self.patch_size),\n",
        "          layer_scale_init_value=self.layer_scale_init_value,\n",
        "          name=f'blocks.{i}',\n",
        "          )(x, train=train)\n",
        "      print(f'after block {i}', x.shape, x[0, 0, 0, :10])\n",
        "    return x\n",
        "\n",
        "\n",
        "SIZE_CONFIGS = {\n",
        "    'B': (768, 12, 12, 0.1, (0, 1, 3, 4, 6, 7, 9, 10)),\n",
        "    'L': (1024, 24, 16, 0.4, (\n",
        "        0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22)),\n",
        "    'H': (1280, 32, 16, 0.5, (\n",
        "        0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21,\n",
        "        22, 24, 25, 26, 27, 28, 29, 30)),\n",
        "}\n"
      ],
      "metadata": {
        "id": "vQqESU6MILoT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import ml_collections\n",
        "backbone_args = ml_collections.ConfigDict()\n",
        "sz = 'B'  # backbone_args.pop('size', 'B')\n",
        "dim, depth, num_heads, dp, window_block_indexes = SIZE_CONFIGS[sz]\n",
        "backbone_args['embed_dim'] = backbone_args.get(\n",
        "    'embed_dim', dim)\n",
        "backbone_args['depth'] = backbone_args.get('depth', depth)\n",
        "backbone_args['num_heads'] = backbone_args.get(\n",
        "    'num_heads', num_heads)\n",
        "backbone_args['drop_path_rate'] = backbone_args.get(\n",
        "    'drop_path_rate', dp)\n",
        "backbone_args['window_block_indexes'] = backbone_args.get(\n",
        "    'window_block_indexes', window_block_indexes)\n",
        "backbone_args['use_rel_pos'] = True\n",
        "backbone_args['use_ln_pre'] = True\n",
        "vit_model = ViT(**backbone_args)\n",
        "\n",
        "rng = {'dropout': jax.random.PRNGKey(0), 'params': jax.random.PRNGKey(0)}\n",
        "input = jax.random.normal(jax.random.PRNGKey(0), (1, 1024, 1024, 3))\n",
        "vit_vars = vit_model.init(rng, input)"
      ],
      "metadata": {
        "id": "CR7nggeHSBKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb81388-ae68-447a-84e0-78166dad9b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (1, 1024, 1024, 3)\n",
            "after conv (1, 64, 64, 768) [ 0.47106028  1.3287854   0.83192176 -0.83944285  1.0247202  -0.48860377\n",
            " -0.12125206  0.8328862   1.0072198  -0.8418225 ]\n",
            "after pos emb (1, 64, 64, 768) [ 0.47106028  1.3287854   0.83192176 -0.83944285  1.0247202  -0.48860377\n",
            " -0.12125206  0.8328862   1.0072198  -0.8418225 ]\n",
            "after ln pre (1, 64, 64, 768) [ 0.43739814  1.274982    0.7897858  -0.84233147  0.97805685 -0.49973089\n",
            " -0.1410054   0.7907276   0.9609675  -0.8446553 ]\n",
            "after block 0 (1, 64, 64, 768) [ 0.9527342   1.6842506   1.1183987  -0.620758    1.0156078  -0.43444997\n",
            "  0.40971595  0.77097815  1.5544728  -1.0375676 ]\n",
            "after block 1 (1, 64, 64, 768) [ 0.9354738   1.1514326   1.3138807  -0.7885489   0.6950543  -0.48360264\n",
            " -0.47691417  1.2413012   1.6396505  -0.9739183 ]\n",
            "after block 2 (1, 64, 64, 768) [ 0.54650915  0.7949942   1.1095765  -0.34691972  0.42791653 -0.5394424\n",
            " -0.71003187  1.6934314   1.8298969  -0.83660793]\n",
            "after block 3 (1, 64, 64, 768) [ 0.40429586  0.07155712  0.8348436  -0.06377494 -0.03645796  0.24721247\n",
            " -0.7760283   1.8129752   1.9634175  -1.528043  ]\n",
            "after block 4 (1, 64, 64, 768) [ 0.8157657   0.75672746  1.0047448  -0.47795695 -0.69131     0.7423003\n",
            " -1.9451964   2.0043      1.9063951  -2.893767  ]\n",
            "after block 5 (1, 64, 64, 768) [ 0.9916612   1.1453046   0.5037965  -0.45060664 -0.27026373  0.37618995\n",
            " -2.0273547   2.4010115   1.6583372  -2.2781038 ]\n",
            "after block 6 (1, 64, 64, 768) [ 2.605226    0.87504095  0.3976016  -0.68965334 -0.12614334 -0.8499212\n",
            " -1.5159993   3.1885996   1.333512   -1.5837252 ]\n",
            "after block 7 (1, 64, 64, 768) [ 3.7126975   1.8546289   1.0695984   0.18318185 -1.2842928  -0.8293556\n",
            " -1.4501106   3.8846254   1.3534719  -1.876102  ]\n",
            "after block 8 (1, 64, 64, 768) [ 4.4966407   0.15326953  1.3065652   0.17217928 -1.8763049   0.23148686\n",
            " -1.7842715   3.866251    0.7179457  -2.4412112 ]\n",
            "after block 9 (1, 64, 64, 768) [ 4.938929   -0.18077338  2.8346472   0.6425916  -3.0323238   0.76069295\n",
            " -1.1072111   4.440237    0.65689665 -4.698646  ]\n",
            "after block 10 (1, 64, 64, 768) [ 5.62897    -0.08148676  3.078935    1.9535469  -3.6440432   2.1288352\n",
            " -0.86140555  3.0621972  -0.1735726  -4.5019507 ]\n",
            "after block 11 (1, 64, 64, 768) [ 4.354451    0.6402224   2.5300777   1.9537549  -3.751364    0.54613084\n",
            " -1.0776676   2.833622    0.11561376 -3.6275783 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret, tree = dfs('', vit_vars['params'], converted_torch_weight)\n",
        "num_params = 0\n",
        "for k, v in converted_torch_weight.items():\n",
        "  num_params += np.prod(v.shape)\n",
        "print('#params in loaded model:', num_params)\n",
        "num_params = 0\n",
        "for k, v in ret:\n",
        "  if 'rel_pos' not in k:\n",
        "    num_params += np.prod(v)\n",
        "print('#params in converted model:', num_params)\n",
        "print('#params in converted model w.o proj bias:', num_params - 768)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrgNQVbYSYzG",
        "outputId": "aea64b0f-f8e5-4076-d761-1a652f754c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patch_embed.proj.bias not in checkpoint\n",
            "blocks.0.attn.rel_pos_h not in checkpoint\n",
            "blocks.0.attn.rel_pos_w not in checkpoint\n",
            "blocks.1.attn.rel_pos_h not in checkpoint\n",
            "blocks.1.attn.rel_pos_w not in checkpoint\n",
            "blocks.2.attn.rel_pos_h not in checkpoint\n",
            "blocks.2.attn.rel_pos_w not in checkpoint\n",
            "blocks.3.attn.rel_pos_h not in checkpoint\n",
            "blocks.3.attn.rel_pos_w not in checkpoint\n",
            "blocks.4.attn.rel_pos_h not in checkpoint\n",
            "blocks.4.attn.rel_pos_w not in checkpoint\n",
            "blocks.5.attn.rel_pos_h not in checkpoint\n",
            "blocks.5.attn.rel_pos_w not in checkpoint\n",
            "blocks.6.attn.rel_pos_h not in checkpoint\n",
            "blocks.6.attn.rel_pos_w not in checkpoint\n",
            "blocks.7.attn.rel_pos_h not in checkpoint\n",
            "blocks.7.attn.rel_pos_w not in checkpoint\n",
            "blocks.8.attn.rel_pos_h not in checkpoint\n",
            "blocks.8.attn.rel_pos_w not in checkpoint\n",
            "blocks.9.attn.rel_pos_h not in checkpoint\n",
            "blocks.9.attn.rel_pos_w not in checkpoint\n",
            "blocks.10.attn.rel_pos_h not in checkpoint\n",
            "blocks.10.attn.rel_pos_w not in checkpoint\n",
            "blocks.11.attn.rel_pos_h not in checkpoint\n",
            "blocks.11.attn.rel_pos_w not in checkpoint\n",
            "#params in loaded model: 85797120\n",
            "#params in converted model: 85797888\n",
            "#params in converted model w.o proj bias: 85797120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_tree = flax.traverse_util.flatten_dict(tree)\n",
        "table = []\n",
        "for k in sorted(flattened_tree):\n",
        "  if 'rel_pos_h' in k or 'rel_pos_w' in k:\n",
        "    continue\n",
        "  v = flattened_tree[k]\n",
        "  table.append((k, f'{v.shape}', f'{v.mean():.3f}', f'{v.std():.3f}'))\n",
        "table_str = tabulate(\n",
        "    table, tablefmt=\"pipe\", headers=[\"Names\", \"shape\", \"mean\", \"std\"])\n",
        "print(table_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHeZV8xs57eA",
        "outputId": "4fcd65fa-1053-4fc4-9736-25d8119e10b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Names                                   | shape                        |   mean |   std |\n",
            "|:----------------------------------------|:-----------------------------|-------:|------:|\n",
            "| ('blocks.0', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0.001 | 0.043 |\n",
            "| ('blocks.0', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       | -0     | 0.011 |\n",
            "| ('blocks.0', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.008 | 0.538 |\n",
            "| ('blocks.0', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      | -0     | 0.014 |\n",
            "| ('blocks.0', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.455 | 0.336 |\n",
            "| ('blocks.0', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.015 |\n",
            "| ('blocks.0', 'mlp', 'fc2', 'bias')      | torch.Size([768])            | -0.004 | 0.074 |\n",
            "| ('blocks.0', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.009 |\n",
            "| ('blocks.0', 'norm1', 'bias')           | torch.Size([768])            | -0.014 | 0.181 |\n",
            "| ('blocks.0', 'norm1', 'scale')          | torch.Size([768])            |  0.493 | 0.335 |\n",
            "| ('blocks.0', 'norm2', 'bias')           | torch.Size([768])            |  0.023 | 0.223 |\n",
            "| ('blocks.0', 'norm2', 'scale')          | torch.Size([768])            |  0.919 | 0.401 |\n",
            "| ('blocks.1', 'attn', 'proj', 'bias')    | torch.Size([768])            | -0.001 | 0.081 |\n",
            "| ('blocks.1', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.011 |\n",
            "| ('blocks.1', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.005 | 0.474 |\n",
            "| ('blocks.1', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.018 |\n",
            "| ('blocks.1', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.392 | 0.221 |\n",
            "| ('blocks.1', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.1', 'mlp', 'fc2', 'bias')      | torch.Size([768])            | -0.005 | 0.102 |\n",
            "| ('blocks.1', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      | -0     | 0.01  |\n",
            "| ('blocks.1', 'norm1', 'bias')           | torch.Size([768])            |  0.001 | 0.159 |\n",
            "| ('blocks.1', 'norm1', 'scale')          | torch.Size([768])            |  0.824 | 0.243 |\n",
            "| ('blocks.1', 'norm2', 'bias')           | torch.Size([768])            |  0.009 | 0.171 |\n",
            "| ('blocks.1', 'norm2', 'scale')          | torch.Size([768])            |  1.172 | 0.474 |\n",
            "| ('blocks.10', 'attn', 'proj', 'bias')   | torch.Size([768])            |  0     | 0.088 |\n",
            "| ('blocks.10', 'attn', 'proj', 'kernel') | torch.Size([768, 768])       | -0     | 0.015 |\n",
            "| ('blocks.10', 'attn', 'qkv', 'bias')    | torch.Size([2304])           |  0.008 | 0.395 |\n",
            "| ('blocks.10', 'attn', 'qkv', 'kernel')  | torch.Size([768, 2304])      | -0     | 0.016 |\n",
            "| ('blocks.10', 'mlp', 'fc1', 'bias')     | torch.Size([3072])           | -0.4   | 0.178 |\n",
            "| ('blocks.10', 'mlp', 'fc1', 'kernel')   | torch.Size([768, 3072])      |  0     | 0.015 |\n",
            "| ('blocks.10', 'mlp', 'fc2', 'bias')     | torch.Size([768])            | -0.002 | 0.1   |\n",
            "| ('blocks.10', 'mlp', 'fc2', 'kernel')   | torch.Size([3072, 768])      | -0     | 0.017 |\n",
            "| ('blocks.10', 'norm1', 'bias')          | torch.Size([768])            |  0.049 | 0.21  |\n",
            "| ('blocks.10', 'norm1', 'scale')         | torch.Size([768])            |  2.018 | 0.146 |\n",
            "| ('blocks.10', 'norm2', 'bias')          | torch.Size([768])            |  0.011 | 0.587 |\n",
            "| ('blocks.10', 'norm2', 'scale')         | torch.Size([768])            |  4.746 | 0.409 |\n",
            "| ('blocks.11', 'attn', 'proj', 'bias')   | torch.Size([768])            | -0.003 | 0.302 |\n",
            "| ('blocks.11', 'attn', 'proj', 'kernel') | torch.Size([768, 768])       |  0     | 0.018 |\n",
            "| ('blocks.11', 'attn', 'qkv', 'bias')    | torch.Size([2304])           |  0.002 | 0.179 |\n",
            "| ('blocks.11', 'attn', 'qkv', 'kernel')  | torch.Size([768, 2304])      | -0     | 0.015 |\n",
            "| ('blocks.11', 'mlp', 'fc1', 'bias')     | torch.Size([3072])           | -0.386 | 0.215 |\n",
            "| ('blocks.11', 'mlp', 'fc1', 'kernel')   | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.11', 'mlp', 'fc2', 'bias')     | torch.Size([768])            | -0.006 | 0.164 |\n",
            "| ('blocks.11', 'mlp', 'fc2', 'kernel')   | torch.Size([3072, 768])      | -0     | 0.014 |\n",
            "| ('blocks.11', 'norm1', 'bias')          | torch.Size([768])            |  0.024 | 0.202 |\n",
            "| ('blocks.11', 'norm1', 'scale')         | torch.Size([768])            |  2.028 | 0.159 |\n",
            "| ('blocks.11', 'norm2', 'bias')          | torch.Size([768])            |  0.044 | 0.523 |\n",
            "| ('blocks.11', 'norm2', 'scale')         | torch.Size([768])            |  1.79  | 0.137 |\n",
            "| ('blocks.2', 'attn', 'proj', 'bias')    | torch.Size([768])            | -0.006 | 0.091 |\n",
            "| ('blocks.2', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       | -0     | 0.012 |\n",
            "| ('blocks.2', 'attn', 'qkv', 'bias')     | torch.Size([2304])           | -0.005 | 0.458 |\n",
            "| ('blocks.2', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.017 |\n",
            "| ('blocks.2', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.331 | 0.162 |\n",
            "| ('blocks.2', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.2', 'mlp', 'fc2', 'bias')      | torch.Size([768])            | -0     | 0.071 |\n",
            "| ('blocks.2', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.011 |\n",
            "| ('blocks.2', 'norm1', 'bias')           | torch.Size([768])            | -0.005 | 0.194 |\n",
            "| ('blocks.2', 'norm1', 'scale')          | torch.Size([768])            |  0.922 | 0.189 |\n",
            "| ('blocks.2', 'norm2', 'bias')           | torch.Size([768])            |  0.008 | 0.146 |\n",
            "| ('blocks.2', 'norm2', 'scale')          | torch.Size([768])            |  1.21  | 0.157 |\n",
            "| ('blocks.3', 'attn', 'proj', 'bias')    | torch.Size([768])            | -0.001 | 0.06  |\n",
            "| ('blocks.3', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| ('blocks.3', 'attn', 'qkv', 'bias')     | torch.Size([2304])           | -0     | 0.405 |\n",
            "| ('blocks.3', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.017 |\n",
            "| ('blocks.3', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.309 | 0.178 |\n",
            "| ('blocks.3', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.3', 'mlp', 'fc2', 'bias')      | torch.Size([768])            |  0.001 | 0.055 |\n",
            "| ('blocks.3', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.01  |\n",
            "| ('blocks.3', 'norm1', 'bias')           | torch.Size([768])            | -0.009 | 0.192 |\n",
            "| ('blocks.3', 'norm1', 'scale')          | torch.Size([768])            |  1.186 | 0.132 |\n",
            "| ('blocks.3', 'norm2', 'bias')           | torch.Size([768])            |  0.01  | 0.145 |\n",
            "| ('blocks.3', 'norm2', 'scale')          | torch.Size([768])            |  1.333 | 0.134 |\n",
            "| ('blocks.4', 'attn', 'proj', 'bias')    | torch.Size([768])            | -0     | 0.057 |\n",
            "| ('blocks.4', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| ('blocks.4', 'attn', 'qkv', 'bias')     | torch.Size([2304])           | -0.008 | 0.322 |\n",
            "| ('blocks.4', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.4', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.316 | 0.177 |\n",
            "| ('blocks.4', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.4', 'mlp', 'fc2', 'bias')      | torch.Size([768])            |  0.001 | 0.057 |\n",
            "| ('blocks.4', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      | -0     | 0.011 |\n",
            "| ('blocks.4', 'norm1', 'bias')           | torch.Size([768])            |  0.002 | 0.255 |\n",
            "| ('blocks.4', 'norm1', 'scale')          | torch.Size([768])            |  1.29  | 0.122 |\n",
            "| ('blocks.4', 'norm2', 'bias')           | torch.Size([768])            | -0.008 | 0.212 |\n",
            "| ('blocks.4', 'norm2', 'scale')          | torch.Size([768])            |  1.444 | 0.105 |\n",
            "| ('blocks.5', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0     | 0.069 |\n",
            "| ('blocks.5', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| ('blocks.5', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.005 | 0.316 |\n",
            "| ('blocks.5', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.5', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.331 | 0.181 |\n",
            "| ('blocks.5', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      |  0     | 0.014 |\n",
            "| ('blocks.5', 'mlp', 'fc2', 'bias')      | torch.Size([768])            |  0     | 0.053 |\n",
            "| ('blocks.5', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      | -0     | 0.011 |\n",
            "| ('blocks.5', 'norm1', 'bias')           | torch.Size([768])            |  0.002 | 0.22  |\n",
            "| ('blocks.5', 'norm1', 'scale')          | torch.Size([768])            |  1.286 | 0.146 |\n",
            "| ('blocks.5', 'norm2', 'bias')           | torch.Size([768])            | -0.052 | 0.306 |\n",
            "| ('blocks.5', 'norm2', 'scale')          | torch.Size([768])            |  1.492 | 0.112 |\n",
            "| ('blocks.6', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0.001 | 0.085 |\n",
            "| ('blocks.6', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| ('blocks.6', 'attn', 'qkv', 'bias')     | torch.Size([2304])           | -0.006 | 0.303 |\n",
            "| ('blocks.6', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.6', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.359 | 0.192 |\n",
            "| ('blocks.6', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      |  0     | 0.015 |\n",
            "| ('blocks.6', 'mlp', 'fc2', 'bias')      | torch.Size([768])            | -0     | 0.062 |\n",
            "| ('blocks.6', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.012 |\n",
            "| ('blocks.6', 'norm1', 'bias')           | torch.Size([768])            |  0.006 | 0.21  |\n",
            "| ('blocks.6', 'norm1', 'scale')          | torch.Size([768])            |  1.369 | 0.117 |\n",
            "| ('blocks.6', 'norm2', 'bias')           | torch.Size([768])            | -0.051 | 0.462 |\n",
            "| ('blocks.6', 'norm2', 'scale')          | torch.Size([768])            |  1.649 | 0.141 |\n",
            "| ('blocks.7', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0     | 0.113 |\n",
            "| ('blocks.7', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.012 |\n",
            "| ('blocks.7', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.015 | 0.308 |\n",
            "| ('blocks.7', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.7', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.368 | 0.188 |\n",
            "| ('blocks.7', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      |  0     | 0.015 |\n",
            "| ('blocks.7', 'mlp', 'fc2', 'bias')      | torch.Size([768])            | -0     | 0.087 |\n",
            "| ('blocks.7', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.013 |\n",
            "| ('blocks.7', 'norm1', 'bias')           | torch.Size([768])            |  0.016 | 0.16  |\n",
            "| ('blocks.7', 'norm1', 'scale')          | torch.Size([768])            |  1.471 | 0.105 |\n",
            "| ('blocks.7', 'norm2', 'bias')           | torch.Size([768])            | -0.095 | 0.464 |\n",
            "| ('blocks.7', 'norm2', 'scale')          | torch.Size([768])            |  1.945 | 0.172 |\n",
            "| ('blocks.8', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0     | 0.046 |\n",
            "| ('blocks.8', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       |  0     | 0.013 |\n",
            "| ('blocks.8', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.009 | 0.334 |\n",
            "| ('blocks.8', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.8', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.379 | 0.216 |\n",
            "| ('blocks.8', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.8', 'mlp', 'fc2', 'bias')      | torch.Size([768])            |  0     | 0.125 |\n",
            "| ('blocks.8', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      |  0     | 0.014 |\n",
            "| ('blocks.8', 'norm1', 'bias')           | torch.Size([768])            |  0.021 | 0.163 |\n",
            "| ('blocks.8', 'norm1', 'scale')          | torch.Size([768])            |  1.598 | 0.115 |\n",
            "| ('blocks.8', 'norm2', 'bias')           | torch.Size([768])            | -0.038 | 0.509 |\n",
            "| ('blocks.8', 'norm2', 'scale')          | torch.Size([768])            |  2.693 | 0.259 |\n",
            "| ('blocks.9', 'attn', 'proj', 'bias')    | torch.Size([768])            |  0.002 | 0.106 |\n",
            "| ('blocks.9', 'attn', 'proj', 'kernel')  | torch.Size([768, 768])       | -0     | 0.014 |\n",
            "| ('blocks.9', 'attn', 'qkv', 'bias')     | torch.Size([2304])           |  0.011 | 0.331 |\n",
            "| ('blocks.9', 'attn', 'qkv', 'kernel')   | torch.Size([768, 2304])      |  0     | 0.016 |\n",
            "| ('blocks.9', 'mlp', 'fc1', 'bias')      | torch.Size([3072])           | -0.382 | 0.241 |\n",
            "| ('blocks.9', 'mlp', 'fc1', 'kernel')    | torch.Size([768, 3072])      | -0     | 0.014 |\n",
            "| ('blocks.9', 'mlp', 'fc2', 'bias')      | torch.Size([768])            |  0.001 | 0.113 |\n",
            "| ('blocks.9', 'mlp', 'fc2', 'kernel')    | torch.Size([3072, 768])      | -0     | 0.015 |\n",
            "| ('blocks.9', 'norm1', 'bias')           | torch.Size([768])            |  0.031 | 0.159 |\n",
            "| ('blocks.9', 'norm1', 'scale')          | torch.Size([768])            |  1.801 | 0.114 |\n",
            "| ('blocks.9', 'norm2', 'bias')           | torch.Size([768])            |  0.057 | 0.665 |\n",
            "| ('blocks.9', 'norm2', 'scale')          | torch.Size([768])            |  4.431 | 0.438 |\n",
            "| ('ln_pre', 'bias')                      | torch.Size([768])            | -0.001 | 0.073 |\n",
            "| ('ln_pre', 'scale')                     | torch.Size([768])            |  0.393 | 0.455 |\n",
            "| ('patch_embed.proj', 'bias')            | (768,)                       |  0     | 0     |\n",
            "| ('patch_embed.proj', 'kernel')          | torch.Size([16, 16, 3, 768]) |  0     | 0.019 |\n",
            "| ('pos_embed',)                          | torch.Size([1, 197, 768])    | -0.007 | 0.028 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import checkpoints\n",
        "out_path = 'clip_b_16'\n",
        "checkpoints.save_checkpoint(out_path, {'params': tree}, 0)\n",
        "# from google.colab import files\n",
        "# files.download(f'{out_path}/checkpoint_0')"
      ],
      "metadata": {
        "id": "LqSx_9EUBI2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56c85acc-436c-42bf-8320-b76cea8ed72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'clip_b_16/checkpoint_0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtZAVRIbNGHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

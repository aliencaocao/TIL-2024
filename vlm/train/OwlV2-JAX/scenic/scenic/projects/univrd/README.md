UniVRD: Unified Visual Relationship Detection with Vision and Language Models
==

UniVRD is a bottom-up visual relationship detector built upon pre-trained vision and language models. It merges visual relationships spanning different datasets during training and thus can make predictions over the union of their label spaces. UniVRD reaches state-of-the-art performance on both human-object interaction and visual relationship detection tasks, e.g., 38.07 mAP on HICO-DET with a ViT-L/14 backbone.

[[Paper]](https://arxiv.org/abs/2303.08998)

## Reference

If you use UniVRD, please cite the [paper](https://arxiv.org/abs/2303.08998):

```
@inproceedings{zhao2023unified,
  title = {Unified Visual Relationship Detection with Vision and Language Models},
  author = {Zhao, Long and Yuan, Liangzhe and Gong, Boqing and Cui, Yin and Schroff, Florian and Yang, Ming-Hsuan and Adam, Hartwig and Liu, Ting},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year = {2023},
}
```

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OwlV2 finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./owlv2-venv/lib/python3.10/site-packages (4.41.0)\n",
      "Requirement already satisfied: datasets in ./owlv2-venv/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: evaluate in ./owlv2-venv/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: accelerate in ./owlv2-venv/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: orjson in ./owlv2-venv/lib/python3.10/site-packages (3.10.3)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-1.4.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: filelock in ./owlv2-venv/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: xxhash in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pandas in ./owlv2-venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: psutil in ./owlv2-venv/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./owlv2-venv/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Collecting opencv-python-headless>=4.9.0\n",
      "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.10.0 in ./owlv2-venv/lib/python3.10/site-packages (from albumentations) (1.13.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in ./owlv2-venv/lib/python3.10/site-packages (from albumentations) (0.23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in ./owlv2-venv/lib/python3.10/site-packages (from albumentations) (4.11.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in ./owlv2-venv/lib/python3.10/site-packages (from albumentations) (1.4.2)\n",
      "Collecting pydantic>=2.7.0\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./owlv2-venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in ./owlv2-venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./owlv2-venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./owlv2-venv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./owlv2-venv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: networkx>=2.8 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.10)\n",
      "Requirement already satisfied: pillow>=9.1 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./owlv2-venv/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.3.0 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: jinja2 in ./owlv2-venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./owlv2-venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./owlv2-venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./owlv2-venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./owlv2-venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./owlv2-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./owlv2-venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./owlv2-venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: pydantic-core, opencv-python-headless, annotated-types, pydantic, albumentations\n",
      "Successfully installed albumentations-1.4.7 annotated-types-0.6.0 opencv-python-headless-4.9.0.80 pydantic-2.7.1 pydantic-core-2.18.2\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets evaluate accelerate orjson albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /host/train_old.zip data/train.zip\n",
    "! cp /host/val_old.zip data/val.zip\n",
    "! mkdir data/train data/val\n",
    "! 7z e data/train.zip -odata/train\n",
    "! 7z e data/val.zip -odata/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import orjson\n",
    "\n",
    "ann_dict = {}\n",
    "\n",
    "with open(\"data/vlm.jsonl\") as ann_file:\n",
    "  for line in ann_file:\n",
    "    # {\"image\": \"image_0.jpg\", \"annotations\": [{\"caption\": \"grey missile\", \"bbox\": [912, 164, 48, 152]}, {\"caption\": \"red, white, and blue light aircraft\", \"bbox\": [1032, 80, 24, 28]}, {\"caption\": \"green and black missile\", \"bbox\": [704, 508, 76, 64]}, {\"caption\": \"white and red helicopter\", \"bbox\": [524, 116, 112, 48]}]}\n",
    "    x = orjson.loads(line)\n",
    "    ann_dict[x[\"image\"]] = x\n",
    "\n",
    "with open(\"data/train.jsonl\", \"wb\") as train_ann_file:\n",
    "  for train_path in glob(\"data/train/*\"):\n",
    "    train_ann_file.write(orjson.dumps(ann_dict[train_path.split(\"/\")[-1]]) + b\"\\n\")\n",
    "\n",
    "with open(\"data/val.jsonl\", \"wb\") as val_ann_file:\n",
    "  for val_path in glob(\"data/val/*\"):\n",
    "    val_ann_file.write(orjson.dumps(ann_dict[val_path.split(\"/\")[-1]]) + b\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yield dataset in CPPE5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "4086it [00:00, 4869.16it/s]\n",
      "1021it [00:00, 6404.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import orjson\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_split(split):\n",
    "  ds_list = []\n",
    "  with open(f\"data/{split}.jsonl\") as ann_file:\n",
    "    for i, line in tqdm(enumerate(ann_file)):\n",
    "      # {\"image\": \"image_0.jpg\", \"annotations\": [{\"caption\": \"grey missile\", \"bbox\": [912, 164, 48, 152]}, {\"caption\": \"red, white, and blue light aircraft\", \"bbox\": [1032, 80, 24, 28]}, {\"caption\": \"green and black missile\", \"bbox\": [704, 508, 76, 64]}, {\"caption\": \"white and red helicopter\", \"bbox\": [524, 116, 112, 48]}]}\n",
    "      x = orjson.loads(line)\n",
    "      anns = x[\"annotations\"]\n",
    "      img = Image.open(f\"data/{split}/{x['image']}\")\n",
    "      # img.load() # bypass PIL lazy loading\n",
    "      ds_list.append({\n",
    "        \"image_id\": int(x[\"image\"][6:-4]),\n",
    "        # \"image_id\": i,\n",
    "        \"image\": img,\n",
    "        \"width\": img.width,\n",
    "        \"height\": img.height,\n",
    "        \"objects\": {\n",
    "          # \"id\" key not used\n",
    "          \"area\": [ann[\"bbox\"][2] * ann[\"bbox\"][3] for ann in anns],\n",
    "          \"bbox\": [ann[\"bbox\"] for ann in anns],\n",
    "          # TODO: categories aren't fixed. How to supply text?\n",
    "          \"caption\": [ann[\"caption\"] for ann in anns],\n",
    "        },\n",
    "      })\n",
    "\n",
    "  return ds_list\n",
    "\n",
    "train_ds = Dataset.from_list(get_split(\"train\"))\n",
    "val_ds = Dataset.from_list(get_split(\"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "o = [{'bbox': (764.0, 124.0, 60.0, 36.0), 'caption': 'black and white commercial aircraft'}, {'bbox': (888.0000000000001, 516.0, 47.999999999999886, 64.0), 'caption': 'green and white fighter plane'}, {'bbox': (804.0, 272.0, 48.000000000000114, 48.0), 'caption': 'white and blue helicopter'}, {'bbox': (712.0, 420.0, 60.0, 63.99999999999994), 'caption': 'grey and black fighter plane'}, {'bbox': (367.99999999999994, 228.00000000000003, 60.00000000000006, 35.99999999999997), 'caption': 'white and black helicopter'}]\n",
    "torch.tensor([[obj[\"bbox\"] for obj in o] for x in batch_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': [3778, 1752],\n",
       " 'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1520x870>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1520x870>],\n",
       " 'width': [1520, 1520],\n",
       " 'height': [870, 870],\n",
       " 'objects': [{'area': [3168, 2464, 1584],\n",
       "   'bbox': [[876, 140, 88, 36], [608, 456, 44, 56], [1236, 104, 44, 36]],\n",
       "   'caption': ['yellow, black, and red helicopter',\n",
       "    'green and white fighter plane',\n",
       "    'yellow fighter jet']},\n",
       "  {'area': [4576, 2816, 1792, 3584, 2688, 4800, 1008, 4560],\n",
       "   'bbox': [[1028, 300, 88, 52],\n",
       "    [1264, 64, 64, 44],\n",
       "    [604, 196, 56, 32],\n",
       "    [1208, 312, 128, 28],\n",
       "    [716, 392, 56, 48],\n",
       "    [1064, 48, 80, 60],\n",
       "    [848, 312, 36, 28],\n",
       "    [928, 444, 76, 60]],\n",
       "   'caption': ['white, blue, and red commercial aircraft',\n",
       "    'black and orange drone',\n",
       "    'white, black, and red drone',\n",
       "    'green and grey helicopter',\n",
       "    'black cargo aircraft',\n",
       "    'green missile',\n",
       "    'yellow fighter plane',\n",
       "    'white and red commercial aircraft']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code yields the dataset as a generator, which is slower for iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import orjson\n",
    "\n",
    "def get_ds_generator(split):\n",
    "  def ds_generator():\n",
    "    with open(f\"data/{split}.jsonl\") as ann_file:\n",
    "      for i, line in enumerate(ann_file):\n",
    "        # {\"image\": \"image_0.jpg\", \"annotations\": [{\"caption\": \"grey missile\", \"bbox\": [912, 164, 48, 152]}, {\"caption\": \"red, white, and blue light aircraft\", \"bbox\": [1032, 80, 24, 28]}, {\"caption\": \"green and black missile\", \"bbox\": [704, 508, 76, 64]}, {\"caption\": \"white and red helicopter\", \"bbox\": [524, 116, 112, 48]}]}\n",
    "        x = orjson.loads(line)\n",
    "        anns = x[\"annotations\"]\n",
    "        img = Image.open(f\"data/{split}/{x['image']}\")\n",
    "        yield {\n",
    "          \"image_id\": int(x[\"image\"][6:-4]),\n",
    "          # \"image_id\": i,\n",
    "          \"image\": img,\n",
    "          \"width\": img.width,\n",
    "          \"height\": img.height,\n",
    "          \"objects\": {\n",
    "            # \"id\" key not used\n",
    "            \"area\": [ann[\"bbox\"][2] * ann[\"bbox\"][3] for ann in anns],\n",
    "            \"bbox\": [ann[\"bbox\"] for ann in anns],\n",
    "            # TODO: categories aren't fixed. How to supply text?\n",
    "            \"caption\": [ann[\"caption\"] for ann in anns],\n",
    "          },\n",
    "        }\n",
    "  \n",
    "  return ds_generator\n",
    "\n",
    "train_ds = Dataset.from_generator(get_ds_generator(\"train\"))\n",
    "val_ds = Dataset.from_generator(get_ds_generator(\"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 3778,\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1520x870>,\n",
       " 'width': 1520,\n",
       " 'height': 870,\n",
       " 'objects': {'area': [3168, 2464, 1584],\n",
       "  'bbox': [[876, 140, 88, 36], [608, 456, 44, 56], [1236, 104, 44, 36]],\n",
       "  'caption': ['yellow, black, and red helicopter',\n",
       "   'green and white fighter plane',\n",
       "   'yellow fighter jet']}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "checkpoint = \"google/owlv2-large-patch14-ensemble\"\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "\n",
    "transforms = albumentations.Compose(\n",
    "  transforms=[\n",
    "    # How noisy will their test images be? var = 2500 is extreme!\n",
    "    albumentations.GaussNoise(var_limit=2500, p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.VerticalFlip(p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(\n",
    "      p=0.5,\n",
    "      brightness_limit=0.3,\n",
    "      contrast_limit=0.3,\n",
    "    ),\n",
    "    # MixUp / Mosaic?\n",
    "  ],\n",
    "  bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def transform_sample(batch):\n",
    "  # batch is a Dict[str, list], NOT a list!\n",
    "  # TODO: add augmentation\n",
    "\n",
    "  # batch_transformed = {\n",
    "  #   \"input_ids\": [],\n",
    "  #   \"attention_mask\": [],\n",
    "  #   \"pixel_values\": [],\n",
    "  #   \"labels\": [],\n",
    "  # }\n",
    "\n",
    "  # for obj, img in zip(batch[\"objects\"], batch[\"image\"]):\n",
    "  #   sample_transformed = processor(\n",
    "  #     text=obj[\"caption\"],\n",
    "  #     images=img,\n",
    "  #     return_tensors=\"pt\",\n",
    "  #   )\n",
    "  #   batch_transformed[\"input_ids\"].append(sample_transformed[\"input_ids\"])\n",
    "  #   batch_transformed[\"attention_mask\"].append(sample_transformed[\"attention_mask\"])\n",
    "  #   batch_transformed[\"pixel_values\"].append(sample_transformed[\"pixel_values\"])\n",
    "  #   batch_transformed[\"labels\"].append(torch.tensor(obj[\"bbox\"]))\n",
    "\n",
    "\n",
    "  batch_transformed = processor(\n",
    "    text=[obj[\"caption\"] for obj in batch[\"objects\"]],\n",
    "    images=batch[\"image\"],\n",
    "    return_tensors=\"pt\",\n",
    "  )\n",
    "  batch_transformed[\"labels\"] = [torch.tensor(obj[\"bbox\"]) for obj in batch[\"objects\"]]\n",
    "  \n",
    "  return batch_transformed\n",
    "\n",
    "def collate_fn(batch_list):\n",
    "  batch = {}\n",
    "\n",
    "  batch[\"input_ids\"] = torch.cat([x[\"input_ids\"] for x in batch_list])\n",
    "  batch[\"attention_mask\"] = torch.cat([x[\"attention_mask\"] for x in batch_list])\n",
    "  batch[\"pixel_values\"] = torch.cat([x[\"pixel_values\"] for x in batch_list])\n",
    "  # batch[\"labels\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "  #   sequences=[x[\"labels\"] for x in batch_list],\n",
    "  #   batch_first=True,\n",
    "  #   padding_value=-1.,\n",
    "  # )\n",
    "  batch[\"labels\"] = torch.cat([x[\"labels\"] for x in batch_list])\n",
    "  \n",
    "  # print(batch)\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.with_transform(transform_sample)\n",
    "val_ds = val_ds.with_transform(transform_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,  1579,  9397, 49407,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,   537,  4287,  1395,  7706, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  5046, 11956, 49407,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,   736,   537,  1579,  6438,  6565, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,   537,  1746,  6438,  5363, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  5046,   537,   736,  6287,  7706, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,  1395,  7706, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,   537,   736,  6287,  7706, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,   537,   736,  1395,  7706, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  4481,  6287,  7706, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1746,   537,  5046,  6438,  6565, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,   537,  1449,  1395,  7706, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406,  1579,  6438,  5363, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [49406, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'pixel_values': tensor([[[[-0.6974, -0.7083, -0.6974,  ...,  1.1316,  1.1237,  1.1237],\n",
       "          [-0.6865, -0.7059, -0.7166,  ...,  1.1017,  1.1044,  1.1128],\n",
       "          [-0.7011, -0.7231, -0.7412,  ...,  1.0836,  1.1017,  1.1128],\n",
       "          ...,\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690],\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690],\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690]],\n",
       "\n",
       "         [[-0.0712, -0.0824, -0.0712,  ...,  1.4939,  1.4858,  1.4858],\n",
       "          [-0.0486, -0.0685, -0.0827,  ...,  1.4631,  1.4659,  1.4746],\n",
       "          [-0.0600, -0.0827, -0.1042,  ...,  1.4446,  1.4631,  1.4746],\n",
       "          ...,\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614],\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614],\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614]],\n",
       "\n",
       "         [[ 0.7097,  0.6991,  0.7097,  ...,  1.8229,  1.8152,  1.8152],\n",
       "          [ 0.6986,  0.6798,  0.6751,  ...,  1.7938,  1.7964,  1.8046],\n",
       "          [ 0.6776,  0.6562,  0.6442,  ...,  1.7762,  1.7938,  1.8046],\n",
       "          ...,\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328],\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328],\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328]]],\n",
       "\n",
       "\n",
       "        [[[-0.0696, -0.0696, -0.0590,  ..., -0.2261, -0.2439, -0.2302],\n",
       "          [-0.0696, -0.0612, -0.0590,  ..., -0.2288, -0.2217, -0.2082],\n",
       "          [-0.0669, -0.0590, -0.0540,  ..., -0.2127, -0.1640, -0.1560],\n",
       "          ...,\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690],\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690],\n",
       "          [ 0.0690,  0.0690,  0.0690,  ...,  0.0690,  0.0690,  0.0690]],\n",
       "\n",
       "         [[ 0.6491,  0.6492,  0.6682,  ..., -0.2742, -0.2804, -0.2663],\n",
       "          [ 0.6491,  0.6579,  0.6682,  ..., -0.2770, -0.2522, -0.2208],\n",
       "          [ 0.6600,  0.6682,  0.6771,  ..., -0.2605, -0.1911, -0.1600],\n",
       "          ...,\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614],\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614],\n",
       "          [ 0.1614,  0.1614,  0.1614,  ...,  0.1614,  0.1614,  0.1614]],\n",
       "\n",
       "         [[ 1.4776,  1.4776,  1.4918,  ..., -0.3747, -0.3844, -0.3711],\n",
       "          [ 1.4776,  1.4858,  1.4918,  ..., -0.3774, -0.3602, -0.3388],\n",
       "          [ 1.4840,  1.4918,  1.4984,  ..., -0.3596, -0.3031, -0.2846],\n",
       "          ...,\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328],\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328],\n",
       "          [ 0.3328,  0.3328,  0.3328,  ...,  0.3328,  0.3328,  0.3328]]]]), 'labels': [tensor([[788, 500,  60,  60],\n",
       "        [508, 268, 132,  44],\n",
       "        [576, 376,  56,  20],\n",
       "        [512, 380,  24,  40],\n",
       "        [664, 376,  44,  28],\n",
       "        [348, 452,  60,  40],\n",
       "        [864, 176,  28,  32],\n",
       "        [800, 120,  76,  32]]), tensor([[908, 500,  52,  36],\n",
       "        [396,  88,  80,  32],\n",
       "        [548, 264,  56,  32],\n",
       "        [748, 264,  44,  28],\n",
       "        [916, 240,  28,  36]])]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2867\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2865\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[1;32m   2866\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m-> 2867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2867\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2865\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[1;32m   2866\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m-> 2867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2867\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2865\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[1;32m   2866\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m-> 2867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=2)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # Adjust as needed (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Using Detr-Loss calculation https://github.com/facebookresearch/detr/blob/main/models/matcher.py\n",
    "# https://www.kaggle.com/code/bibhasmondal96/detr-from-scratch\n",
    "class BoxUtils(object):\n",
    "    @staticmethod\n",
    "    def box_cxcywh_to_xyxy(x):\n",
    "        x_c, y_c, w, h = x.unbind(-1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def box_xyxy_to_cxcywh(x):\n",
    "        x0, y0, x1, y1 = x.unbind(-1)\n",
    "        b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "             (x1 - x0), (y1 - y0)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_bboxes(out_bbox, size):\n",
    "        img_h, img_w = size\n",
    "        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def box_area(boxes):\n",
    "        \"\"\"\n",
    "        Computes the area of a set of bounding boxes, which are specified by its\n",
    "        (x1, y1, x2, y2) coordinates.\n",
    "        Arguments:\n",
    "            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
    "                are expected to be in (x1, y1, x2, y2) format\n",
    "        Returns:\n",
    "            area (Tensor[N]): area for each box\n",
    "        \"\"\"\n",
    "        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    # modified from torchvision to also return the union\n",
    "    def box_iou(boxes1, boxes2):\n",
    "        area1 = BoxUtils.box_area(boxes1)\n",
    "        area2 = BoxUtils.box_area(boxes2)\n",
    "\n",
    "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "        union = area1[:, None] + area2 - inter\n",
    "\n",
    "        iou = inter / union\n",
    "        return iou, union\n",
    "\n",
    "    @staticmethod\n",
    "    def generalized_box_iou(boxes1, boxes2):\n",
    "        \"\"\"\n",
    "        Generalized IoU from https://giou.stanford.edu/\n",
    "        The boxes should be in [x0, y0, x1, y1] format\n",
    "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "        and M = len(boxes2)\n",
    "        \"\"\"\n",
    "        # degenerate boxes gives inf / nan results\n",
    "        # so do an early check\n",
    "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n",
    "\n",
    "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "        return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        print(\"HungarianMatcher.forward() called!\")\n",
    "        print(\"outputs=\", outputs)\n",
    "        print(\"targets=\", targets)\n",
    "\n",
    "        logging.info(f\"{outputs.keys()=}\")\n",
    "        bs, num_queries = outputs[\"logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
    "        logging.info(f\"forward - {tgt_ids}\")\n",
    "        tgt_ids = tgt_ids.int()\n",
    "        logging.info(f\"forward - {tgt_ids}\")\n",
    "\n",
    "\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "        # Compute the giou cost betwen boxes\n",
    "        cost_giou = -BoxUtils.generalized_box_iou(\n",
    "            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n",
    "            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n",
    "        )\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        logging.info(f\"loss_labels - {outputs.keys()}\")\n",
    "        assert 'logits' in outputs\n",
    "        src_logits = outputs['logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)]).to(torch.int64)\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device).to(torch.int64)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n",
    "            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n",
    "            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n",
    "        )\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        logging.info(f\"{type(outputs)=}\")\n",
    "        logging.info(f\"{type(targets)=}\")\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "# TODO: scale params based on batch size\n",
    "optimizer = torch.optim.AdamW(\n",
    "  params=model.parameters(),\n",
    "  lr=1e-5,\n",
    "  betas=(0.9, 0.999),\n",
    "  eps=1e-8,\n",
    "  weight_decay=1e-2,\n",
    ")\n",
    "\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=100,\n",
    "  num_training_steps=10000,\n",
    "  num_cycles=10,\n",
    "  last_epoch=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, scheduler = accelerator.prepare(\n",
    "  model, optimizer, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB. GPU \u0004 has a total capacity of 23.48 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 1.50 GiB is allocated by PyTorch, and 133.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m   output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlv2-large-patch14-ensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m   num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m   label_smoothing_factor\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnextafter(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     24\u001b[0m   model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m   args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m   data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m   labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:189\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:110\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    108\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    109\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m--> 110\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m    113\u001b[0m buffers_rg: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:83\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m \u001b[43mBroadcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tensor_copies[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     85\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))]\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/TIL-2024/vlm/train/OwlV2/owlv2-venv/lib/python3.10/site-packages/torch/nn/parallel/comm.py:57\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     56\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB. GPU \u0004 has a total capacity of 23.48 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 1.50 GiB is allocated by PyTorch, and 133.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"owlv2-large-patch14-ensemble\",\n",
    "  num_train_epochs=30,\n",
    "  learning_rate=5e-6,\n",
    "  # lr_scheduler_type=schedule,\n",
    "  eval_strategy=\"epoch\",\n",
    "  # auto_find_batch_size=True,\n",
    "  # TODO: adjust batch size\n",
    "  per_device_train_batch_size=1,\n",
    "  per_device_eval_batch_size=1,\n",
    "  save_strategy=\"epoch\",\n",
    "  bf16=True,\n",
    "  dataloader_num_workers=8,\n",
    "  remove_unused_columns=False,\n",
    "\n",
    "  # stupid workaround\n",
    "  label_smoothing_factor=np.nextafter(0, 1),\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=val_ds,\n",
    "  data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def custom_loss(logits, labels):\n",
    "  num_classes = 4 # what the fuck to do with this???\n",
    "  matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
    "  weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "  losses = ['labels', 'boxes', 'cardinality']\n",
    "  criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)\n",
    "  criterion.to(accelerator.device)\n",
    "  loss = criterion(logits, labels)\n",
    "  return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    loss = custom_loss(outputs, labels)\n",
    "    loss_ce = loss['loss_ce'].cpu().item()\n",
    "    loss_bbox = loss['loss_bbox'].cpu().item()\n",
    "    loss_giou = loss['loss_giou'].cpu().item()\n",
    "    cardinality_error = loss['cardinality_error'].cpu().item()\n",
    "    # print(\n",
    "    #     f\"loss_ce={loss_ce:.2f}\",\n",
    "    #     f\"loss_bbox={loss_bbox:.2f}\",\n",
    "    #     f\"loss_giou={loss_giou:.2f}\",\n",
    "    #     f\"cardinality_error={cardinality_error:.2f}\",\n",
    "    #     sep=\"\\t\")\n",
    "    loss = sum(loss.values())[0] #add\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"owlv2-large-patch14-ensemble\",\n",
    "  num_train_epochs=30,\n",
    "  learning_rate=5e-6,\n",
    "  # lr_scheduler_type=schedule,\n",
    "  eval_strategy=\"epoch\",\n",
    "  # auto_find_batch_size=True,\n",
    "  # TODO: adjust batch size\n",
    "  per_device_train_batch_size=1,\n",
    "  per_device_eval_batch_size=1,\n",
    "  save_strategy=\"epoch\",\n",
    "  bf16=True,\n",
    "  dataloader_num_workers=32,\n",
    "  remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  data_collator=collate_fn,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=train_ds,\n",
    "  tokenizer=processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlv2-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

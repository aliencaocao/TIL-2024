{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# General information\n",
    "\n",
    "Example colab for SigLIP models described in [the SigLIP paper](https://arxiv.org/abs/2303.15343).\n",
    "\n",
    "**These models are not official Google products and were trained and released for research purposes.**\n",
    "\n",
    "If you find our model(s) useful for your research, consider citing\n",
    "\n",
    "```\n",
    "@article{zhai2023sigmoid,\n",
    "  title={Sigmoid loss for language image pre-training},\n",
    "  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},\n",
    "  journal={International Conference on Computer Vision ({ICCV})},\n",
    "  year={2023}\n",
    "}\n",
    "```\n",
    "\n",
    "If you use our released models in your products, we will appreciate any direct feedback. We are reachable by xzhai@google.com, basilm@google.com, akolesnikov@google.com and lbeyer@google.com.\n",
    "\n",
    "\n",
    "Only the models explicitly marked with `i18n` in the name are expected to perform reasonably well on non-english data."
   ],
   "metadata": {
    "id": "wR53lePHuiP-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown # Environment setup\n",
    "#@markdown **IMPORTANT NOTE**: Modern jax (>0.4) does not support the Colab TPU\n",
    "#@markdown anymore, so don't select TPU runtime here. CPU and GPU work and are both fast enough.\n",
    "\n",
    "# Install the right jax version for TPU/GPU/CPU\n",
    "import os\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  raise \"TPU colab not supported.\"\n",
    "elif 'NVIDIA_PRODUCT_NAME' in os.environ:\n",
    "  !nvidia-smi\n",
    "import jax\n",
    "jax.devices()\n",
    "\n",
    "\n",
    "# Get latest version of big_vision codebase.\n",
    "!git clone --quiet --branch=main --depth=1 https://github.com/google-research/big_vision\n",
    "!cd big_vision && git pull --rebase --quiet\n",
    "!pip -q install -r big_vision/big_vision/requirements.txt\n",
    "# Gives us ~2x faster gsutil cp to get the model checkpoints.\n",
    "!pip3 -q install --no-cache-dir -U crcmod\n",
    "\n",
    "%cd big_vision\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "\n",
    "from google.colab.output import _publish as publish"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXSdSXVg2PAI",
    "outputId": "ba908946-0cd3-4468-9034-cd108529986f",
    "cellView": "form"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choose and load model, perform inference"
   ],
   "metadata": {
    "id": "byHpmgAO6inM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pick your hero: (WHEN CHANGING THIS, RERUN IMAGE/TEXT EMBEDDING CELLS)\n",
    "# Give this cell 1-3mins.\n",
    "\n",
    "# VARIANT, RES = 'B/16', 224\n",
    "# VARIANT, RES = 'B/16', 256\n",
    "# VARIANT, RES = 'B/16', 384\n",
    "# VARIANT, RES = 'B/16', 512\n",
    "# VARIANT, RES = 'L/16', 256\n",
    "VARIANT, RES = 'L/16', 384\n",
    "# VARIANT, RES = 'So400m/14', 224\n",
    "# VARIANT, RES = 'So400m/14', 384\n",
    "# VARIANT, RES = 'B/16-i18n', 256\n",
    "\n",
    "CKPT, TXTVARIANT, EMBDIM, SEQLEN, VOCAB = {\n",
    "    ('B/16', 224): ('webli_en_b16_224_63724782.npz', 'B', 768, 64, 32_000),\n",
    "    ('B/16', 256): ('webli_en_b16_256_60500360.npz', 'B', 768, 64, 32_000),\n",
    "    ('B/16', 384): ('webli_en_b16_384_68578854.npz', 'B', 768, 64, 32_000),\n",
    "    ('B/16', 512): ('webli_en_b16_512_68580893.npz', 'B', 768, 64, 32_000),\n",
    "    ('L/16', 256): ('webli_en_l16_256_60552751.npz', 'L', 1024, 64, 32_000),\n",
    "    ('L/16', 384): ('webli_en_l16_384_63634585.npz', 'L', 1024, 64, 32_000),\n",
    "    ('So400m/14', 224): ('webli_en_so400m_224_57633886.npz', 'So400m', 1152, 16, 32_000),\n",
    "    ('So400m/14', 384): ('webli_en_so400m_384_58765454.npz', 'So400m', 1152, 64, 32_000),\n",
    "    ('B/16-i18n', 256): ('webli_i18n_b16_256_66117334.npz', 'B', 768, 64, 250_000),\n",
    "}[VARIANT, RES]\n",
    "\n",
    "# It is significantly faster to first copy the checkpoint (30s vs 8m30 for B and 1m vs ??? for L)\n",
    "!test -f /tmp/{CKPT} || gsutil cp gs://big_vision/siglip/{CKPT} /tmp/\n",
    "\n",
    "if VARIANT.endswith('-i18n'):\n",
    "  VARIANT = VARIANT[:-len('-i18n')]\n",
    "\n",
    "import big_vision.models.proj.image_text.two_towers as model_mod\n",
    "\n",
    "model_cfg = ml_collections.ConfigDict()\n",
    "model_cfg.image_model = 'vit'  # TODO(lbeyer): remove later, default\n",
    "model_cfg.text_model = 'proj.image_text.text_transformer'  # TODO(lbeyer): remove later, default\n",
    "model_cfg.image = dict(variant=VARIANT, pool_type='map')\n",
    "model_cfg.text = dict(variant=TXTVARIANT, vocab_size=VOCAB)\n",
    "model_cfg.out_dim = (None, EMBDIM)  # (image_out_dim, text_out_dim)\n",
    "model_cfg.bias_init = -10.0\n",
    "model_cfg.temperature_init = 10.0\n",
    "\n",
    "model = model_mod.Model(**model_cfg)\n",
    "\n",
    "# Using `init_params` is slower but will lead to `load` below performing sanity-checks.\n",
    "# init_params = jax.jit(model.init, backend=\"cpu\")(jax.random.PRNGKey(42), jnp.zeros([1, RES, RES, 3], jnp.float32), jnp.zeros([1, SEQLEN], jnp.int32))['params']\n",
    "init_params = None  # Faster but bypasses loading sanity-checks.\n",
    "\n",
    "params = model_mod.load(init_params, f'/tmp/{CKPT}', model_cfg)"
   ],
   "metadata": {
    "id": "0DsOabGD7MRG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5afc9f52-7eb4-4a0d-b681-3ab5945ce9b4"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load and embed images\n",
    "\n",
    "import big_vision.pp.builder as pp_builder\n",
    "import big_vision.pp.ops_general\n",
    "import big_vision.pp.ops_image\n",
    "import big_vision.pp.ops_text\n",
    "import PIL\n",
    "\n",
    "!wget -q https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg\n",
    "!wget -q https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg\n",
    "!wget -q 'https://images.unsplash.com/photo-1566467021888-b03548769dd1?ixlib=rb-4.0.3&q=85&fm=jpg&crop=entropy&cs=srgb&dl=svetlana-gumerova-hQHm2D1fH70-unsplash.jpg&w=640' -O cold_drink.jpg\n",
    "!wget -q 'https://images.rawpixel.com/image_1300/czNmcy1wcml2YXRlL3Jhd3BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvbHIvdXB3azU4ODU5NzY1LXdpa2ltZWRpYS1pbWFnZS1rb3diMmhkeC5qcGc.jpg' -O hot_drink.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/authors.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/siglip.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/caffeine.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/robosign.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/fried_fish.jpeg\n",
    "!wget -q 'https://pbs.twimg.com/media/FTyEyxyXsAAyKPc?format=jpg&name=small' -O cow_beach.jpg\n",
    "!wget -q 'https://storage.googleapis.com/big_vision/siglip/cow_beach2.jpg' -O cow_beach2.jpg\n",
    "!wget -q 'https://pbs.twimg.com/media/Frb6NIEXwAA8-fI?format=jpg&name=medium' -O mountain_view.jpg\n",
    "\n",
    "\n",
    "images = [PIL.Image.open(fname) for fname in [\n",
    "    'apple-ipod.jpg',\n",
    "    'apple-blank.jpg',\n",
    "    'cold_drink.jpg',\n",
    "    'hot_drink.jpg',\n",
    "    'caffeine.jpg',\n",
    "    'siglip.jpg',\n",
    "    'authors.jpg',\n",
    "    'robosign.jpg',\n",
    "    'cow_beach.jpg',\n",
    "    'cow_beach2.jpg',\n",
    "    'mountain_view.jpg',\n",
    "]]\n",
    "\n",
    "pp_img = pp_builder.get_preprocess_fn(f'resize({RES})|value_range(-1, 1)')\n",
    "imgs = np.array([pp_img({'image': np.array(image)})['image'] for image in images])\n",
    "zimg, _, out = model.apply({'params': params}, imgs, None)\n",
    "\n",
    "print(imgs.shape, zimg.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmuXfCfBjgeF",
    "outputId": "3627819b-007e-4107-e1f4-06b7ad3ac03a"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Tokenize and embed texts\n",
    "\n",
    "texts = [\n",
    "    'an apple',\n",
    "    'a picture of an apple',\n",
    "    'an ipod',\n",
    "    'granny smith',\n",
    "    'an apple with a note saying \"ipod\"',\n",
    "    'a cold drink on a hot day',\n",
    "    'a hot drink on a cold day',\n",
    "    'a photo of a cold drink on a hot day',\n",
    "    'a photo of a hot drink on a cold day',\n",
    "    #\n",
    "    'a photo of two guys in need of caffeine',\n",
    "    'a photo of two guys in need of water',\n",
    "    'a photo of the SigLIP authors',\n",
    "    'a photo of a rock band',\n",
    "    'a photo of researchers at Google Brain',\n",
    "    'a photo of researchers at OpenAI',\n",
    "    #\n",
    "    'a robot on a sign',\n",
    "    'a photo of a robot on a sign',\n",
    "    'an empty street',\n",
    "    'autumn in Toronto',\n",
    "    'a photo of autumn in Toronto',\n",
    "    'a photo of Toronto in autumn',\n",
    "    'a photo of Toronto in summer',\n",
    "    'autumn in Singapore',\n",
    "    #\n",
    "    'cow',\n",
    "    'a cow in a tuxedo',\n",
    "    'a cow on the beach',\n",
    "    'a cow in the prairie',\n",
    "    #\n",
    "    'the real mountain view',\n",
    "    'Zürich',\n",
    "    'San Francisco',\n",
    "    'a picture of a laptop with the lockscreen on, a cup of cappucino, salt and pepper grinders. The view through the window reveals lake Zürich and the Alps in the background of the city.',\n",
    "]\n",
    "\n",
    "TOKENIZERS = {\n",
    "    32_000: 'c4_en',\n",
    "    250_000: 'mc4',\n",
    "}\n",
    "pp_txt = pp_builder.get_preprocess_fn(f'tokenize(max_len={SEQLEN}, model=\"{TOKENIZERS[VOCAB]}\", eos=\"sticky\", pad_value=1, inkey=\"text\")')\n",
    "txts = np.array([pp_txt({'text': text})['labels'] for text in texts])\n",
    "_, ztxt, out = model.apply({'params': params}, None, txts)\n",
    "\n",
    "print(txts.shape, ztxt.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGrpkRTtjU-L",
    "outputId": "7c43b56e-cd53-4801-b1e3-66774368a1d2"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# This is how to get all probabilities:\n",
    "print(f\"Learned temperature {out['t'].item():.1f}, learned bias: {out['b'].item():.1f}\")\n",
    "probs = jax.nn.sigmoid(zimg @ ztxt.T * out['t'] + out['b'])\n",
    "print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n",
    "print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")"
   ],
   "metadata": {
    "id": "TIdAVw9VGEAw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22fc0d9a-8986-4679-ca89-6e4330a55c6e"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Pretty demo (code)\n",
    "from IPython.display import Javascript\n",
    "\n",
    "DEMO_IMG_SIZE = 96\n",
    "\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def bv2rgb(bv_img):\n",
    "  return (bv_img * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "def html_img(*, enc_img=None, pixels=None, id=None, size=100, max_size=None, max_height=None, style=\"\"):\n",
    "  if enc_img is None and pixels is not None:\n",
    "    with io.BytesIO() as buf:\n",
    "      PIL.Image.fromarray(np.asarray(pixels)).save(buf, format=\"JPEG\")\n",
    "      enc_img = buf.getvalue()\n",
    "\n",
    "  img_data = base64.b64encode(np.ascontiguousarray(enc_img)).decode('ascii')\n",
    "\n",
    "  id_spec = f'id={id}' if id else ''\n",
    "  if size is not None:\n",
    "    style_spec = f'style=\"{style}; width: {size}px; height: {size}px\"'\n",
    "  elif max_size is not None:\n",
    "    style_spec = f'style=\"{style}; width: auto; height: auto; max-width: {max_size}px; max-height: {max_size}px;\"'\n",
    "  elif max_height is not None:\n",
    "    style_spec = f'style=\"{style}; object-fit: cover; width: auto; height: {max_height}px;\"'\n",
    "  else: style_spec = ''\n",
    "\n",
    "  return f'<img {id_spec} {style_spec} src=\"data:image/png;base64,{img_data}\"/>'\n",
    "\n",
    "\n",
    "def make_table(zimg, ztxt, out):\n",
    "  # The default learnable bias is a little conservative. Play around with it!\n",
    "  t, b = out['t'].item(), out['b'].item()\n",
    "  tempered_logits = zimg @ ztxt.T * t\n",
    "  probs = 1 / (1 + np.exp(-tempered_logits - b))\n",
    "  publish.javascript(f\"var logits = {tempered_logits.tolist()};\")\n",
    "\n",
    "  def color(p):\n",
    "    return mpl.colors.rgb2hex(mpl.cm.Greens(p / 2)) if p >= 0.01 else \"transparent\"\n",
    "\n",
    "  publish.javascript(f\"var cmap = {[color(x) for x in np.linspace(0, 1, 50)]};\")\n",
    "  def cell(x, iimg, itxt):\n",
    "    return f\"<td id=td_{iimg}_{itxt} style=background-color:{color(x)} class=pct><pre id=p_{iimg}_{itxt}>{x * 100:>4.0f}%</pre>\"\n",
    "\n",
    "  html = f'''\n",
    "  <p>\n",
    "  <label for=b>Bias value:</label>\n",
    "  <input id=b type=range min=-15 max=0 step=0.1 name=b value={b} style=vertical-align:middle>\n",
    "  <output id=value></output>\n",
    "  </p>\n",
    "  '''\n",
    "\n",
    "  html += \"<table>\\n\"\n",
    "  html += \"<tr>\"\n",
    "  html += \"\".join([f\"<td style='width:{DEMO_IMG_SIZE}px;line-height:0'>\" + html_img(pixels=bv2rgb(img), size=DEMO_IMG_SIZE) for img in imgs])\n",
    "  html += \"<td>\"\n",
    "  for itxt, txt in enumerate(texts):\n",
    "    html += f\"<tr>\" + \"\".join([cell(probs[iimg, itxt], iimg, itxt) for iimg in range(len(imgs))]) + f\"<td class=txt>{txt}\"\n",
    "\n",
    "  publish.css(r\"\"\"\n",
    "  table {\n",
    "    border-collapse: collapse;\n",
    "  }\n",
    "\n",
    "  tr {\n",
    "    border: 1px transparent;\n",
    "  }\n",
    "\n",
    "  tr:nth-child(odd) {\n",
    "    background-color: #F5F5F5;\n",
    "  }\n",
    "\n",
    "  tr:hover {\n",
    "    background-color: lightyellow;\n",
    "    border: 1px solid black;\n",
    "  }\n",
    "\n",
    "  td.pct {\n",
    "    text-align: center;\n",
    "  }\n",
    "  \"\"\")\n",
    "  publish.html(html)\n",
    "\n",
    "  # JS code to compute and write all probs from the logits.\n",
    "  display(Javascript('''\n",
    "  function update(b) {\n",
    "    for(var iimg = 0; iimg < logits.length; iimg++) {\n",
    "      for(var itxt = 0; itxt < logits[iimg].length; itxt++) {\n",
    "        const el = document.getElementById(`p_${iimg}_${itxt}`);\n",
    "        const p = Math.round(100 / (1 + Math.exp(-logits[iimg][itxt] - b)));\n",
    "        const pad = p < 10.0 ? '  ' : p < 100.0 ? ' ' : ''\n",
    "        el.innerHTML = pad + (p).toFixed(0) + '%';\n",
    "\n",
    "        const td = document.getElementById(`td_${iimg}_${itxt}`);\n",
    "        const c = cmap[Math.round(p / 100 * (cmap.length - 1))];\n",
    "        td.style.backgroundColor = c;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  '''))\n",
    "\n",
    "  # JS code to connect the bias value slider\n",
    "  display(Javascript('''\n",
    "  const value = document.querySelector(\"#value\");\n",
    "  const input = document.querySelector(\"#b\");\n",
    "  value.textContent = input.value;\n",
    "  input.addEventListener(\"input\", (event) => {\n",
    "    value.textContent = event.target.value;\n",
    "    update(event.target.value);\n",
    "  });\n",
    "  '''))\n",
    "\n",
    "  # Make the cell output as large as the table to avoid annoying scrollbars.\n",
    "  display(Javascript(f'update({b})'))\n",
    "  display(Javascript('google.colab.output.resizeIframeToContent()'))"
   ],
   "metadata": {
    "cellView": "form",
    "id": "eolOc7vd_ZSj"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "make_table(zimg, ztxt, out)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "mt5BIywzzA6c",
    "outputId": "3b06cfb9-a3da-42d7-8caf-d5366d058f8b"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More international examples (choose i18n model for this)"
   ],
   "metadata": {
    "id": "f5lIiaD700UK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load and embed images\n",
    "\n",
    "import big_vision.pp.builder as pp_builder\n",
    "import big_vision.pp.ops_general\n",
    "import big_vision.pp.ops_image\n",
    "import big_vision.pp.ops_text\n",
    "import PIL\n",
    "\n",
    "!wget -q 'https://live.staticflickr.com/4152/5189547658_3b2a7126cb_b.jpg' -O ants_climbing_a_tree_food.jpg\n",
    "!wget -q 'https://storage.googleapis.com/big_vision/siglip/pexels-poranimm-athithawatthee-842401.jpg' -O ants_climbing_tree.jpg\n",
    "!wget -q 'https://images.rawpixel.com/image_1300/cHJpdmF0ZS9zdGF0aWMvaW1hZ2Uvd2Vic2l0ZS8yMDIyLTA0L2xyL3B4OTE3NDYyLWltYWdlLWt3eW8ydmxrLmpwZw.jpg' -O lion_head.jpg\n",
    "!wget -q 'https://images.rawpixel.com/image_1300/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIzLTA5L3Jhd3BpeGVsX29mZmljZV8yN19taW5pbWFsX3NpbXBsZV9fbGlvbl9fcGFwZXJfY29sbGFnZV9taW5pbWFsX183OGRlOGU3OS02ZTE3LTQ2YzAtYTUyOS02ZDAxM2YzNDg0OWVfMi5qcGc.jpg' -O lion_head_red.jpg\n",
    "!wget -q https://live.staticflickr.com/232/551040940_87299a85ec_h.jpg -O meat_ball.jpg\n",
    "!wget -q https://storage.googleapis.com/big_vision/siglip/squirrel_fish.jpg -O squirrel_fish.jpg\n",
    "!wget -q 'https://ideogram.ai/api/images/direct/F3lMxBprSk6ligq5Vy3XSw' -O squirrel_fish2.jpg\n",
    "!wget -q 'https://pbs.twimg.com/media/FTyEyxyXsAAyKPc?format=jpg&name=small' -O cow_beach.jpg\n",
    "!wget -q 'https://storage.googleapis.com/big_vision/siglip/cow_beach2.jpg' -O cow_beach2.jpg\n",
    "\n",
    "\n",
    "images = [PIL.Image.open(fname) for fname in [\n",
    "    'ants_climbing_a_tree_food.jpg',\n",
    "    'ants_climbing_tree.jpg',\n",
    "    'meat_ball.jpg',\n",
    "    'lion_head.jpg',\n",
    "    'lion_head_red.jpg',\n",
    "    'fried_fish.jpeg',\n",
    "    'squirrel_fish.jpg',\n",
    "    'squirrel_fish2.jpg',\n",
    "    'cow_beach.jpg',\n",
    "    'cow_beach2.jpg',\n",
    "]]\n",
    "\n",
    "pp_img = pp_builder.get_preprocess_fn(f'resize({RES})|value_range(-1, 1)')\n",
    "imgs = np.array([pp_img({'image': np.array(image)})['image'] for image in images])\n",
    "zimg, _, out = model.apply({'params': params}, imgs, None)\n",
    "\n",
    "print(imgs.shape, zimg.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsK74v2J04Xp",
    "outputId": "63f024ad-205c-4dd3-a5af-4dfd5ff198ca"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Tokenize and embed texts\n",
    "\n",
    "texts = [\n",
    "    '蚂蚁上树',\n",
    "    '肉末粉丝',\n",
    "    'ants climbing a tree',\n",
    "    'minced pork rice noodle',\n",
    "    #\n",
    "    '红烧狮子头',\n",
    "    'red burned lion head',\n",
    "    'lion head',\n",
    "    'meat ball with soy sauce',\n",
    "    #\n",
    "    '松鼠鳜鱼',\n",
    "    'squirrel',\n",
    "    'squirrel and fish',\n",
    "    'squirrel mandarinfish',\n",
    "    'squirrel mandarin fish',\n",
    "    'sweet and sour mandarin fish',\n",
    "    #\n",
    "    'cow',\n",
    "    'a cow in a tuxedo',\n",
    "    'a cow on the beach',\n",
    "    'a cow in the prairie',\n",
    "    'une vache sur la plage',\n",
    "    'eine Kuh am Strand',\n",
    "    'วัวอยู่ที่ชายหาด',\n",
    "    '一只躺在沙滩上的牛',\n",
    "    '一只沙滩上的牛',\n",
    "    'корова на пляже',\n",
    "    'بقرة على الشاطئ',\n",
    "]\n",
    "\n",
    "TOKENIZERS = {\n",
    "    32_000: 'c4_en',\n",
    "    250_000: 'mc4',\n",
    "}\n",
    "pp_txt = pp_builder.get_preprocess_fn(f'tokenize(max_len={SEQLEN}, model=\"{TOKENIZERS[VOCAB]}\", eos=\"sticky\", pad_value=1, inkey=\"text\")')\n",
    "txts = np.array([pp_txt({'text': text})['labels'] for text in texts])\n",
    "_, ztxt, out = model.apply({'params': params}, None, txts)\n",
    "\n",
    "print(txts.shape, ztxt.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAzAuYJh1eQ3",
    "outputId": "6c07c1a2-c236-4b68-b7e3-f92dcc070fcc"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "make_table(zimg, ztxt, out)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "JlMwn6K1-62i",
    "outputId": "6b8fa113-06f3-492c-ffa7-942d4799cae3"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation for non-Chinese speakers:\n",
    "\n",
    "- The first dish is literally called \"ants climbing a tree\" in Chinese.\n",
    "- The second dish is literally called \"red burned lion head\" in Chinese.\n",
    "- The third dish is literally called \"squirrel mandarinfish\" in Chinese.\n",
    "\n",
    "We are looking for more interesting examples that highlight culture-language aspects and where a non-EN model should \"get it\" while an EN-only does not."
   ],
   "metadata": {
    "id": "bNGoftU3y4UQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example image credits\n",
    "\n",
    "- The apple and apple + iPod images are from OpenAI.\n",
    "- [Cold drink on hot day](https://unsplash.com/fr/photos/hQHm2D1fH70).\n",
    "- [Hot drink on cold day](https://www.rawpixel.com/image/3282934).\n",
    "- Cows on beach were created by Chitwan Saharia using the Imagen model and shared with permission.\n",
    "- [\"ant climbing tree\" noodles](https://www.flickr.com/photos/avlxyz/5189547658)\n",
    "- [actual ants climbing on a tree](https://www.pexels.com/photo/macro-photo-of-five-orange-ants-842401/)\n",
    "- [real lion head](https://www.rawpixel.com/image/5941715/free-public-domain-cc0-photo)\n",
    "- [cartoon red lion head](https://www.rawpixel.com/image/12447997/image-texture-paper-png)\n",
    "- Collaged [squirrel](https://www.pexels.com/photo/brown-squirrel-47547/) and [fish](https://zh.wikipedia.org/zh-hans/%E9%B3%9C%E9%B1%BC) images.\n",
    "- cartoon [squirrel and fish](https://ideogram.ai/g/zgoma01ASS21U1YwIC7MrA/2) generated by [ideogram.ai](http://ideogram.ai) [with permission](https://x.com/ideogram_ai/status/1697428471184515316?s=20).\n",
    "- The remaining pictures are personal photos taken by the authors, long after the models were trained."
   ],
   "metadata": {
    "id": "etDZ3sl4kZ_q"
   }
  }
 ]
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T14:51:16.809653Z",
     "start_time": "2024-05-29T14:51:16.797653Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sys\n",
    "import cv2\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    SiglipModel,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    AutoProcessor,\n",
    "    EvalPrediction\n",
    ")\n",
    "from modeling_siglip import SiglipModel\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:44:34.785931Z",
     "start_time": "2024-05-29T14:44:31.130758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrained_model_path = \"siglip-finetune-3090-ep10-0.874\"\n",
    "\n",
    "model = SiglipModel.from_pretrained(pretrained_model_path).to('cuda')\n",
    "processor = AutoProcessor.from_pretrained(pretrained_model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "# image_processor = AutoImageProcessor.from_pretrained(pretrained_model_path)\n",
    "config = model.config"
   ],
   "id": "8063ec144fe688b5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:44:37.167444Z",
     "start_time": "2024-05-29T14:44:35.403850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the ds into train and val. the image column is in format \"image_1234_1.jpg\". Take 1234 as the image id. Check if the id is smaller than 4086. If yes, put this in train, else in val.\n",
    "def get_ds():\n",
    "    # If error about missing name, use datasets==2.15.0\n",
    "    dataset = load_dataset(path='.', data_files=\"til_siglip_ds.json\")\n",
    "    # train_dataset = dataset.filter(lambda example: int(example[\"image\"].split(\"_\")[1]) < 4086)\n",
    "    # val_dataset = dataset.filter(lambda example: int(example[\"image\"].split(\"_\")[1]) >= 4086)\n",
    "    # return train_dataset, val_dataset\n",
    "    return dataset\n",
    "\n",
    "# train_dataset, val_dataset = get_ds()\n",
    "train_dataset = get_ds()"
   ],
   "id": "163a43f250fc836a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42091d7aa0784c6a9ad2f446aa958e75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1bb39487a0b469da72b46afa132c9b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfd82a407fb644769d206d0e6aec451b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:06:05.692610Z",
     "start_time": "2024-05-29T15:06:05.673499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.albu_transforms = A.Compose([\n",
    "            A.Resize(image_size, image_size, interpolation=cv2.INTER_LANCZOS4),\n",
    "            A.GaussNoise(var_limit=500, p=0.5),\n",
    "            A.ISONoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "            A.Flip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.CLAHE(p=0.1),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.5, p=0.5),\n",
    "            A.RandomGamma(p=0.2),\n",
    "            A.Perspective(p=0.5),\n",
    "            A.ImageCompression(quality_lower=75, p=0.5),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2()  # CHW\n",
    "        ])\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = np.asarray(x.permute(1, 2, 0), dtype=np.uint8)  # torch CHW to HWC as required by albumentations\n",
    "            x = self.albu_transforms(image=x)['image']\n",
    "            x = x.float()\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, processor.image_processor.image_mean, processor.image_processor.image_std\n",
    ")\n",
    "# image_transformations = torch.jit.script(image_transformations)"
   ],
   "id": "571b1395a15b4c7e",
   "outputs": [],
   "execution_count": 279
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:32.397152Z",
     "start_time": "2024-05-29T14:49:32.379152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset['train']\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image\"\n",
    "    caption_column = \"label\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        text_inputs = processor.tokenizer(captions, padding=\"max_length\")\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        return examples\n",
    "    \n",
    "    data = data.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "    )\n",
    "    \n",
    "    def transform_images(examples):\n",
    "        image_dir = 'til_siglip_ds/'\n",
    "        images = [read_image(image_dir + image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_images)\n",
    "    return data"
   ],
   "id": "4e0c213ca65d6e1e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:36.411370Z",
     "start_time": "2024-05-29T14:49:34.326847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "# val_dataset = preprocess_dataset(val_dataset)"
   ],
   "id": "c279944e2c697544",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/27913 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "852a93c565be442882f7397ab3f05d17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:38.823836Z",
     "start_time": "2024-05-29T14:49:38.820845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ],
   "id": "9693726e20379541",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:40.756496Z",
     "start_time": "2024-05-29T14:49:40.743487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    return {\"accuracy\": (predictions == labels).mean().item(), 'F1': f1_score(labels, predictions, average='macro')}"
   ],
   "id": "ddf8f55e81aac8a3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SigLIP loss from https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/trainers/proj/image_text/siglip.py#L287\n",
    "# To add into https://github.com/huggingface/transformers/blob/bdb9106f247fca48a71eb384be25dbbd29b065a8/src/transformers/models/siglip/modeling_siglip.py#L1230\n",
    "if return_loss:\n",
    "    eye = torch.eye(logits_per_text.size(0), device=logits_per_text.device)\n",
    "    m1_diag1 = -torch.ones_like(logits_per_text) + 2 * eye\n",
    "    loglik = torch.nn.functional.logsigmoid(m1_diag1 * logits_per_text)\n",
    "    nll = -torch.sum(loglik, dim=-1)\n",
    "    loss = nll.mean()"
   ],
   "id": "5c27b19d51d41776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:43.387773Z",
     "start_time": "2024-05-29T14:49:43.383772Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "4b855c1fafd7e9ae",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:49:59.842661Z",
     "start_time": "2024-05-29T14:49:57.512026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"siglip-finetune\",\n",
    "    gradient_accumulation_steps=16,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,  # decrease from 0.999\n",
    "    num_train_epochs=20,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # epoch have bug\n",
    "    # eval_strategy=\"epoch\",  # eval bugged causing oom\n",
    "    save_total_limit=5,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
    "    tf32=True,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model='F1',\n",
    "    # greater_is_better=True,\n",
    "    optim='adamw_torch_fused',\n",
    "    # optim='adafactor',\n",
    "    resume_from_checkpoint=pretrained_model_path,\n",
    "    report_to='none',\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile = sys.platform == 'linux',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "train_result = trainer.train()"
   ],
   "id": "3af0728682135276",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 41\u001B[0m\n\u001B[0;32m      1\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      2\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m      3\u001B[0m     warmup_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     torch_compile \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinux\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     33\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     34\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     35\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics\n\u001B[0;32m     40\u001B[0m )\n\u001B[1;32m---> 41\u001B[0m train_result \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\transformers\\trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\transformers\\trainer.py:2178\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2175\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   2177\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 2178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   2179\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39minclude_num_input_tokens_seen:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\accelerate\\data_loader.py:454\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 454\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2799\u001B[0m, in \u001B[0;36mDataset.__getitems__\u001B[1;34m(self, keys)\u001B[0m\n\u001B[0;32m   2797\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, keys: List) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List:\n\u001B[0;32m   2798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2799\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2800\u001B[0m     n_examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(batch))])\n\u001B[0;32m   2801\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [{col: array[i] \u001B[38;5;28;01mfor\u001B[39;00m col, array \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_examples)]\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2795\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2793\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[0;32m   2794\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2795\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2780\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[1;34m(self, key, **kwargs)\u001B[0m\n\u001B[0;32m   2778\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[0;32m   2779\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m query_table(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data, key, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m-> 2780\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpa_subtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformat_columns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_all_columns\u001B[49m\n\u001B[0;32m   2782\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2783\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001B[0m, in \u001B[0;36mformat_table\u001B[1;34m(table, key, formatter, format_columns, output_all_columns)\u001B[0m\n\u001B[0;32m    627\u001B[0m python_formatter \u001B[38;5;241m=\u001B[39m PythonFormatter(features\u001B[38;5;241m=\u001B[39mformatter\u001B[38;5;241m.\u001B[39mfeatures)\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\formatting\\formatting.py:400\u001B[0m, in \u001B[0;36mFormatter.__call__\u001B[1;34m(self, pa_table, query_type)\u001B[0m\n\u001B[0;32m    398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat_column(pa_table)\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\datasets\\formatting\\formatting.py:515\u001B[0m, in \u001B[0;36mCustomFormatter.format_batch\u001B[1;34m(self, pa_table)\u001B[0m\n\u001B[0;32m    513\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_arrow_extractor()\u001B[38;5;241m.\u001B[39mextract_batch(pa_table)\n\u001B[0;32m    514\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_features_decoder\u001B[38;5;241m.\u001B[39mdecode_batch(batch)\n\u001B[1;32m--> 515\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 28\u001B[0m, in \u001B[0;36mpreprocess_dataset.<locals>.transform_images\u001B[1;34m(examples)\u001B[0m\n\u001B[0;32m     26\u001B[0m image_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtil_siglip_ds/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     27\u001B[0m images \u001B[38;5;241m=\u001B[39m [read_image(image_dir \u001B[38;5;241m+\u001B[39m image_file, mode\u001B[38;5;241m=\u001B[39mImageReadMode\u001B[38;5;241m.\u001B[39mRGB) \u001B[38;5;28;01mfor\u001B[39;00m image_file \u001B[38;5;129;01min\u001B[39;00m examples[image_column]]\n\u001B[1;32m---> 28\u001B[0m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [image_transformations(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m examples\n",
      "Cell \u001B[1;32mIn[8], line 28\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     26\u001B[0m image_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtil_siglip_ds/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     27\u001B[0m images \u001B[38;5;241m=\u001B[39m [read_image(image_dir \u001B[38;5;241m+\u001B[39m image_file, mode\u001B[38;5;241m=\u001B[39mImageReadMode\u001B[38;5;241m.\u001B[39mRGB) \u001B[38;5;28;01mfor\u001B[39;00m image_file \u001B[38;5;129;01min\u001B[39;00m examples[image_column]]\n\u001B[1;32m---> 28\u001B[0m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [\u001B[43mimage_transformations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m examples\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[7], line 24\u001B[0m, in \u001B[0;36mTransform.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# x = x.type(torch.uint8).permute(1, 2, 0).numpy()\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malbu_transforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     25\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\albumentations\\core\\composition.py:247\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, force_apply, *args, **data)\u001B[0m\n\u001B[0;32m    244\u001B[0m transforms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms \u001B[38;5;28;01mif\u001B[39;00m need_to_run \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_always_apply\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_check_args:\n\u001B[1;32m--> 247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_args(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m    250\u001B[0m     p\u001B[38;5;241m.\u001B[39mensure_data_valid(data)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\albumentations\\core\\composition.py:320\u001B[0m, in \u001B[0;36mCompose._check_args\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m internal_data_name \u001B[38;5;129;01min\u001B[39;00m checked_single:\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 320\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be numpy array type\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    321\u001B[0m     shapes\u001B[38;5;241m.\u001B[39mappend(data\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m internal_data_name \u001B[38;5;129;01min\u001B[39;00m checked_multi \u001B[38;5;129;01mand\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data):\n",
      "\u001B[1;31mTypeError\u001B[0m: image must be numpy array type"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T10:39:41.442965Z",
     "start_time": "2024-05-26T10:39:38.794960Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.save_model(\"siglip-finetune-ep2\")",
   "id": "ba70756941154076",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:55.458932Z",
     "start_time": "2024-05-25T17:52:43.592934Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sys\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    SiglipModel,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    AutoProcessor,\n",
    "    EvalPrediction\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:56.748932Z",
     "start_time": "2024-05-25T17:52:55.459932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrained_model_path = \"../siglip-large-patch16-384\"\n",
    "\n",
    "model = SiglipModel.from_pretrained(pretrained_model_path, ).to('cuda')\n",
    "processor = AutoProcessor.from_pretrained(pretrained_model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "# image_processor = AutoImageProcessor.from_pretrained(pretrained_model_path)\n",
    "config = model.config"
   ],
   "id": "8063ec144fe688b5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:56.794932Z",
     "start_time": "2024-05-25T17:52:56.749932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the ds into train and val. the image column is in format \"image_1234_1.jpg\". Take 1234 as the image id. Check if the id is smaller than 4086. If yes, put this in train, else in val.\n",
    "def get_ds():\n",
    "    dataset = load_dataset(path='.', data_files=\"til_siglip_ds.json\")\n",
    "    train_dataset = dataset.filter(lambda example: int(example[\"image\"].split(\"_\")[1]) < 4086)\n",
    "    val_dataset = dataset.filter(lambda example: int(example[\"image\"].split(\"_\")[1]) >= 4086)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = get_ds()"
   ],
   "id": "163a43f250fc836a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:56.962932Z",
     "start_time": "2024-05-25T17:52:56.795933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.NEAREST),  # lesson from TIL 2023: https://github.com/aliencaocao/TIL-2023?tab=readme-ov-file#finals-specific-tuning-2\n",
    "            CenterCrop(image_size),\n",
    "            ConvertImageDtype(torch.float16),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, processor.image_processor.image_mean, processor.image_processor.image_std\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)"
   ],
   "id": "571b1395a15b4c7e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:56.978933Z",
     "start_time": "2024-05-25T17:52:56.963932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset['train']\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image\"\n",
    "    caption_column = \"label\"\n",
    "    dataset_columns = (image_column, caption_column)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        text_inputs = processor.tokenizer(captions, padding=\"max_length\")\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        return examples\n",
    "    \n",
    "    data = data.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "    )\n",
    "    \n",
    "    def transform_images(examples):\n",
    "        image_dir = 'til_siglip_ds/'\n",
    "        images = [read_image(image_dir + image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_images)\n",
    "    return data"
   ],
   "id": "4e0c213ca65d6e1e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:57.010932Z",
     "start_time": "2024-05-25T17:52:56.979932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "val_dataset = preprocess_dataset(val_dataset)"
   ],
   "id": "c279944e2c697544",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_dataset.<locals>.transform_images at 0x000001AB4D8AECA0> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:57.025932Z",
     "start_time": "2024-05-25T17:52:57.011932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ],
   "id": "9693726e20379541",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:52:57.182932Z",
     "start_time": "2024-05-25T17:52:57.167933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    return {\"accuracy\": (predictions == labels).mean().item(), 'F1': f1_score(labels, predictions, average='macro')}"
   ],
   "id": "ddf8f55e81aac8a3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SigLIP loss from https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/trainers/proj/image_text/siglip.py#L287\n",
    "# To add into https://github.com/huggingface/transformers/blob/bdb9106f247fca48a71eb384be25dbbd29b065a8/src/transformers/models/siglip/modeling_siglip.py#L1230\n",
    "if return_loss:\n",
    "    eye = torch.eye(logits_per_text.size(0), device=logits_per_text.device)\n",
    "    m1_diag1 = -torch.ones_like(logits_per_text) + 2 * eye\n",
    "    loglik = torch.nn.functional.logsigmoid(m1_diag1 * logits_per_text)\n",
    "    nll = -torch.sum(loglik, dim=-1)\n",
    "    loss = nll.mean()"
   ],
   "id": "5c27b19d51d41776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:51:41.561950Z",
     "start_time": "2024-05-25T17:51:41.557951Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "4b855c1fafd7e9ae",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T18:01:19.495933Z",
     "start_time": "2024-05-25T17:53:03.099941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"siglip-finetune\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,  # decrease from 0.999\n",
    "    num_train_epochs=20,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=5,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
    "    tf32=True,\n",
    "    dataloader_num_workers=4 if sys.platform == 'linux' else 0,  # no work on windows\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='F1',\n",
    "    greater_is_better=True,\n",
    "    # optim='adamw_torch_fused',\n",
    "    optim='adafactor',\n",
    "    # resume_from_checkpoint=False,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile = sys.platform == 'linux',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "train_result = trainer.train()"
   ],
   "id": "3af0728682135276",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='6940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  81/6940 08:07 < 11:44:43, 0.16 it/s, Epoch 0.23/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 42\u001B[0m\n\u001B[0;32m      2\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      3\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m      4\u001B[0m     warmup_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m     torch_compile \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinux\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     33\u001B[0m )\n\u001B[0;32m     34\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     35\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     36\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics\n\u001B[0;32m     41\u001B[0m )\n\u001B[1;32m---> 42\u001B[0m train_result \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\transformers\\trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\transformers\\trainer.py:2216\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2213\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2216\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2219\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2220\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2221\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2222\u001B[0m ):\n\u001B[0;32m   2223\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2224\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\transformers\\trainer.py:3250\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3248\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3250\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:2125\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[0;32m   2124\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2125\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

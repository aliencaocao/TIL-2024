{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T16:12:17.904014Z",
     "start_time": "2024-06-06T16:12:14.857868Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "from ensemble_boxes import weighted_boxes_fusion"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T16:12:18.434599Z",
     "start_time": "2024-06-06T16:12:17.905016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pt_model1 = YOLO(\"runs/detect/yolov9e 0.995 0.825/weights/epoch62.pt\")  # load a pretrained model (recommended for training)\n",
    "# pt_model2 = YOLO(\"../yolov9c_0.99_0.769.pt\")  # load a pretrained model (recommended for training)\n",
    "# pt_model2 = YOLO(\"runs/detect/yolov9e 0.995 0.825/weights/epoch65.engine\")  # load a pretrained model (recommended for training)\n",
    "pt_model2 = YOLO(\"../yolov9e_0.995_0.823_epoch65.pt\")  # load a pretrained model (recommended for training)"
   ],
   "id": "3ee1c22ed716d8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:27:14.912093Z",
     "start_time": "2024-06-05T16:21:12.160856Z"
    }
   },
   "cell_type": "code",
   "source": "pt_model2.export(format=\"engine\", imgsz=(896, 768), simplify=True, workspace=8, batch=6, half=True, dynamic=False)",
   "id": "77c27058598f38c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.2.18  Python-3.9.13 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "YOLOv9e summary (fused): 687 layers, 57377171 parameters, 0 gradients, 189.1 GFLOPs\n",
      "\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from '..\\yolov9e_0.995_0.823_epoch65.pt' with input shape (6, 3, 896, 768) BCHW and output shape(s) (6, 5, 14112) (334.6 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m simplifying with onnxsim 0.4.36...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m export success  10.9s, saved as '..\\yolov9e_0.995_0.823_epoch65.onnx' (219.3 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m starting export with TensorRT 10.0.1...\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m input \"images\" with shape(6, 3, 896, 768) DataType.FLOAT\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m output \"output0\" with shape(6, 5, 14112) DataType.FLOAT\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m building FP16 engine as ..\\yolov9e_0.995_0.823_epoch65.engine\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m export success  360.4s, saved as '..\\yolov9e_0.995_0.823_epoch65.engine' (115.1 MB)\n",
      "\n",
      "Export complete (362.7s)\n",
      "Results saved to \u001B[1mC:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2024\\vlm\\multistage\u001B[0m\n",
      "Predict:         yolo predict task=detect model=..\\yolov9e_0.995_0.823_epoch65.engine imgsz=896,768 half \n",
      "Validate:        yolo val task=detect model=..\\yolov9e_0.995_0.823_epoch65.engine imgsz=896,768 data=til.yaml half WARNING  non-PyTorch val requires square images, 'imgsz=[896, 768]' will not work. Use export 'imgsz=896' if val is required.\n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\yolov9e_0.995_0.823_epoch65.engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T16:18:08.614682Z",
     "start_time": "2024-06-06T16:12:41.433162Z"
    }
   },
   "cell_type": "code",
   "source": "pt_model2.export(format=\"engine\", imgsz=(1600, 1600), simplify=True, workspace=8, batch=1, half=True, dynamic=False)",
   "id": "3dc4198e06154d68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.2.18  Python-3.9.13 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "YOLOv9e summary (fused): 687 layers, 57377171 parameters, 0 gradients, 189.1 GFLOPs\n",
      "\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from '..\\yolov9e_0.995_0.823_epoch65.pt' with input shape (1, 3, 1600, 1600) BCHW and output shape(s) (1, 5, 52500) (334.6 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m starting export with onnx 1.16.1 opset 17...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m simplifying with onnxsim 0.4.36...\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m export success  11.3s, saved as '..\\yolov9e_0.995_0.823_epoch65.onnx' (219.7 MB)\n",
      "\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m starting export with TensorRT 10.0.1...\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m input \"images\" with shape(1, 3, 1600, 1600) DataType.FLOAT\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m output \"output0\" with shape(1, 5, 52500) DataType.FLOAT\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m building FP16 engine as ..\\yolov9e_0.995_0.823_epoch65.engine\n",
      "\u001B[34m\u001B[1mTensorRT:\u001B[0m export success  325.8s, saved as '..\\yolov9e_0.995_0.823_epoch65.engine' (116.2 MB)\n",
      "\n",
      "Export complete (327.2s)\n",
      "Results saved to \u001B[1mC:\\Users\\alien\\Documents\\PyCharm-Projects\\TIL-2024\\vlm\\multistage\u001B[0m\n",
      "Predict:         yolo predict task=detect model=..\\yolov9e_0.995_0.823_epoch65.engine imgsz=1600 half \n",
      "Validate:        yolo val task=detect model=..\\yolov9e_0.995_0.823_epoch65.engine imgsz=1600 data=til.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\yolov9e_0.995_0.823_epoch65.engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:50:45.872847Z",
     "start_time": "2024-06-05T16:50:45.851847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "im1 = Image.open('../../../data/images/image_5095.jpg')\n",
    "im2 = Image.open('../../../data/images/image_5100.jpg')"
   ],
   "id": "519edd47a61897b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:02:41.001191Z",
     "start_time": "2024-06-02T17:02:40.982194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "im1 = Image.open('2.jpg')\n",
    "im2 = Image.open('3.jpg')"
   ],
   "id": "2157516c014ee0a2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T11:55:00.332528Z",
     "start_time": "2024-06-02T11:54:56.681139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result1 = pt_model1.predict([im1, im2], imgsz=1600, conf=0.3, iou=0.1, max_det=10)  # max F1, try augment=True and iou\n",
    "result1_n = [(r.boxes.xyxyn.tolist(), r.boxes.conf.tolist()) for r in result1]\n",
    "result1_n = [tuple(zip(*r)) for r in result1_n]\n",
    "result1 = [(r.boxes.xyxy.tolist(), r.boxes.conf.tolist()) for r in result1]\n",
    "result1 = [tuple(zip(*r)) for r in result1]  # list of tuple[box, conf] in each image in xyxy format"
   ],
   "id": "9f96f9e359db4a92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 928x1600 7 targets, 124.4ms\n",
      "1: 928x1600 7 targets, 124.4ms\n",
      "Speed: 7.6ms preprocess, 124.4ms inference, 932.8ms postprocess per image at shape (1, 3, 928, 1600)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:50:33.027802Z",
     "start_time": "2024-06-05T16:50:26.323492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction"
   ],
   "id": "4ead7f691487e3b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2024 00:50:30 - INFO - numexpr.utils -   Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "06/06/2024 00:50:30 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n",
      "06/06/2024 00:50:32 - INFO - albumentations.check_version -   A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:50:33.293840Z",
     "start_time": "2024-06-05T16:50:33.028803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",\n",
    "    model_path='../yolov9e_0.995_0.823_epoch65.pt',\n",
    "    confidence_threshold=0.5,\n",
    "    image_size=896,\n",
    "    standard_pred_image_size=1600,  # not used for TRT as it dont support standard pred\n",
    "    device=\"cuda\",\n",
    "    cfg={\"task\": 'detect', \"names\": {'0': 'target'}, \"imgsz\": (896, 768), \"half\": True}\n",
    ")"
   ],
   "id": "c82a138171e44afe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:51:11.648322Z",
     "start_time": "2024-06-05T16:51:11.506324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = get_sliced_prediction(im1, model, perform_standard_pred=False, postprocess_class_agnostic=True, batch=6).object_prediction_list\n",
    "result_n = [([r.bbox.minx / 1520, r.bbox.miny / 870, r.bbox.maxx / 1520, r.bbox.maxy / 870], r.score.value) for r in result]\n",
    "result = [([r.bbox.minx, r.bbox.miny, r.bbox.maxx, r.bbox.maxy], r.score.value) for r in result]"
   ],
   "id": "af92c26be481e516",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 6 slices.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m get_sliced_prediction(im1, model, perform_standard_pred\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, postprocess_class_agnostic\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m)\u001B[38;5;241m.\u001B[39mobject_prediction_list\n\u001B[0;32m      2\u001B[0m result_n \u001B[38;5;241m=\u001B[39m [([r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mminx \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1520\u001B[39m, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mminy \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m870\u001B[39m, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mmaxx \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1520\u001B[39m, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mmaxy \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m870\u001B[39m], r\u001B[38;5;241m.\u001B[39mscore\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m result]\n\u001B[0;32m      3\u001B[0m result \u001B[38;5;241m=\u001B[39m [([r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mminx, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mminy, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mmaxx, r\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mmaxy], r\u001B[38;5;241m.\u001B[39mscore\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m result]\n",
      "File \u001B[1;32m~\\Documents\\PyCharm-Projects\\TIL-2024\\vlm\\multistage\\yolov8\\sahi-main\\sahi\\predict.py:258\u001B[0m, in \u001B[0;36mget_sliced_prediction\u001B[1;34m(image, detection_model, slice_height, slice_width, overlap_height_ratio, overlap_width_ratio, perform_standard_pred, postprocess_type, postprocess_match_metric, postprocess_match_threshold, postprocess_class_agnostic, verbose, merge_buffer_length, auto_slice_resolution, slice_export_prefix, slice_dir, batch)\u001B[0m\n\u001B[0;32m    256\u001B[0m     shift_amount_list\u001B[38;5;241m.\u001B[39mappend(slice_image_result\u001B[38;5;241m.\u001B[39mstarting_pixels[group_ind \u001B[38;5;241m*\u001B[39m num_batch \u001B[38;5;241m+\u001B[39m image_ind])\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# perform batch prediction\u001B[39;00m\n\u001B[1;32m--> 258\u001B[0m prediction_result \u001B[38;5;241m=\u001B[39m \u001B[43mget_prediction\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_list\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mnum_batch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdetection_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdetection_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshift_amount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshift_amount_list\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mnum_batch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfull_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mslice_image_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moriginal_image_height\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mslice_image_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moriginal_image_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    265\u001B[0m \n\u001B[0;32m    266\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;66;03m# convert sliced predictions to full predictions\u001B[39;00m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m object_prediction \u001B[38;5;129;01min\u001B[39;00m prediction_result\u001B[38;5;241m.\u001B[39mobject_prediction_list:\n",
      "File \u001B[1;32m~\\Documents\\PyCharm-Projects\\TIL-2024\\vlm\\multistage\\yolov8\\sahi-main\\sahi\\predict.py:99\u001B[0m, in \u001B[0;36mget_prediction\u001B[1;34m(image, detection_model, shift_amount, full_shape, postprocess, verbose, num_batch)\u001B[0m\n\u001B[0;32m     97\u001B[0m     images_as_pil \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mascontiguousarray(read_image_as_pil(img)) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m image]\n\u001B[0;32m     98\u001B[0m     time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 99\u001B[0m     \u001B[43mdetection_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages_as_pil\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m time_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m time_start\n\u001B[0;32m    101\u001B[0m durations_in_seconds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m time_end\n",
      "File \u001B[1;32m~\\Documents\\PyCharm-Projects\\TIL-2024\\vlm\\multistage\\yolov8\\sahi-main\\sahi\\models\\yolov8.py:92\u001B[0m, in \u001B[0;36mYolov8DetectionModel.perform_inference\u001B[1;34m(self, image, num_batch)\u001B[0m\n\u001B[0;32m     90\u001B[0m     prediction_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(image[:, :, ::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# YOLOv8 expects numpy arrays to have BGR\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 92\u001B[0m     prediction_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel([img[:, :, ::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m image], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# YOLOv8 expects numpy arrays to have BGR\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_mask:\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m prediction_result[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmasks:\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:177\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    156\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    157\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    159\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    160\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(source, stream, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:453\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[0;32m    452\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[1;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[1;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 35\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\engine\\predictor.py:243\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    240\u001B[0m paths, im0s, s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch\n\u001B[0;32m    242\u001B[0m \u001B[38;5;66;03m# Preprocess\u001B[39;00m\n\u001B[1;32m--> 243\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m    244\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(im0s)\n\u001B[0;32m    246\u001B[0m \u001B[38;5;66;03m# Inference\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\utils\\ops.py:46\u001B[0m, in \u001B[0;36mProfile.__enter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     45\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Start timing.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtime\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\ultralytics\\utils\\ops.py:61\u001B[0m, in \u001B[0;36mProfile.time\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get current time.\"\"\"\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcuda:\n\u001B[1;32m---> 61\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\cuda\\__init__.py:792\u001B[0m, in \u001B[0;36msynchronize\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    790\u001B[0m _lazy_init()\n\u001B[0;32m    791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[1;32m--> 792\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_synchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:06:39.875006Z",
     "start_time": "2024-06-05T15:06:39.660962Z"
    }
   },
   "cell_type": "code",
   "source": "pt_model2.predict([Image.new('RGB', (870, 760)) for i in range(6)], imgsz=896, conf=0.5, iou=0.1, max_det=10)",
   "id": "a69b36a34b6eddf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 800x896 (no detections), 26.0ms\n",
      "1: 800x896 (no detections), 26.0ms\n",
      "2: 800x896 (no detections), 26.0ms\n",
      "3: 800x896 (no detections), 26.0ms\n",
      "4: 800x896 (no detections), 26.0ms\n",
      "5: 800x896 (no detections), 26.0ms\n",
      "Speed: 3.8ms preprocess, 26.0ms inference, 0.2ms postprocess per image at shape (1, 3, 800, 896)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image1.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image2.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image3.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image4.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'target'}\n",
       " obb: None\n",
       " orig_img: array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)\n",
       " orig_shape: (760, 870)\n",
       " path: 'image5.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.833611806233724, 'inference': 25.99974473317464, 'postprocess': 0.16673405965169272}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:02:43.787754Z",
     "start_time": "2024-06-02T17:02:43.613756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result2 = pt_model2.predict([im1, im2], imgsz=1600, conf=0.3, iou=0.1, max_det=10)  # max F1, try augment=True and iou\n",
    "result2_n = [(r.boxes.xyxyn.tolist(), r.boxes.conf.tolist()) for r in result2]\n",
    "result2_n = [tuple(zip(*r)) for r in result2_n]\n",
    "result2 = [(r.boxes.xyxy.tolist(), r.boxes.conf.tolist()) for r in result2]\n",
    "result2 = [tuple(zip(*r)) for r in result2]  # list of tuple[box, conf] in each image in xyxy format"
   ],
   "id": "5a79892ee9769534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 928x1600 4 targets, 58.5ms\n",
      "1: 928x1600 3 targets, 58.5ms\n",
      "Speed: 8.0ms preprocess, 58.5ms inference, 2.0ms postprocess per image at shape (1, 3, 928, 1600)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T10:51:14.364867Z",
     "start_time": "2024-06-02T10:51:14.279868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result2 = pt_model2.predict([im1], imgsz=(1536, 896), conf=0.3, iou=0.1, max_det=10)  # max F1, try augment=True and iou\n",
    "result2_n = [(r.boxes.xyxyn.tolist(), r.boxes.conf.tolist()) for r in result2]\n",
    "result2_n = [tuple(zip(*r)) for r in result2_n]\n",
    "result2 = [(r.boxes.xyxy.tolist(), r.boxes.conf.tolist()) for r in result2]\n",
    "result2 = [tuple(zip(*r)) for r in result2]  # list of tuple[box, conf] in each image in xyxy format"
   ],
   "id": "6a8a9c9b28875228",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1536x896 7 targets, 45.0ms\n",
      "Speed: 8.0ms preprocess, 45.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1536, 896)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T11:55:14.134310Z",
     "start_time": "2024-06-02T11:55:14.114311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "boxes_list = [[r[0] for r in result1_n[0]], [r[0] for r in result2_n[0]]]\n",
    "scores_list = [[r[1] for r in result1_n[0]], [r[1] for r in result2_n[0]]]\n",
    "labels_list = [[0] * len(result1_n[0]), [0] * len(result2_n[0])]\n",
    "weights = [0.1, 1]\n",
    "boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=0.5, skip_box_thr=1e-4)\n",
    "boxes = boxes.tolist()\n",
    "# normalize\n",
    "boxes = [[x1*1520, y1*870, x2*1520, y2*870] for x1, y1, x2, y2 in boxes]\n",
    "scores = scores.tolist()\n",
    "wbf_result = [list(zip(boxes, scores))]"
   ],
   "id": "b3e26981a4a7c078",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:03:39.721315Z",
     "start_time": "2024-06-02T17:03:39.187716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot\n",
    "for im, boxes in zip([im1, im2], result2):\n",
    "    im = im.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    for (x1, y1, x2, y2), conf in boxes:\n",
    "        # print(x1, y1, x2, y2, conf)\n",
    "        print([x1, y1, x2 - x1, y2 - y1], conf)\n",
    "        draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "        draw.text((x1, y1), text=f'{conf:.2f}', fill='red')\n",
    "    im.show()"
   ],
   "id": "7597d8bcbd40d45a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[805.4537963867188, 266.6635437011719, 65.88726806640625, 51.044525146484375] 0.8840249180793762\n",
      "[780.4070434570312, 344.8293762207031, 60.640869140625, 30.552337646484375] 0.8493534326553345\n",
      "[247.44850158691406, 504.761474609375, 110.45524597167969, 38.27239990234375] 0.8326224684715271\n",
      "[1415.942138671875, 158.29652404785156, 104.057861328125, 83.5587158203125] 0.797531008720398\n",
      "[926.0911865234375, 295.0468444824219, 114.6827392578125, 57.679962158203125] 0.8768801093101501\n",
      "[1131.968994140625, 235.7434844970703, 57.12158203125, 48.26167297363281] 0.8663981556892395\n",
      "[859.451904296875, 518.83251953125, 67.2882080078125, 43.86517333984375] 0.8398226499557495\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

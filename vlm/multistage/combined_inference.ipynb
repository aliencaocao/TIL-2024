{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T12:55:51.142455Z",
     "start_time": "2024-05-25T12:55:41.717463Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor, AutoModelForZeroShotImageClassification, AutoTokenizer, ZeroShotImageClassificationPipeline\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import orjson\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T12:55:51.323967Z",
     "start_time": "2024-05-25T12:55:51.143455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# yolo_model = YOLO(\"yolov9e_0.995_0.801.pt\")  # load a pretrained model (recommended for training)\n",
    "yolo_model = YOLO(\"yolov8/runs/detect/yolov9e 0.995 0.825/weights/last.pt\")  # load a pretrained model (recommended for training)"
   ],
   "id": "eeb454dee613ce7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T12:55:53.028998Z",
     "start_time": "2024-05-25T12:55:51.324968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PipelineWithoutPostprocess(ZeroShotImageClassificationPipeline):\n",
    "    def postprocess(self, model_outputs):\n",
    "        candidate_labels = model_outputs.pop(\"candidate_labels\")\n",
    "        logits = model_outputs[\"logits\"][0]\n",
    "        if self.framework == \"pt\" and self.model.config.model_type == \"siglip\":\n",
    "            probs = torch.sigmoid(logits).squeeze(-1)\n",
    "            scores = probs.tolist()\n",
    "            if not isinstance(scores, list):\n",
    "                scores = [scores]\n",
    "        elif self.framework == \"pt\":\n",
    "            # probs = logits.softmax(dim=-1).squeeze(-1)\n",
    "            probs = logits.squeeze(-1)  # no softmax because only 1 target class at test time, softmax causes it to go 1.0 for all\n",
    "            scores = probs.tolist()\n",
    "            if not isinstance(scores, list):\n",
    "                scores = [scores]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported framework: {self.framework}\")\n",
    "\n",
    "        result = [\n",
    "            {\"score\": score, \"label\": candidate_label}\n",
    "            for score, candidate_label in sorted(zip(scores, candidate_labels), key=lambda x: -x[0])\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "clip_path = 'siglip-large-patch16-384'\n",
    "image_classifier = PipelineWithoutPostprocess(task=\"zero-shot-image-classification\",\n",
    "                                              model=AutoModelForZeroShotImageClassification.from_pretrained(clip_path),\n",
    "                                              tokenizer=AutoTokenizer.from_pretrained(clip_path),\n",
    "                                              image_processor=AutoImageProcessor.from_pretrained(clip_path),\n",
    "                                              batch_size=4, device='cuda')"
   ],
   "id": "fed0d17b62497788",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SigLIP NAViT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from siglip_so400m_14_980_flash_attn2_navit.modeling_siglip import SiglipModel, SiglipVisionModel, SiglipTextModel\n",
    "from siglip_so400m_14_980_flash_attn2_navit.image_processing_siglip import SiglipImageProcessor\n",
    "from siglip_so400m_14_980_flash_attn2_navit.tokenization_siglip import SiglipTokenizer\n",
    "from siglip_so400m_14_980_flash_attn2_navit.processing_siglip import SiglipProcessor\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "PATCH_SIZE = 14\n",
    "\n",
    "pixel_attention_mask = [\n",
    "    [\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "        [1] * 14 + [1] * 14  + [1] * 14,\n",
    "\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "        [0] * 14 + [0] * 14  + [0] * 14,\n",
    "    ],\n",
    "    [\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "        [1] * 14 + [1] * 14  + [0] * 14,\n",
    "    ],\n",
    "]\n",
    "pixel_attention_mask = torch.tensor(pixel_attention_mask, dtype=torch.bool, device=DEVICE)\n",
    "patches_subgrid = pixel_attention_mask.unfold(\n",
    "    dimension=1, size=PATCH_SIZE, step=PATCH_SIZE\n",
    ").unfold(dimension=2, size=PATCH_SIZE, step=PATCH_SIZE)\n",
    "patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# model = SiglipModel.from_pretrained(\"siglip_so400m_14_384_flash_attn2_navit\").to(DEVICE, dtype=torch.float16)\n",
    "vision_model = SiglipVisionModel.from_pretrained(\"siglip_so400m_14_384_flash_attn2_navit\", _flash_attn_2_enabled=False).to(DEVICE, dtype=torch.float16)\n",
    "text_model = SiglipTextModel.from_pretrained(\"siglip_so400m_14_384_flash_attn2_navit\", _flash_attn_2_enabled=False).to(DEVICE, dtype=torch.float16)\n",
    "processor = SiglipProcessor.from_pretrained(\"siglip_so400m_14_384_flash_attn2_navit\")\n",
    "\n",
    "logit_scale_exp = torch.tensor([112.4375], device=DEVICE, dtype=torch.float16)\n",
    "logit_bias = torch.tensor([-16.5469], device=DEVICE, dtype=torch.float16)\n",
    "\n",
    "image = Image.open(\"../green rocket.jpg\")\n",
    "image = np.asarray(image)\n",
    "image = torch.tensor(image, dtype=torch.float16, device=DEVICE).permute(2, 0, 1)\n",
    "image.shape\n",
    "\n",
    "feats = processor(images=[image, image], text=['grey missile','red white and blue light aircraft','green and black missile','white and red helicopter'], padding=True, return_tensors='pt')\n",
    "\n",
    "feats['pixel_values'] = feats['pixel_values'].type(torch.float16).to(DEVICE)\n",
    "feats['input_ids'] = feats['input_ids'].to(DEVICE)\n",
    "image_feat = vision_model.vision_model(pixel_values=feats['pixel_values'])\n",
    "text_feat = text_model.text_model(input_ids=feats['input_ids'])\n",
    "image_feat = image_feat.pooler_output / image_feat.pooler_output.norm(dim=-1, keepdim=True)\n",
    "text_feat = text_feat.pooler_output / text_feat.pooler_output.norm(dim=-1, keepdim=True)\n",
    "similarity_score = image_feat @ text_feat.T * logit_scale_exp + logit_bias\n",
    "similarity_score"
   ],
   "id": "afd98de94e850a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# EVA_CLIP\n",
    "import torch\n",
    "import sys\n",
    "eva_path = 'eva-2/EVA-CLIP/rei/'\n",
    "sys.path.insert(0, 'eva-2/EVA-CLIP/rei/')\n",
    "from eva_clip import create_model_and_transforms, get_tokenizer\n",
    "from PIL import Image\n",
    "model_name = \"EVA02-CLIP-L-14-336\" \n",
    "pretrained = eva_path + \"EVA02_CLIP_L_336_psz14_s6B.pt\" # or \"/path/to/EVA02_CLIP_B_psz16_s8B.pt\"\n",
    "\n",
    "EVA, _, preprocess = create_model_and_transforms(model_name, pretrained, force_custom_clip=True, precision='fp16')\n",
    "EVA_tokenizer = get_tokenizer(model_name)\n",
    "EVA = EVA.to('cuda')"
   ],
   "id": "bfd547c3071c1428",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:27:16.713011Z",
     "start_time": "2024-05-24T17:27:16.577599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
    "\n",
    "model_name = '../real-esrgan/realesr-general-x4v3'\n",
    "rrdb_net = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')  # https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\n",
    "netscale = 4\n",
    "ESRGAN = RealESRGANer(\n",
    "    scale=netscale,\n",
    "    model_path=model_name+ '.pth',\n",
    "    model=rrdb_net,\n",
    "    pre_pad=10,\n",
    "    half=True)"
   ],
   "id": "f76126545350000f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T12:55:53.060997Z",
     "start_time": "2024-05-25T12:55:53.029998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../../data/vlm.jsonl', 'r') as f:\n",
    "    instances = [orjson.loads(line.strip()) for line in f if line.strip() != \"\"]\n",
    "results = []\n",
    "val_percent = 0.2\n",
    "val_split = int(len(instances) * val_percent)\n",
    "train, val = instances[:-val_split], instances[-val_split:]\n",
    "bs = 4\n",
    "batched_instances = [val[i:i + bs] for i in range(0, len(val), bs)]"
   ],
   "id": "7d7734c1329ac540",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T12:59:58.068706Z",
     "start_time": "2024-05-25T12:56:06.024131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch_instance in tqdm(batched_instances):\n",
    "    images = [Image.open(os.path.join('../../data/images/', i['image'])) for i in batch_instance]\n",
    "    \n",
    "    # YOLO object det\n",
    "    yolo_result = yolo_model.predict(images, imgsz=1600, conf=0.1, iou=0.1, max_det=10, verbose=False)  # max F1, try augment=True and adjusting iou\n",
    "    yolo_result = [(r.boxes.xyxy.tolist(), r.boxes.conf.tolist()) for r in yolo_result]\n",
    "    yolo_result = [tuple(zip(*r)) for r in yolo_result]  # list of tuple[box, conf] in each image in xyxy format\n",
    "    \n",
    "    # crop the boxes out\n",
    "    cropped_boxes = []\n",
    "    for im, boxes in zip(images, yolo_result):\n",
    "        im_boxes = []\n",
    "        for (x1, y1, x2, y2), _ in boxes:\n",
    "            cropped = im.crop((x1, y1, x2, y2))\n",
    "            # if not any(s <= 10 for s in cropped.size):\n",
    "            #     cropped = np.asarray(cropped)\n",
    "            #     cropped = ESRGAN.enhance(cropped, outscale=netscale)[0]\n",
    "            #     cropped = Image.fromarray(cropped)\n",
    "            im_boxes.append(cropped)\n",
    "        cropped_boxes.append(im_boxes)\n",
    "    \n",
    "    captions_list = [[anno['caption'] for anno in img['annotations']] for img in batch_instance]  # list of list of str, len is n_img == 4\n",
    "    assert len(cropped_boxes) == len(captions_list)\n",
    "    \n",
    "    # CLIP inference\n",
    "    clip_results = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for boxes, captions in zip(cropped_boxes, captions_list):\n",
    "            r = image_classifier(boxes, candidate_labels=captions)  # for HF pipeline\n",
    "            \n",
    "            # BEGIN SIGLIP NAVIT\n",
    "            # boxes = [torch.tensor(np.asarray(box), dtype=torch.float16, device=DEVICE).permute(2, 0, 1) for box in boxes]\n",
    "            # feats = processor(images=boxes, text=captions, padding=True, return_tensors='pt')\n",
    "            # feats['pixel_values'] = feats['pixel_values'].type(torch.float16).to(DEVICE)\n",
    "            # feats['input_ids'] = feats['input_ids'].to(DEVICE)\n",
    "            # \n",
    "            # image_feat = vision_model.vision_model(pixel_values=feats['pixel_values'], patch_attention_mask=patch_attention_mask)\n",
    "            # text_feat = text_model.text_model(input_ids=feats['input_ids'])\n",
    "            # image_feat = image_feat.pooler_output / image_feat.pooler_output.norm(dim=-1, keepdim=True)\n",
    "            # text_feat = text_feat.pooler_output / text_feat.pooler_output.norm(dim=-1, keepdim=True)\n",
    "            # similarity_score = image_feat @ text_feat.T * logit_scale_exp + logit_bias\n",
    "            # \n",
    "            # r = []\n",
    "            # for image, score in zip(images, similarity_score):\n",
    "            #     image_scores = [{'label': caption, 'score': score.item()} for caption, score in zip(captions, score)]\n",
    "            #     r.append(image_scores)\n",
    "            # END SIGLIP NAVIT\n",
    "            \n",
    "            \n",
    "            # BEGIN EVA CLIP, outputs same format as HF pipeline\n",
    "            # image_batched = [preprocess(im) for im in boxes]\n",
    "            # image_batched = torch.stack(image_batched).to('cuda')\n",
    "            # tokenized_captions = EVA_tokenizer(captions).to('cuda')\n",
    "            # \n",
    "            # image_features = EVA.encode_image(image_batched)\n",
    "            # text_features = EVA.encode_text(tokenized_captions)\n",
    "            # image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            # text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            # similarity_score = (100.0 * image_features @ text_features.T)\n",
    "            # r = []\n",
    "            # for image, score in zip(images, similarity_score):\n",
    "            #     image_scores = [{'label': caption, 'score': score.item()} for caption, score in zip(captions, score)]\n",
    "            #     r.append(image_scores)\n",
    "            # END EVA CLIP\n",
    "            \n",
    "            image_to_text_scores = {caption: [] for caption in captions}  # {caption: [score1, score2, ...]}, scores in sequence of bbox\n",
    "            for box in r:\n",
    "                for label_score in box:\n",
    "                    image_to_text_scores[label_score['label']].append(label_score['score'])\n",
    "            clip_results.append(image_to_text_scores)\n",
    "\n",
    "    # combine the results\n",
    "    visualize = False\n",
    "    for im, cropped_box_PIL, yolo_box, similarity_scores, instance in zip(images, cropped_boxes, yolo_result, clip_results, batch_instance):\n",
    "        if visualize: im_cp = im.copy()\n",
    "        result_for_im = {}\n",
    "        for caption, caption_scores in similarity_scores.items():\n",
    "            box_idx = np.argmax(caption_scores)\n",
    "            highest_caption_score = max(caption_scores)\n",
    "            box = cropped_box_PIL[box_idx]\n",
    "            result_for_im[caption] = yolo_box[box_idx][0]  # dict[caption] = xyxy in list\n",
    "            if visualize:\n",
    "                draw = ImageDraw.Draw(im_cp)  # noqa\n",
    "                (x1, y1, x2, y2), box_conf = yolo_box[box_idx]\n",
    "                draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "                draw.text((x1, y1), text=f'{caption} {box_conf:.2f} {highest_caption_score:.2f}', fill='red')\n",
    "        if visualize: im_cp.show()\n",
    "        results.append({'image': instance['image'], 'annotations': [{'bbox': v, 'caption': k} for k, v in result_for_im.items()]})\n",
    "        # save every image in case of crash\n",
    "        with open('evals/yolov9e-1600-epoch67-conf0.1-siglip-large-patch16-384-zeroshot.json', 'wb+') as f:\n",
    "            f.write(orjson.dumps(results))"
   ],
   "id": "bd1de3e86eac3aaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]C:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "  1%|          | 2/256 [00:04<09:09,  2.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 256/256 [03:52<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T16:51:41.770628Z",
     "start_time": "2024-05-24T16:51:41.763626Z"
    }
   },
   "cell_type": "code",
   "source": "similarity_score",
   "id": "b8cb69dcdcd4e9fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8984,  7.3516, 10.9141,  7.6680],\n",
       "        [10.1953,  7.6016, 11.8984,  5.8672],\n",
       "        [ 5.3203, 12.2734,  0.1947,  0.9014],\n",
       "        [13.1719,  6.2891,  9.3750,  1.9512]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:02:19.576994Z",
     "start_time": "2024-05-22T05:02:18.959162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot bbox\n",
    "for im, boxes in zip(ims, yolo_result):\n",
    "    im = im.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    for (x1, y1, x2, y2), conf in boxes:\n",
    "        draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "        draw.text((x1, y1), text=f'{conf:.2f}', fill='red')\n",
    "    im.show()"
   ],
   "id": "d43145a5bb3bfb6a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T06:47:34.854298Z",
     "start_time": "2024-05-22T06:47:34.846299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "visualize = False\n",
    "for im, cropped_box_PIL, yolo_box, similarity_scores in zip(ims, cropped_boxes, yolo_result, clip_results):\n",
    "    if visualize: im_cp = im.copy()\n",
    "    result_for_im = {}\n",
    "    for caption, caption_scores in similarity_scores.items():\n",
    "        box_idx = np.argmax(caption_scores)\n",
    "        highest_caption_score = max(caption_scores)\n",
    "        box = cropped_box_PIL[box_idx]\n",
    "        result_for_im[caption] = yolo_box[box_idx][0]  # dict[caption] = (xyxy in list, conf)\n",
    "        if visualize:\n",
    "            draw = ImageDraw.Draw(im_cp)\n",
    "            (x1, y1, x2, y2), box_conf = yolo_box[box_idx]\n",
    "            draw.rectangle(xy=((x1, y1), (x2, y2)), outline='red')\n",
    "            draw.text((x1, y1), text=f'{caption} {box_conf:.2f} {highest_caption_score:.2f}', fill='red')\n",
    "    if visualize: im_cp.show()\n",
    "    results.append(result_for_im)"
   ],
   "id": "868a3fa28f6b717c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T06:47:37.331350Z",
     "start_time": "2024-05-22T06:47:37.312831Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "43cb38d309a9426e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'grey missile': [705.0738525390625,\n",
       "   506.7243347167969,\n",
       "   782.65283203125,\n",
       "   563.574951171875],\n",
       "  'red, white, and blue light aircraft': [1030.6815185546875,\n",
       "   77.49951934814453,\n",
       "   1056.74853515625,\n",
       "   110.44055938720703],\n",
       "  'green and black missile': [705.0738525390625,\n",
       "   506.7243347167969,\n",
       "   782.65283203125,\n",
       "   563.574951171875],\n",
       "  'white and red helicopter': [527.7639770507812,\n",
       "   118.3411865234375,\n",
       "   624.7859497070312,\n",
       "   161.6909637451172]},\n",
       " {'grey camouflage fighter jet': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'grey and white fighter plane': [1117.64501953125,\n",
       "   514.673828125,\n",
       "   1254.2855224609375,\n",
       "   553.1058959960938],\n",
       "  'white and black drone': [356.56414794921875,\n",
       "   455.2095031738281,\n",
       "   402.8783264160156,\n",
       "   486.3287353515625],\n",
       "  'white and black fighter jet': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'white missile': [400.4502868652344,\n",
       "   158.0403289794922,\n",
       "   455.9124450683594,\n",
       "   193.24575805664062],\n",
       "  'black and white commercial aircraft': [807.0028686523438,\n",
       "   521.8709716796875,\n",
       "   875.6414794921875,\n",
       "   572.2413940429688]}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

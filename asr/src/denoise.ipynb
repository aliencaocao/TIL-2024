{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T03:51:02.238538Z",
     "start_time": "2024-05-17T03:51:00.647800Z"
    }
   },
   "source": [
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "import librosa"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:55:41.587412Z",
     "start_time": "2024-05-17T03:55:40.546671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# make the OUTPUT_DIR if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "frcrn = pipeline(Tasks.acoustic_noise_suppression, model='speech_frcrn_ans_cirm_16k')\n",
    "# Load default model\n",
    "model, df_state, model_name_suffix = init_df(model_base_dir=\"DeepFilterNet3\")\n",
    "# Get our SpeakerID audio\n",
    "speakerID_audio_folder = \".\"\n",
    "audio_paths = glob.glob(f\"{speakerID_audio_folder}/*.wav\")"
   ],
   "id": "148a36b85012d80a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:55:40,559 - modelscope - INFO - initiate model from speech_frcrn_ans_cirm_16k\n",
      "2024-05-17 11:55:40,560 - modelscope - INFO - initiate model from location speech_frcrn_ans_cirm_16k.\n",
      "2024-05-17 11:55:40,567 - modelscope - INFO - initialize model from speech_frcrn_ans_cirm_16k\n",
      "2024-05-17 11:55:41,482 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-05-17 11:55:41,483 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-05-17 11:55:41,483 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'speech_frcrn_ans_cirm_16k'}. trying to build by task and model information.\n",
      "2024-05-17 11:55:41,484 - modelscope - WARNING - No preprocessor key ('speech_frcrn_ans_cirm_16k', 'acoustic-noise-suppression') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mLoading model settings of DeepFilterNet3\u001B[0m\n",
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mUsing DeepFilterNet3 model at C:\\Users\\alien\\AppData\\Local\\DeepFilterNet\\DeepFilterNet\\Cache\\DeepFilterNet3\u001B[0m\n",
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mInitializing model `deepfilternet3`\u001B[0m\n",
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mFound checkpoint C:\\Users\\alien\\AppData\\Local\\DeepFilterNet\\DeepFilterNet\\Cache\\DeepFilterNet3\\checkpoints\\model_120.ckpt.best with epoch 120\u001B[0m\n",
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mRunning on device cuda:0\u001B[0m\n",
      "\u001B[32m2024-05-17 11:55:41\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mDF\u001B[0m | \u001B[1mModel loaded\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T04:01:21.502629Z",
     "start_time": "2024-05-17T04:01:21.497587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_filename = os.path.splitext(os.path.basename(audio_paths[0]))[0]\n",
    "output_path = f\"{OUTPUT_DIR}/{audio_filename}-loudnormed.wav\""
   ],
   "id": "748f706d6cc25527",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:55:45.577641Z",
     "start_time": "2024-05-17T03:55:45.563580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data, rate = sf.read(audio_paths[0]) # load audio\n",
    "# peak normalize audio to -0.1 dB as frcrn tend to output very soft\n",
    "peak_normalized_audio = pyln.normalize.peak(data, -0.1)  # not using loudness norm here as it causes a bit of clipping\n",
    "sf.write(output_path, peak_normalized_audio, rate)"
   ],
   "id": "a056ee2f9b202608",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T04:01:26.377871Z",
     "start_time": "2024-05-17T04:01:24.944719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frcrn_output_path = os.path.join(OUTPUT_DIR, audio_filename+'frcrn'+'.wav')\n",
    "frcrn_processed = frcrn(output_path, output_path=frcrn_output_path)['output_pcm']"
   ],
   "id": "5af0d175d2712f6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:(1, 122463)\n",
      "padding: 26463\n",
      "inputs after padding:(1, 148926)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T04:01:44.098476Z",
     "start_time": "2024-05-17T04:01:44.086478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frcrn_loudnormed_output_path = os.path.join(OUTPUT_DIR, audio_filename+'frcrn-loudnormed'+'.wav')\n",
    "data, rate = sf.read(frcrn_output_path) # load audio\n",
    "# peak normalize audio to -0.1 dB as frcrn tend to output very soft\n",
    "peak_normalized_audio = pyln.normalize.peak(data, -0.1)  # not using loudness norm here as it causes a bit of clipping\n",
    "sf.write(frcrn_loudnormed_output_path, peak_normalized_audio, rate)"
   ],
   "id": "f495d3d1ca05c7fc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T04:04:09.802337Z",
     "start_time": "2024-05-17T04:04:09.653613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df3_output_path = os.path.join(OUTPUT_DIR, audio_filename+'df3'+'.wav')\n",
    "audio, _ = load_audio(output_path, sr=df_state.sr())\n",
    "enhanced = enhance(model, df_state, audio)\n",
    "save_audio(df3_output_path, enhanced, df_state.sr(), dtype=torch.float16)"
   ],
   "id": "286995a48c93dfa4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python39\\lib\\site-packages\\df\\io.py:106: UserWarning: \"sinc_interpolation\" resampling method name is being deprecated and replaced by \"sinc_interp_hann\" in the next release. The default behavior remains unchanged.\n",
      "  return ta_resample(audio, orig_sr, new_sr, **params)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T04:05:26.302477Z",
     "start_time": "2024-05-17T04:05:26.278438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data, rate = sf.read(df3_output_path) # load audio\n",
    "# peak normalize audio to -0.1 dB as frcrn tend to output very soft\n",
    "peak_normalized_audio = pyln.normalize.peak(data, -0.1)  # not using loudness norm here as it causes a bit of clipping\n",
    "df3_loudnormed_output_path = os.path.join(OUTPUT_DIR, audio_filename+'df3-loudnormed'+'.wav')\n",
    "sf.write(df3_loudnormed_output_path, peak_normalized_audio, rate)"
   ],
   "id": "59013e833b6783d0",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Enhance each audio\n",
    "for audio_path in tqdm(audio_paths):\n",
    "    # get the audio filename without .wav extension\n",
    "    audio_filename = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    output_path = f\"{OUTPUT_DIR}/{audio_filename}.wav\"\n",
    "    frcrn(audio_path, output_path=output_path)\n",
    "\n",
    "    data, rate = sf.read(output_path) # load audio\n",
    "    # peak normalize audio to -0.1 dB as frcrn tend to output very soft\n",
    "    peak_normalized_audio = pyln.normalize.peak(data, -0.1)  # not using loudness norm here as it causes a bit of clipping\n",
    "    sf.write(output_path, peak_normalized_audio, rate)\n",
    "    audio, _ = load_audio(output_path, sr=df_state.sr())\n",
    "    # Denoise the audio\n",
    "    enhanced = enhance(model, df_state, audio)\n",
    "    # Save for listening\n",
    "    save_audio(output_path, enhanced, df_state.sr(), dtype=torch.float16)  # default is torch.int16 which causes clipping on some audios\n",
    "\n",
    "    data, rate = sf.read(output_path) # load audio\n",
    "    normalized_audio = pyln.normalize.peak(data, -0.1)  # not using loudness norm here as it causes a bit of clipping on non palmtree clips\n",
    "    sf.write(output_path, normalized_audio, rate)\n"
   ],
   "id": "22238b6451d0a842"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arach\\Desktop\\ML\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"faster_whisper\").setLevel(logging.DEBUG)\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperTokenizer\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "from ctranslate2.converters import TransformersConverter\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "weights = \"Checkpoints/large-checkpoints/checkpoint-709\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arach\\Desktop\\ML\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arach\\.cache\\huggingface\\hub\\models--openai--whisper-large-v3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Checkpoints/medium-checkpoints/checkpoint-best\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "import numpy as np\n",
    "\n",
    "# weights = \"whisper-checkpoints/1_checkpoint-2000-wer-0.034826/\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-large-v3',resume_download = None)\n",
    "processor.tokenizer.save_pretrained(weights)\n",
    "processor.feature_extractor.save_pretrained(weights)\n",
    "# print(processor.tokenizer.convert_tokens_to_ids('niner'), processor.tokenizer.convert_tokens_to_ids('eufhweif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256 50256\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "outputDir = \"Checkpoints/LargeCombined\"\n",
    "ModelConverter = TransformersConverter(weights)\n",
    "modelPath = ModelConverter.convert(\n",
    "    outputDir,\n",
    "    force = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Converter for niner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create tokenizer.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256 50256\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "import numpy as np\n",
    "\n",
    "# weights = \"whisper-checkpoints/1_checkpoint-2000-wer-0.034826/\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(weights, resume_download = None)\n",
    "processor.tokenizer.save_pretrained(weights)\n",
    "processor.feature_extractor.save_pretrained(weights)\n",
    "print(processor.tokenizer.convert_tokens_to_ids('niner'), processor.tokenizer.convert_tokens_to_ids('eufhweif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer\n",
    "new_tokens=['niner']\n",
    "# Verify custom tokens\n",
    "for token in new_tokens:\n",
    "    assert tokenizer.convert_tokens_to_ids(token) is not None, f\"Token {token} not found in tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2832924454.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    --copy_files tokenizer.json preprocessor_config.json --quantization float16\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!ct2-transformers-converter --model whisper-niner-combined-checkpoints/checkpoint-2485-030704wer/ --output_dir TTNC-2485\n",
    "--copy_files tokenizer_config.json preprocessor_config.json --quantization float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!ct2-transformers-converter --model whisper-niner-combined-checkpoints/checkpoint-2485-030704wer/ --output_dir TTNC-2485 --copy_files tokenizer_config.json preprocessor_config.json added_tokens.json generation_config.json vocab.json special_tokens_map.json normalizer.json --quantization float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer, WhisperForConditionalGeneration\n",
    "\n",
    "# load pre-trained tokenizer and model\n",
    "ckpt = \"openai/whisper-small.en\"\n",
    "tokenizer = WhisperTokenizer.from_pretrained(ckpt, resume_download = None , use_fast=True)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\n",
    "#     model_name,  # checkpoint_name\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     mask_time_prob=0.5,  # 0.05\n",
    "#     mask_time_length=10, # 10\n",
    "#     mask_feature_prob=0.5, # 0\n",
    "#     mask_feature_length=10, # 10\n",
    "#     apply_spec_augment=True,\n",
    "#     resume_download = None\n",
    "# )\n",
    "\n",
    "\n",
    "# define new tokens to add to vocab\n",
    "new_tokens = [\"niner\"]\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "# check if the new tokens are already in the vocabulary\n",
    "# Dont need to check cos we know\n",
    "# new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "\n",
    "\n",
    "# add new random embeddings for the appended tokens\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.freeze_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['whisper-niner-combined-checkpoints/checkpoint-2485-0.030704wer/preprocessor_config.json']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "import numpy as np\n",
    "\n",
    "# weights = \"whisper-checkpoints/1_checkpoint-2000-wer-0.034826/\"\n",
    "\n",
    "# processor = WhisperProcessor.from_pretrained('openai/whisper-small.en',resume_download = None)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(weights, resume_download = None)\n",
    "processor.tokenizer.save_pretrained(weights)\n",
    "processor.feature_extractor.save_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WhisperProcessor in module transformers.models.whisper.processing_whisper:\n",
      "\n",
      "class WhisperProcessor(transformers.processing_utils.ProcessorMixin)\n",
      " |  WhisperProcessor(feature_extractor, tokenizer)\n",
      " |  \n",
      " |  Constructs a Whisper processor which wraps a Whisper feature extractor and a Whisper tokenizer into a single\n",
      " |  processor.\n",
      " |  \n",
      " |  [`WhisperProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and [`WhisperTokenizer`]. See\n",
      " |  the [`~WhisperProcessor.__call__`] and [`~WhisperProcessor.decode`] for more information.\n",
      " |  \n",
      " |  Args:\n",
      " |      feature_extractor (`WhisperFeatureExtractor`):\n",
      " |          An instance of [`WhisperFeatureExtractor`]. The feature extractor is a required input.\n",
      " |      tokenizer (`WhisperTokenizer`):\n",
      " |          An instance of [`WhisperTokenizer`]. The tokenizer is a required input.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WhisperProcessor\n",
      " |      transformers.processing_utils.ProcessorMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Forwards the `audio` argument to WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] and the `text`\n",
      " |      argument to [`~WhisperTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more\n",
      " |      information.\n",
      " |  \n",
      " |  __init__(self, feature_extractor, tokenizer)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  batch_decode(self, *args, **kwargs)\n",
      " |      This method forwards all its arguments to WhisperTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n",
      " |      refer to the docstring of this method for more information.\n",
      " |  \n",
      " |  decode(self, *args, **kwargs)\n",
      " |      This method forwards all its arguments to WhisperTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n",
      " |      the docstring of this method for more information.\n",
      " |  \n",
      " |  get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True)\n",
      " |  \n",
      " |  get_prompt_ids(self, text: str, return_tensors='np')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  feature_extractor_class = 'WhisperFeatureExtractor'\n",
      " |  \n",
      " |  tokenizer_class = 'WhisperTokenizer'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.processing_utils.ProcessorMixin:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str\n",
      " |      Upload the processor files to the 🤗 Model Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your processor to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload processor\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`List[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoProcessor\n",
      " |      \n",
      " |      processor = AutoProcessor.from_pretrained(\"google-bert/bert-base-cased\")\n",
      " |      \n",
      " |      # Push the processor to your namespace with the name \"my-finetuned-bert\".\n",
      " |      processor.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the processor to an organization with the name \"my-finetuned-bert\".\n",
      " |      processor.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs)\n",
      " |      Saves the attributes of this processor (feature extractor, tokenizer...) in the specified directory so that it\n",
      " |      can be reloaded using the [`~ProcessorMixin.from_pretrained`] method.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This class method is simply calling [`~feature_extraction_utils.FeatureExtractionMixin.save_pretrained`] and\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`]. Please refer to the docstrings of the\n",
      " |      methods above for more information.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will\n",
      " |              be created if it does not exist).\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  to_dict(self) -> Dict[str, Any]\n",
      " |      Serializes this instance to a Python dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Dictionary of all the attributes that make up this processor instance.\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save this instance to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file in which this processor instance's parameters will be saved.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes this instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: String containing all the attributes that make up this feature_extractor instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.processing_utils.ProcessorMixin:\n",
      " |  \n",
      " |  from_args_and_dict(args, processor_dict: Dict[str, Any], **kwargs) from builtins.type\n",
      " |      Instantiates a type of [`~processing_utils.ProcessingMixin`] from a Python dictionary of parameters.\n",
      " |      \n",
      " |      Args:\n",
      " |          processor_dict (`Dict[str, Any]`):\n",
      " |              Dictionary that will be used to instantiate the processor object. Such a dictionary can be\n",
      " |              retrieved from a pretrained checkpoint by leveraging the\n",
      " |              [`~processing_utils.ProcessingMixin.to_dict`] method.\n",
      " |          kwargs (`Dict[str, Any]`):\n",
      " |              Additional parameters from which to initialize the processor object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~processing_utils.ProcessingMixin`]: The processor object instantiated from those\n",
      " |          parameters.\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) from builtins.type\n",
      " |      Instantiate a processor associated with a pretrained model.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This class method is simply calling the feature extractor\n",
      " |      [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`], image processor\n",
      " |      [`~image_processing_utils.ImageProcessingMixin`] and the tokenizer\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] methods. Please refer to the docstrings of the\n",
      " |      methods above for more information.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              This can be either:\n",
      " |      \n",
      " |              - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on\n",
      " |                huggingface.co.\n",
      " |              - a path to a *directory* containing a feature extractor file saved using the\n",
      " |                [`~SequenceFeatureExtractor.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      " |              - a path or url to a saved feature extractor JSON *file*, e.g.,\n",
      " |                `./my_model_directory/preprocessor_config.json`.\n",
      " |          **kwargs\n",
      " |              Additional keyword arguments passed along to both\n",
      " |              [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] and\n",
      " |              [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\n",
      " |  \n",
      " |  get_processor_dict(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n",
      " |      processor of type [`~processing_utils.ProcessingMixin`] using `from_args_and_dict`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      " |              specify the folder name here.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the processor object.\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoProcessor') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom feature extractors as the ones\n",
      " |      in the library are already mapped with `AutoProcessor`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoProcessor\"`):\n",
      " |              The auto class to register this new feature extractor with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.processing_utils.ProcessorMixin:\n",
      " |  \n",
      " |  model_input_names\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.processing_utils.ProcessorMixin:\n",
      " |  \n",
      " |  attributes = ['feature_extractor', 'tokenizer']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WhisperProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "outputDir = \"TTNC_whisper-2485-converted\"\n",
    "ModelConverter = TransformersConverter(weights)\n",
    "modelPath = ModelConverter.convert(\n",
    "    outputDir,\n",
    "    force = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "from transformers import WhisperForConditionalGeneration, WhisperTokenizer\n",
    "\n",
    "# Define paths\n",
    "weights = \"whisper-niner-combined-checkpoints/checkpoint-3905-0.0304512wer/\"\n",
    "output_path = \"whisper-niner-converted-3905\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = WhisperForConditionalGeneration.from_pretrained(weights)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Verify custom tokens\n",
    "for token in new_tokens:\n",
    "    assert tokenizer.convert_tokens_to_ids(token) is not None, f\"Token {token} not found in tokenizer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# weights = \"whisper-checkpoints/1_checkpoint-2000-wer-0.034826/\"\n",
    "ModelConverter = TransformersConverter(weights)\n",
    "ModelConverter.load_tokenizer(WhisperTokenizer, weights)\n",
    "modelPath = ModelConverter.convert(\n",
    "    output_path,\n",
    "    force = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = \"TTNC-2485\"\n",
    "weights = \"Checkpoints/LargeCombined\"\n",
    "\n",
    "\n",
    "# weights = \"whisper-checkpoints/1_checkpoint-2000-wer-0.034826/\"\n",
    "\n",
    "# processor = WhisperProcessor.from_pretrained('openai/whisper-small.en',resume_download = None)\n",
    "# processor.tokenizer.save_pretrained(weights)\n",
    "# processor.feature_extractor.save_pretrained(weights)\n",
    "\n",
    "model = WhisperModel(weights, device = \"cuda\", compute_type=\"float16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "filename = \"195.wav\" \n",
    "\n",
    "frequency = 16000\n",
    "waveform, sr = librosa.load(filename, sr = frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.7858061e-04, -1.5157997e-03, -1.1202005e-03, ...,\n",
       "        8.4337598e-04,  4.0823518e-04,  2.5857822e-05], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faster_whisper:Processing audio with duration 00:04.120\n",
      "DEBUG:faster_whisper:Processing segment at 00:00.000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "<|startoftranscript|> token was not found in the prompt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m segments, info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(waveform, beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m segments:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124ms -> \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124ms] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (segment\u001b[38;5;241m.\u001b[39mstart, segment\u001b[38;5;241m.\u001b[39mend, segment\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[1;32mc:\\Users\\arach\\Desktop\\ML\\.env\\lib\\site-packages\\faster_whisper\\transcribe.py:580\u001b[0m, in \u001b[0;36mWhisperModel.generate_segments\u001b[1;34m(self, features, tokenizer, options, encoder_output)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seek \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m encoder_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    573\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(segment)\n\u001b[0;32m    575\u001b[0m (\n\u001b[0;32m    576\u001b[0m     result,\n\u001b[0;32m    577\u001b[0m     avg_logprob,\n\u001b[0;32m    578\u001b[0m     temperature,\n\u001b[0;32m    579\u001b[0m     compression_ratio,\n\u001b[1;32m--> 580\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mno_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     should_skip \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mno_speech_prob \u001b[38;5;241m>\u001b[39m options\u001b[38;5;241m.\u001b[39mno_speech_threshold\n",
      "File \u001b[1;32mc:\\Users\\arach\\Desktop\\ML\\.env\\lib\\site-packages\\faster_whisper\\transcribe.py:870\u001b[0m, in \u001b[0;36mWhisperModel.generate_with_fallback\u001b[1;34m(self, encoder_output, prompt, tokenizer, options)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: options\u001b[38;5;241m.\u001b[39mbeam_size,\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: options\u001b[38;5;241m.\u001b[39mpatience,\n\u001b[0;32m    868\u001b[0m     }\n\u001b[1;32m--> 870\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    871\u001b[0m     encoder_output,\n\u001b[0;32m    872\u001b[0m     [prompt],\n\u001b[0;32m    873\u001b[0m     length_penalty\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mlength_penalty,\n\u001b[0;32m    874\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mrepetition_penalty,\n\u001b[0;32m    875\u001b[0m     no_repeat_ngram_size\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mno_repeat_ngram_size,\n\u001b[0;32m    876\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    877\u001b[0m     return_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    878\u001b[0m     return_no_speech_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    879\u001b[0m     suppress_blank\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39msuppress_blank,\n\u001b[0;32m    880\u001b[0m     suppress_tokens\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39msuppress_tokens,\n\u001b[0;32m    881\u001b[0m     max_initial_timestamp_index\u001b[38;5;241m=\u001b[39mmax_initial_timestamp_index,\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    883\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    885\u001b[0m tokens \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msequences_ids[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    887\u001b[0m \u001b[38;5;66;03m# Recover the average log prob from the returned score.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: <|startoftranscript|> token was not found in the prompt"
     ]
    }
   ],
   "source": [
    "segments, info = model.transcribe(waveform, beam_size=5)\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
